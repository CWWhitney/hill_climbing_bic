@article{COUTINHO2023108211,
title = {Bayesian Optimization for automatic tuning of digital multi-loop PID controllers},
journal = {Computers & Chemical Engineering},
volume = {173},
pages = {108211},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108211},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423000807},
author = {João P.L. Coutinho and Lino O. Santos and Marco S. Reis},
keywords = {Multi-loop PID tuning, Automatic tuning, Bayesian optimization, Internal model control, Sequential loop closing},
abstract = {In recent years, the use of Bayesian optimization (BO) for efficient automatic tuning of general controller structures through iterative closed-loop experiments, has been attracting increasing interest. However, its potential for tuning interactive multi-loop PID controllers in Multi Input Multi Output (MIMO) processes remains largely unexplored. Even though the optimization domain greatly affects closed-loop performance and safety, it is usually defined manually, through expert knowledge or experimentation. This paper presents a novel systematic methodology for defining the optimization domain for automatic multi-loop PID tuning using BO. Sequential loop closing, system identification and tuning relations are used to constrain the bounds on controller parameters to meaningful ranges, including gains and sampling times. This provides an effective way to improve the convergence of BO and secure process safety during closed-loop experiments, without requiring a MIMO process model or extensive prior knowledge. The methodology can be applied ”as is” to single-loop PID controllers.}
}
@article{ZHANG2023677,
title = {Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system},
journal = {Applied Mathematical Modelling},
volume = {119},
pages = {677-697},
year = {2023},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23001269},
author = {Yiming Zhang and Dingyang Zhang and Xiaoge Zhang and Lemiao Qiu and Felix T.S. Chan and Zili Wang and Shuyou Zhang},
keywords = {Deep Reinforcement Learning, Multi-component System, Probabilistic Machine Learning, Maintenance Scheduling, Sampling-Efficient Learning},
abstract = {In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.}
}
@article{HENNEBOLD20241296,
title = {Combination of Process Mining and Causal Discovery Generated Graph Models for Comprehensive Process Modeling},
journal = {Procedia CIRP},
volume = {130},
pages = {1296-1302},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.242},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013994},
author = {Christoph Hennebold and Muhammad M. Islam and Jonas Krauß and Marco F. Huber},
keywords = {Process Mining, Causal Discovery, Domain Knowledge, Hybrid Modeling},
abstract = {The extraction of process knowledge and its use for modeling and subsequent analysis is a very valuable approach to optimizing processes. Process Mining (PM) methods are widely used knowledge extraction techniques based on process knowledge recorded in event logs. One shortcoming is that PM approaches cannot easily access data in non-event log format, and secondly, that models are generated using directly-follows relations, which in the case of processes with concurrent activities means that process transitions may not be modeled correctly, and the resulting models do not correspond to reality as well as being unnecessarily complex. In addition, increasing digitization in manufacturing enables an ever greater collection of various data sources throughout the process with varying levels of information. Besides PM, in recent years great progress has been made in learning causal structures via causal discovery (CD) as well as in explicitly using additional expert knowledge. We argue that CD and expert knowledge are valuable additions to existing PM based approaches, as they allow knowledge extraction and modeling on different process levels. To combine these levels of information, this paper uses a manufacturing use case to show heterogeneous data sources and expert knowledge are used to create hybrid models. The results show that the combination of data-driven modeling methods with expert knowledge help to compensate for the weaknesses of the individual methods and achieve better overall results.}
}
@article{HE2022939,
title = {Multi-objective optimization of the textile manufacturing process using deep-Q-network based multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {939-949},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000728},
author = {Zhenglei He and Kim Phuc Tran and Sebastien Thomassey and Xianyi Zeng and Jie Xu and Changhai Yi},
keywords = {Deep reinforcement learning, Deep Q-networks, Multi-objective, Optimization, Decision, Process, Textile Manufacturing},
abstract = {Multi-objective optimization, such as quality, productivity, and cost, of the textile manufacturing process is increasingly challenging because of the growing complexity involved in the development of textile industry in the upcoming big data era. It is hard for traditional methods to deal with high-dimension decision space in this issue, and prior experts’ knowledge is required as well as human intervention. This paper proposed a novel framework that transformed the textile process optimization problem into a stochastic game, and introduced deep Q-networks algorithm instead of current methods to approach it in a multi-agent system. The developed multi-agent reinforcement learning system applied a utilitarian selection mechanism to maximize the sum of all agents’ rewards (obeying the increasing ε-greedy policy) in each state, to avoid the interruption of multiple equilibria and achieve the correlated equilibrium optimal solutions of the textile process. The case study result reflects that the proposed MARL system can achieve the optimal solutions for the textile ozonation process, and it performs better than the traditional approaches.}
}
@article{LUOMA2021112614,
title = {Developing a conceptual influence diagram for socio-eco-technical systems analysis of biofouling management in shipping – A Baltic Sea case study},
journal = {Marine Pollution Bulletin},
volume = {170},
pages = {112614},
year = {2021},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2021.112614},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X21006482},
author = {Emilia Luoma and Lauri Nevalainen and Elias Altarriba and Inari Helle and Annukka Lehikoinen},
keywords = {Systems analysis, Risk management, Influence diagram, Alien species, Biofouling management, Baltic Sea},
abstract = {Ship hulls create a vector for the transportation of harmful non-indigenous species (NIS) all over the world. To sustainably prevent NIS introductions, the joint consideration of environmental, economic and social aspects in the search of optimal biofouling management strategies is needed. This article presents a multi-perspective soft systems analysis of the biofouling management problem, based on an extensive literature review and expert knowledge collected in the Baltic Sea area during 2018–2020. The resulting conceptual influence diagram (CID) reveals the multidimensionality of the problem by visualizing the causal relations between the key elements and demonstrating the entanglement of social, ecological and technical aspects. Seen as a boundary object, we suggest the CID can support open dialogue and better risk communication among stakeholders by providing an illustrative and directly applicable starting point for the discussions. It also provides a basis for quantitative management optimization in the future.}
}
@article{GONZALEZ2023108110,
title = {New paradigms for exploiting parallel experiments in Bayesian optimization},
journal = {Computers & Chemical Engineering},
volume = {170},
pages = {108110},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108110},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422004434},
author = {Leonardo D. González and Victor M. Zavala},
keywords = {Bayesian optimization, High-throughput experiments, Parallelization},
abstract = {Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential algorithm (one experiment is proposed per round) and thus cannot directly exploit high-throughput (parallel) experiments. Diverse modifications to the BO framework have been proposed in the literature to enable exploitation of parallel experiments but such approaches are limited in the degree of parallelization that they can achieve and can lead to redundant experiments (thus wasting resources and potentially compromising performance). In this work, we present new parallel BO paradigms that exploit the structure of the system to partition the design space. Specifically, we propose an approach that partitions the design space by following the level sets of the performance function and an approach that exploits the partially separable structure of the performance function found. We conduct extensive numerical experiments using a reactor case study to benchmark the effectiveness of these approaches against a variety of state-of-the-art parallel algorithms reported in the literature. Our computational results show that our approaches significantly reduce the required search time and increase the probability of finding a global (rather than local) solution.}
}
@article{ZHOU201669,
title = {An empirical study of Bayesian network parameter learning with monotonic influence constraints},
journal = {Decision Support Systems},
volume = {87},
pages = {69-79},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616300744},
author = {Yun Zhou and Norman Fenton and Cheng Zhu},
keywords = {BN parameter learning, Monotonic influences, Exterior constraints, Experiments on publicly available BNs, Real medical study},
abstract = {Learning the conditional probability table (CPT) parameters of Bayesian networks (BNs) is a key challenge in real-world decision support applications, especially when there are limited data available. A conventional way to address this challenge is to introduce domain knowledge/expert judgments that are encoded as qualitative parameter constraints. In this paper we focus on a class of constraints which is naturally encoded in the edges of BNs with monotonic influences. Experimental results indicate that such monotonic influence constraints are widespread in practical BNs (all BNs used in the study contain such monotonic influences). To exploit expert knowledge about such constraints we have developed an improved constrained optimization algorithm, which achieves good parameter learning performance using these constraints, especially when data are limited. Specifically, this algorithm outperforms the previous state-of-the-art and is also robust to errors in labelling the monotonic influences. The method is applied to a real world medical decision support BN where we had access to expert-provided constraints and real hospital data. The results suggest that incorporating expert judgments about monotonic influence constraints can lead to more accurate BNs for decision support and risk analysis.}
}
@article{AV2025113138,
title = {Accelerated experimental design using a human–AI teaming framework},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113138},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113138},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001856},
author = {Arun Kumar A.V. and Alistair Shilton and Sunil Gupta and Shannon Ryan and Majid Abdolshah and Hung Le and Santu Rana and Julian Berk and Mahad Rashid and Svetha Venkatesh},
keywords = {Machine learning, Experimental design, Sample-efficiency, Gaussian process, Bayesian optimization, Human–AI teaming},
abstract = {In this paper we propose a human–AI teaming framework for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our proposed algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. We validate our proposed algorithm using synthetic data and with human experts performing real-world experiments.}
}
@article{ZHANG201265,
title = {Robust hyperspectral vision-based classification for multi-season weed mapping},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {69},
pages = {65-73},
year = {2012},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2012.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271612000421},
author = {Yun Zhang and David C. Slaughter and Erik S. Staab},
keywords = {Computer vision, Plant recognition, Machine learning, Multiclassifier system, Weed control, Seasonal variability},
abstract = {This study investigated the robustness of hyperspectral image-based plant recognition to seasonal variability in a natural farming environment in the context of automated in-row weed control. A machine vision system was developed and equipped with a CCD camera integrated with a line-imaging spectrograph for close-range weed sensing and mapping. Three canonical Bayesian classifiers were developed using canopy reflectance (400–795nm) collected over three seasons for tomato and weeds. The performance of the three season-specific classifiers was tested by changing environmental conditions, resulting in an increase in total error rate of up to 36%. Global calibration across the complete span of the three seasons produced overall classification accuracies of 85.0%, 90.0% and 92.7%, respectively, for 2005, 2006 and 2008. To improve the stability of global classifier over multiple seasons, a multiclassifier system was constructed with three canonical Bayesian classifiers optimized for the three seasons individually. This system was tested on a data set simulating an upcoming season with field conditions similar to that in 2005. The system increased the total discrimination accuracy to 95.8% for the tested season under simulation. This method provided an innovative direction for achieving robust plant recognition over multiple seasons by integrating expert knowledge from historical data that most closely matched the new field environment.}
}
@article{KANGAS2000157,
title = {Improving the quality of landscape ecological forest planning by utilising advanced decision-support tools},
journal = {Forest Ecology and Management},
volume = {132},
number = {2},
pages = {157-171},
year = {2000},
issn = {0378-1127},
doi = {https://doi.org/10.1016/S0378-1127(99)00221-2},
url = {https://www.sciencedirect.com/science/article/pii/S0378112799002212},
author = {Jyrki Kangas and Ron Store and Pekka Leskinen and Lauri Mehtätalo},
keywords = {Bayesian statistics, Ecological modelling, Forest management, GIS, Multi-criteria decision support, Uncertainty},
abstract = {The quality of landscape ecological analyses and their integration with the multi-objective comparison of forest plans can be improved by making use of the decision-support methods, techniques, and tools produced by recent research on forest planning, as demonstrated in this study. Special attention is given to strengthening the ecological grounds of calculations through modelling expert knowledge, quantification of ecological evaluations, integration of different objectives and different phases of the planning process, and analysing the effects of uncertainty in ecological judgments on the final results. The planning process is illustrated by a case study. The landscape ecological approach is finding increasing application in practical forest planning, especially in boreal forestry. Unfortunately, gaps in the available ecological knowledge, and the inefficiency of the planning methods and tools used often lead to vague planning processes. In many cases, only methods originally developed for wood-production planning are still applied, and planning advances (e.g. multi-objective optimisation, Geographical Information Systems (GIS) tools, and modelling expert knowledge) are under-utilised. In this study, HERO heuristic multi-objective optimisation, GIS operations, pairwise comparisons techniques, and Bayesian analysis are applied in an integrated planning process. Efficient forest plan alternatives are generated for further consideration by utilising heuristic optimization and GIS. Given the multi-objective choice situation, the plans generated are holistically evaluated by means of multiple decision-support tools and techniques.}
}
@article{TRUONG2013390,
title = {Web-based tool for expert elicitation of the variogram},
journal = {Computers & Geosciences},
volume = {51},
pages = {390-399},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2012.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300412002890},
author = {Phuong N. Truong and Gerard B.M. Heuvelink and John Paul Gosling},
keywords = {Spatial variability, Expert knowledge, Geostatistics, Subjective prior information},
abstract = {The variogram is the keystone of geostatistics. Estimation of the variogram is deficient and difficult when there are no or too few observations available due to budget constraints or physical and temporal obstacles. In such cases, expert knowledge can be an important source of information. Expert knowledge can also fulfil the increasing demand for an a priori variogram in Bayesian geostatistics and spatial sampling optimization. Formal expert elicitation provides a sound scientific basis to reliably and consistently extract knowledge from experts. In this study, we aimed at applying existing statistical expert elicitation techniques to extract the variogram of a regionalized variable that is assumed to have either a multivariate normal or lognormal spatial probability distribution from expert knowledge. To achieve this, we developed an elicitation protocol and implemented it as a web-based tool to facilitate the elicitation of beliefs from multiple experts. Our protocol has two main rounds: elicitation of the marginal probability distribution and elicitation of the variogram. The web-based tool has three main components: a web interface for expert elicitation and feedback; a component for statistical computation and mathematical pooling of multiple experts’ knowledge; and a database management component. Results from a test case study show that the protocol is adequate and that the online elicitation tool functions satisfactorily. The web-based tool is free to use and supports scientists to conveniently elicit the variogram of spatial random variables from experts. The source code is available from the journal FTP site under the GNU General Public License.}
}
@article{POESCHL2016247,
title = {Situation-based Methodology for Planning the Commissioning of Special Machinery Using Bayesian Networks},
journal = {Procedia CIRP},
volume = {57},
pages = {247-252},
year = {2016},
note = {Factories of the Future in the digital environment - Proceedings of the 49th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116311970},
author = {Sebastian Poeschl and Frank Wirth and Thomas Bauernhansl},
keywords = {Special Machinery, Bayesian Networks, Process Heuristics, Process Optimization},
abstract = {In German mechanical engineering customized systems and integration solutions are the biggest trends which are mainly applied in special machinery. This paper shows a method to decrease test and commissioning time by using expert knowledge and by considering the risk of failing processes. In literature and practice there is a wide research on virtual commissioning. However, research on methods to optimize production is very rare for complex machinery. In the proposed method, for planning and adapting processes, the authors use heuristics because of their ability to optimize processes using expert knowledge. For the decision of the right application of a heuristic, Bayesian Networks are applied to rate and compare different alternatives. Thus, the result is a method which allows to rate the processes with the needed time and the possible risk for an elimination and a substitution of these processes. Using this method the throughput time of a laser system in production in one single commissioning process is decreased in the validation example by approximately three days.}
}
@article{SOARES2018715,
title = {Experiences of Structured Elicitation for Model-Based Cost-Effectiveness Analyses},
journal = {Value in Health},
volume = {21},
number = {6},
pages = {715-723},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518302274},
author = {Marta O. Soares and Linda Sharples and Alec Morton and Karl Claxton and Laura Bojke},
keywords = {Bayesian, cost effectiveness, decision modeling, elicitation, expert judgment, subjective},
abstract = {Background
Empirical evidence supporting the cost-effectiveness estimates of particular health care technologies may be limited, or it may even be missing entirely. In these situations, additional information, often in the form of expert judgments, is needed to reach a decision. There are formal methods to quantify experts’ beliefs, termed as structured expert elicitation (SEE), but only limited research is available in support of methodological choices. Perhaps as a consequence, the use of SEE in the context of cost-effectiveness modelling is limited.
Objectives
This article reviews applications of SEE in cost-effectiveness modelling with the aim of summarizing the basis for methodological choices made in each application and recording the difficulties and challenges reported by the authors in the design, conduct, and analyses.
Methods
The methods used in each application were extracted along with the criteria used to support methodological and practical choices and any issues or challenges discussed in the text. Issues and challenges were extracted using an open field, and then categorised and grouped for reporting.
Results
The review demonstrates considerable heterogeneity in methods used, and authors acknowledge great methodological uncertainty in justifying their choices. Specificities of the context area emerging as potentially important in determining further methodological research in elicitation are between- expert variation and its interpretation, the fact that substantive experts in the area may not be trained in quantitative subjects, that judgments are often needed on various parameter types, the need for some form of assessment of validity, and the need for more integration with behavioural research to devise relevant debiasing strategies.
Conclusions
This review of experiences of SEE highlights a number of specificities/constraints that can shape the development of guidance and target future research efforts in this area.}
}
@article{SEIXAS2014140,
title = {A Bayesian network decision model for supporting the diagnosis of dementia, Alzheimer׳s disease and mild cognitive impairment},
journal = {Computers in Biology and Medicine},
volume = {51},
pages = {140-158},
year = {2014},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2014.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0010482514000961},
author = {Flávio Luiz Seixas and Bianca Zadrozny and Jerson Laks and Aura Conci and Débora Christina {Muchaluat Saade}},
keywords = {Clinical decision support system, Bayesian network, Dementia, Alzheimer׳s disease, Mild cognitive impairment},
abstract = {Population aging has been occurring as a global phenomenon with heterogeneous consequences in both developed and developing countries. Neurodegenerative diseases, such as Alzheimer׳s Disease (AD), have high prevalence in the elderly population. Early diagnosis of this type of disease allows early treatment and improves patient quality of life. This paper proposes a Bayesian network decision model for supporting diagnosis of dementia, AD and Mild Cognitive Impairment (MCI). Bayesian networks are well-suited for representing uncertainty and causality, which are both present in clinical domains. The proposed Bayesian network was modeled using a combination of expert knowledge and data-oriented modeling. The network structure was built based on current diagnostic criteria and input from physicians who are experts in this domain. The network parameters were estimated using a supervised learning algorithm from a dataset of real clinical cases. The dataset contains data from patients and normal controls from the Duke University Medical Center (Washington, USA) and the Center for Alzheimer׳s Disease and Related Disorders (at the Institute of Psychiatry of the Federal University of Rio de Janeiro, Brazil). The dataset attributes consist of predisposal factors, neuropsychological test results, patient demographic data, symptoms and signs. The decision model was evaluated using quantitative methods and a sensitivity analysis. In conclusion, the proposed Bayesian network showed better results for diagnosis of dementia, AD and MCI when compared to most of the other well-known classifiers. Moreover, it provides additional useful information to physicians, such as the contribution of certain factors to diagnosis.}
}
@article{DANG201930,
title = {A Bayesian Belief Network – Based approach to link ecosystem functions with rice provisioning ecosystem services},
journal = {Ecological Indicators},
volume = {100},
pages = {30-44},
year = {2019},
note = {Sven Erik Jørgensen - Memorial Issue},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X1830308X},
author = {Kinh Bac Dang and Wilhelm Windhorst and Benjamin Burkhard and Felix Müller},
keywords = {Agriculture, Ecosystem service demand, Ecosystem service supply, Ecosystem service budget, Socio-ecological system, Scenario},
abstract = {The complex interactions between environmental and anthropogenic components have significantly influenced rice cultivation. The clear understanding of these interactions is important to (i) optimize rice provisioning ecosystem service (ES) supply, (ii) minimize negative impacts on other ES and (iii) choose suitable strategies for sustainable agriculture. Impacts of environmental and anthropogenic components on rice provisioning ES supply largely depend on site selection and farming practices. The demand for rice can be determined by the size of the population and imports/exports of rice products. Rice provisioning ES supply and demand need to be balanced if the goal is an import-independent and sustainable agriculture. As a decision support tool, Bayesian Belief Networks (BBN) are used for quantifying various ES supply types, demands as well as their budgets. The BBN network presented in this study was developed through interviews, expert knowledge, geographical information systems and statistical models. The results show that the capacity of rice provision can be optimized through site selection and farming practice. The results can help to reduce crop failures and to choose suitable areas for the use of new practices and technologies. Moreover, the presented BBN has been used to forecast future patterns of rice provision through effective or ineffective options of the environmental and human-derived components in eight scenarios. Thereby, the BBN turns out to be a promising decision support tool for agricultural managers in predicting probabilities of success in scenarios of agricultural planning.}
}
@article{JENSEN2023102005,
title = {Machine learning guided development of high-performance nano-structured nickel electrodes for alkaline water electrolysis},
journal = {Applied Materials Today},
volume = {35},
pages = {102005},
year = {2023},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2023.102005},
url = {https://www.sciencedirect.com/science/article/pii/S2352940723002743},
author = {Veronica Humlebæk Jensen and Enzo Raffaele Moretti and Jonas Busk and Emil Howaldt Christiansen and Sofie Marie Skov and Emilie Jacobsen and Mikkel Rykær Kraglund and Arghya Bhowmik and Ragnar Kiebach},
keywords = {Water electrolysis, Nano catalyst, Hydrogen evolution reaction, Bayesian optimization, Technical electrodes, Human in the loop},
abstract = {Utilizing a human in the loop Bayesian optimisation paradigm based on Gaussian process regression, we optimized an Ni electrodeposition method to synthesize nano-structured, high-performance hydrogen evolution reaction electrodes. Via exploration-exploitation stages, the synthesis process variables current density, temperature, ligand concentration and deposition time were optimized influencing the deposition layer morphology and, consequently, hydrogen evolution reaction activity. The resulting structures range from micrometre-sized, star-shaped features to nano-sized sandpaper-like structures with very high specific surface areas and good hydrogen evolution reaction activity. Using the overpotential at 10 mA cm−2 as the figure of merit, hydrogen evolution reaction overpotentials as low as -117 mV were reached, approaching the best known technical high surface area electrodes (e.g. Raney Ni). This is achieved with considerably fewer experiments than what would have been necessary with a linear grid search, as the machine learning model could capture the unintuitive interdependencies of the synthesis variables.}
}
@article{LEE2011868,
title = {Grammatical error simulation for computer-assisted language learning},
journal = {Knowledge-Based Systems},
volume = {24},
number = {6},
pages = {868-876},
year = {2011},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2011.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S095070511100058X},
author = {Sungjin Lee and Jonghoon Lee and Hyungjong Noh and Kyusong Lee and Gary Geunbae Lee},
keywords = {Intelligent grammar tutoring, Corrective feedback, Grammatical error detection, Language learner simulation, Grammar quiz},
abstract = {This paper presents an automated method to generate realistic grammatical errors that can perform crucial functions for advanced technologies in computer-assisted language learning (CALL), including generating corrective feedback in dialog-based CALL (DB-CALL) systems, simulating a language learner to optimize tutoring strategies, and generating context-dependent grammar quizzes as educational materials. The goal of this study is to make grammatical errors generated by automatic simulators more realistic. To generate realistic errors, expert knowledge of language learners’ error characteristics was imported into a statistical modeling system that uses Markov logic, which provides a theoretically sound way to encode knowledge into probabilistic first-order logic. We learned the weights of first-order formulas from a learner corpus. The improved quality of the proposed method was demonstrated through comparative experiments using automatic evaluations (precision and recall rate and Kullback–Leibler divergence between error distributions) and human assessments. The proposed method increased precision by 6% and recall by 8.33% averaged across all proficiency levels. It also exhibited a relative improvement of 37.5% in the average Kullback–Leibler divergence. Judgment by human evaluators showed that the proposed method increased the average scores in two different evaluation tasks by 7 and by 0.411. Finally, we present a measure of labor savings to help predict the time and cost associated with this method for those who plan to exploit grammatical error simulation for their applications. The results indicate that using the proposed method could reduce the grammatical error generation time by 59% in average.}
}
@article{TANG2023102989,
title = {Towards a model of human-cyber–physical automata and a synthesis framework for control policies},
journal = {Journal of Systems Architecture},
volume = {144},
pages = {102989},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102989},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001686},
author = {Xiaochen Tang and Miaomiao Zhang and Wanwei Liu and Bowen Du and Zhiming Liu},
keywords = {HCPS, HCPA, Markov decision process, Model-free reinforcement learning, Model synthesis},
abstract = {Advances in research and increasing applications of Cyber–Physical Systems (CPSs) show the need to consider factors of humans in the loop. This has led to the growing research focus on Human-Cyber–Physical Systems (HCPSs). In general, humans in an HCPS interact with both the cyber and physical systems, as well as among the humans themselves. For a better understanding, correct design, development, operation, and maintenance of HCPSs, a computational theory based on a computational model is required. This paper presents our initial work towards a model of human-cyber–physical automata (HCPA). We consider an HCPS as a combination of a human-physical system (HPS) and a CPS in which the control switches between the humans and the machines. We define an HCPA by connecting the automaton of the HPS and the automaton of the CPS through a switch control automaton. The switch control automaton makes switching decision in some critical states shared by the HPS and the CPS. Our theorem shows that the control switching between the HPS and the CPS increases the probability of satisfying a given property. We model the behaviour of a human in specified applications or even in carry out specific tasks, instead of general human intelligence. Therefore, a human can make mistakes to decision making and thus it is a probabilistic automaton with learning ability. The switching between the human and the machine is modelled by an oracle. The oracle learns about the human behaviour, the machine behaviour, as well as the environment to make the control decisions. To generate the control policies of the human and the oracle, we propose a synthesis framework to maximize the probability of the satisfaction of a property specified in Linear Temporal Logic (LTL) by the HCPA. We present a prototype implementation of the framework by extending the model-free reinforcement learning (RL) algorithm and model-free deep-RL algorithm, and our experiment shows that our synthesis framework is effective in obtaining switch policies.}
}
@article{LENG2024111713,
title = {An adaptive convolutional neural network based on transmissibility grayscale image for online identification of offshore platform damage pattern},
journal = {Mechanical Systems and Signal Processing},
volume = {221},
pages = {111713},
year = {2024},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111713},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024006113},
author = {Jiancheng Leng and Jinyong Ma and Huiyu Feng},
keywords = {Damage pattern recognition, Convolutional neural network, Transmissibility grayscale image, Attention mechanism, Variational Bayesian},
abstract = {In response to the challenges in offshore platform damage pattern recognition, where traditional methods lack adaptability owing to relying on feature extraction and expert knowledge, a novel method for online recognition of offshore platform damage patterns using an improved convolutional neural network (CNN) based on transmissibility grayscale image is proposed. The original transmissibility function is converted into a two-dimensional grayscale image to capture the essential features in the original signal, which is then used as the input for the deep learning model. Subsequently, an attention mechanism is introduced to enable the CNN to focus on key areas of the image, emphasize important feature channels, dynamically adjust feature representations, and enhance its overall performance. Meanwhile, the model is optimized by virtue of variational Bayesian algorithm and early stopping criteria, and finally, the trained offline model is applied to online damage pattern recognition of offshore platforms. Through on indoor vibration monitoring experiments, seven different damage patterns of the platform model are recognized accurately, which show that the accuracy of the improved CNN model is increased by an average of 8.5 % as compared to traditional intelligent methods. Furthermore, the proposed method is applied to the online damage identification of offshore platforms in the field, and the corresponding prediction accuracy reaches 98.9 %, demonstrating the feasibility and generalizability of the proposed method in adaptive feature extraction and precise identification of damage patterns.}
}
@article{MAHMOOD2024205283,
title = {Optimizing natural gas pipeline risk assessment using hybrid fuzzy bayesian networks and expert elicitation for effective decision-making strategies},
journal = {Gas Science and Engineering},
volume = {125},
pages = {205283},
year = {2024},
issn = {2949-9089},
doi = {https://doi.org/10.1016/j.jgsce.2024.205283},
url = {https://www.sciencedirect.com/science/article/pii/S2949908924000797},
author = {Yasir Mahmood and Jessica Chen and Nita Yodo and Ying Huang},
keywords = {Risk assessment, Fuzzy set theory, Cause and effect, Fuzzy bayesian network, Pipeline failure, External and internal failure factors},
abstract = {Natural gas pipelines are susceptible to external and internal risk factors, such as corrosion, environmental conditions, external interferences, construction and design faults, and equipment failures. Bayesian Networks (BN) is a promising risk assessment approach widely used to evaluate these risk factors. One of BN's inherent limitations is its inability to accurately capture statistical dependencies and causal relationships, which can be overcome by incorporating expert elicitation into BN. To account for uncertainty and vagueness in assessing pipeline failure risks, fuzzy set theory (FST) can be combined with BN, commonly known as Fuzzy Bayesian Networks (FBN). This study developed an FBN framework that uses linguistic variables to calculate fuzzy probability (FPr) through domain expert elicitation, and crisp probabilities (CPr) are computed using historical incident data from the Pipeline and Hazardous Materials Safety Administration (PHMSA). Based on the findings from the case study of the Midwest region of the USA, external factors, i.e., third-party interference, outside force, and other incidents, significantly impact pipeline performance and reliability. Diagnosis inference indicates that in the Midwest region of the USA, pipeline material and age are critical factors leading to corrosion failure by threatening pipeline integrity. The findings from this study suggested that a targeted risk mitigation strategy is paramount for minimizing the risks associated with pipeline networks.}
}
@article{BOUYAKHSAINE2024114519,
title = {Prediction of residential building occupancy using Machine learning with integrated sensor and survey Data: Insights from a living lab in Morocco},
journal = {Energy and Buildings},
volume = {319},
pages = {114519},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114519},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824006352},
author = {Khadija Bouyakhsaine and Abderrahim Brakez and Mohcine Draou},
keywords = {Occupancy prediction, Machine learning, Data-driven, Internet of Things, Survey data, Living lab},
abstract = {Building occupancy information is essential for effective energy management in buildings through the adoption of energy conservation and occupant-centric control strategies. These strategies endeavor to contribute to optimizing energy consumption while ensuring occupant comfort. This study focuses on advanced occupancy modeling techniques to enhance energy efficiency in residential buildings, utilizing various data-driven techniques. Various Machine Learning models, such as Random Forest, Bayesian Network, Decision Trees, Support Vector Machines, K-Nearest Neighbors, eXtreme Gradient Boosting, and Regularized Greedy Forest, have been evaluated for predicting and classifying building occupancy. Employing a living lab approach within a residential setting, the research evaluates model performance using two ground truth datasets: IoT sensor and survey data. Experimental tests use Accuracy and F1_score as evaluation criteria, demonstrating accuracy rates ranging from 70% to 95.96%. The results highlight the performance of these models in predicting residential occupancy, offering valuable insight into two approaches to occupancy modeling. The Random Forest model performs exceptionally well in capturing occupancy trends, while the Bayesian Network model, combined with expert knowledge, provides detailed predictions of occupancy types and zones. The research also addresses challenges related to data collection, including privacy concerns. It presents effective occupancy modeling strategies for residential energy management.}
}
@article{SOARES20241469,
title = {Recommendations on the Use of Structured Expert Elicitation Protocols for Healthcare Decision Making: A Good Practices Report of an ISPOR Task Force},
journal = {Value in Health},
volume = {27},
number = {11},
pages = {1469-1478},
year = {2024},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524028432},
author = {Marta Soares and Abigail Colson and Laura Bojke and Salah Ghabri and Osvaldo Ulises Garay and Jenna K. Felli and Karen Lee and Elizabeth Molsen-David and Oswaldo Morales-Napoles and Victoria A. Shaffer and Maarten J. IJzerman},
keywords = {Bayesian estimation, IDEA protocol, modified Delphi, SHELF, structured expert elicitation, uncertainty},
abstract = {Healthcare decision making, including regulatory and reimbursement decisions, is based on uncertain assessments of clinical and economic value. This arises from the evidence supporting those assessments being uncertain, incomplete, or even absent. Qualitative, structured expert elicitation (SEE) is a valuable tool for extracting expert knowledge about an uncertain quantity and formulating that knowledge as a probability distribution. This creates a useful input to decision modeling and support, particularly in areas with limited evidence, such as advanced therapy products, precision medicine, rare diagnoses, and other areas with high uncertainty. Structured SEE protocols are used to improve the transparency, accuracy, and consistency of quantitative judgments from experts, limiting the effect of heuristics and biases. This task force report introduces 5 commonly used protocols for SEE (Sheffield elicitation framework; modified Delphi method; Cooke’s classical method; investigate, discuss, estimate, aggregate protocol; and the Medical Research Council reference protocol). It describes the common elements of SEE, discusses how these protocols differ in their implementation of those elements and illustrates the use of the protocols. The report then reviews the relevant constraints on implementing SEE within the context of healthcare decision making and considers the strengths and weaknesses of these protocols in light of those considerations. Because this is an introductory report on an emerging topic, specific recommendations on practice are not made. However, there are broad recommendations based on the suitability of the different protocols in various decision contexts. The report concludes with recommendations for further research to better guide future practice.}
}
@article{NEIL2025110948,
title = {Calibrating multi-constraint ensemble ecosystem models using genetic algorithms and Approximate Bayesian Computation: A case study of rewilding at the Knepp Estate, UK},
journal = {Ecological Modelling},
volume = {500},
pages = {110948},
year = {2025},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110948},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024003363},
author = {Emily Neil and Ernesto Carrella and Richard Bailey},
keywords = {Ensemble ecosystem modelling, Lotka-Volterra, Rewilding, Species reintroductions, Calibration, Genetic algorithms, Optimisation},
abstract = {This paper presents a new ensemble ecosystem model (EEM) which predicts the impacts of species reintroductions and optimises potential future management interventions at the Knepp Estate rewilding project, UK. Compared to other EEMs, Knepp has a relatively high level of data availability that can be used to constrain the model, including time-series abundance data and expert knowledge. This could improve the realism of outputs and enable more nuanced and context-specific management intervention recommendations. Calibrating EEMs can be challenging, however, and as the number of constraints increases, so does the complexity of the model fitting process. We use a new Genetic Algorithm-Approximate Bayesian Computation (GA-ABC) approach wherein GA outputs are used to inform the prior distributions for ABC. To reduce the parameter search space, we fixed twelve parameters - the consumer self-interaction strengths αi,iand negative growth rates – based on theoretical assumptions. While the GA-ABC method proved effective at efficiently searching the parameter space and optimising multiple constraints, it was computationally intensive and struggled to identify a broad range of outputs. Ultimately, this led to an ensemble of models with similar trajectories. Several potential ways to address this are discussed. Our results reinforce the findings of previous studies that the EEM methodology has potential for guiding conservation management and decision-making. Outputs suggest that reintroducing large herbivores was key to maintaining a diverse grassland-scrubland-woodland ecosystem, and optimisation experiments informed species characteristics and stocking densities needed to achieve specific goals. Ultimately, refining the EEM methodology to improve calibration and facilitate the integration of additional data will enhance its utility for ecosystem management, helping to achieve more effective and informed outcomes.}
}
@article{DENG2021106854,
title = {Build complementary models on human feedback for simulation to the real world},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106854},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106854},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001179},
author = {Zixuan Deng and Yanping Xiang and Zhongfeng Kang},
keywords = {Safe reinforcement learning, Human-in-the-loop reinforcement learning, Markov decision processes, Supervised learning},
abstract = {Using simulators is a cost-effective way to meet human needs. Nevertheless, inevitable errors derived from the gap between simulation and the real world sometimes cause great losses and must be taken seriously. This paper focuses on one cause of the gap, which is the incomplete state representation in simulation, and proposes a supervised learning approach, correcting human-unacceptable policies calculated by simulators, based on human feedback. The approach first detects the related blind spots by classifiers which are trained on data from aggregation of noisy human feedback. Then, it corrects the human-unacceptable policies through the complementary model presented based on linear function approximation (LFA) and a policy iteration algorithm FRU-SADPP that uses radial basis functions (RBFs). We evaluate our approach on two simulated domains and demonstrate its higher accuracy of policies than two baselines, in terms of three typical kinds of human suboptimality and human errors, and three types of human feedback. Experiments also show the scalability of our approach.}
}
@article{ECHEVESTE202443,
title = {Enhancing Hip Exoskeleton Tuning Performance with Machine Learning: An Anthropometric Data-Driven Approach},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {28},
pages = {43-48},
year = {2024},
note = {The 4th Modeling, Estimation, and Control Conference – 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324023413},
author = {Salvador Echeveste and Md. Safwan Mondal and Subramanian Ramasay and Pranav A. Bhounsule},
keywords = {Assistive, Rehabilitation Robotics, Optimal Control, Machine Learning in modeling, estimation, control},
abstract = {Hip exoskeletons offer significant potential for enhancing human movement, especially for those with mobility impairments. However, optimizing their performance typically involves lengthy discrete and continuous optimization methods. To address this, we propose a novel approach using machine learning to predict controller parameter classes, aiming to improve the tuning process. Our method relies on subject-specific anthropometric data to predict optimal controller parameters for hip exoskeletons. Through a machine learning framework, we develop predictive models to determine the most effective parameter settings tailored to individual users. By employing feature engineering, data synthesis techniques, and model training, we enhance the initialization of Bayesian Human-in-the-loop (HIL) optimization. Results indicate that our machine learning models can predict control parameter classes with 75% accuracy, leading to a 9.98% improvement in optimized exoskeleton performance for users.}
}
@article{GRAY2015404,
title = {An Abductive Diagnosis and Modeling Concept for Wind Power Plants},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {21},
pages = {404-409},
year = {2015},
note = {9th IFAC Symposium on Fault Detection, Supervision andSafety for Technical Processes SAFEPROCESS 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.09.560},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315016894},
author = {Christopher S. Gray and Roxane Koitz and Siegfried Psutka and Franz Wotawa},
keywords = {Fault diagnosis, Model-based diagnosis, ATMS, Abductive diagnosis, Wind turbines},
abstract = {The number and complexity of industrial wind turbine installations have increased significantly over the last decades. As maintenance costs are high and down-times lead to substantial revenue loss, increasing the reliability and optimizing the maintenance process are crucial tasks from an industrial perspective. However, many of the proposed diagnosis systems merely focus on parts of the turbine or only locate a portion of the faults. Model-based diagnosis has been applied successfully in industrial settings and further provides a solid theoretical background. Therefore, we propose a model-based approach depending on automatically retrieved health variables and on an extensive expert knowledge on specific component-oriented failure modes as well as their effects on measurable signals. As the expert assessment provides causal links between faults and their manifestations, we formally create a Propositional Horn Clause Abduction Problem (PHCAP). In this paper, we present a modeling concept taking advantage of existing expert knowledge and show how it can be used for wind turbine diagnosis employing already existing algorithms and structures. Our models enable us to directly determine root causes from the links between malfunctions to observable turbine signals on a system level with a relatively low effort compared to other approaches.}
}
@article{LIU2024545,
title = {Task graph offloading via deep reinforcement learning in mobile edge computing},
journal = {Future Generation Computer Systems},
volume = {158},
pages = {545-555},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001638},
author = {Jiagang Liu and Yun Mi and Xinyu Zhang and Xiaocui Li},
keywords = {Mobile edge computing, Task graph, Computation offloading, Reinforcement learning},
abstract = {Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Markov Decision Process (MDP). Then, we design a deep reinforcement learning algorithm (SATA-DRL) to learn the task scheduling strategy from the interaction with the environment, to improve user experience. Extensive simulations validate that SATA-DRL is superior to existing strategies in terms of reducing average makespan and deadline violation.}
}
@article{RIVAS200727,
title = {Application of Bayesian networks to the evaluation of roofing slate quality},
journal = {Engineering Geology},
volume = {94},
number = {1},
pages = {27-37},
year = {2007},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0013795207001299},
author = {T. Rivas and J.M. Matías and J. Taboada and A. Argüelles},
keywords = {Roofing slate, Bayesian networks, Quality index, Ornamental stone},
abstract = {We present the results of an application of Bayesian networks to the evaluation of the quality of roofing slate. Using data from borehole samples of a slate deposit, two networks constructed with different levels of expert knowledge input were evaluated for their capacities for inference and prediction of the quality of slate for roofing. It can be concluded from the results that Bayesian networks are an extremely useful automated tool for evaluating the quality of a resource such as slate for the following reasons: they allow final quality to be assessed immediately and in probabilistic terms with a tolerable degree of uncertainty; they enable the probability of obtaining different final qualities to be estimated when new data is introduced into the network; they speed up the evaluation process by simulating and optimising the work of the expert (during field data collection and borehole description) in identifying the parameters with the greatest influence on final quality; and finally, they have a satisfactory capacity for prediction.}
}
@article{PINERODEPLAZA2025105810,
title = {Human-centred AI for emergency cardiac care: Evaluating RAPIDx AI with PROLIFERATE_AI},
journal = {International Journal of Medical Informatics},
volume = {196},
pages = {105810},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.105810},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625000279},
author = {Maria Alejandra {Pinero de Plaza} and Kristina Lambrakis and Fernando Marmolejo-Ramos and Alline Beleigoli and Mandy Archibald and Lalit Yadav and Penelope McMillan and Robyn Clark and Michael Lawless and Erin Morton and Jeroen Hendriks and Alison Kitson and Renuka Visvanathan and Derek P. Chew and Carlos Javier {Barrera Causil}},
keywords = {Artificial intelligence, Emergency medicine, Decision support, Cardiac biomarkers, Usability, Adoption, Human-centred evaluation},
abstract = {Background
Chest pain diagnosis in emergency care is hindered by overlapping cardiac and non-cardiac symptoms, causing diagnostic uncertainty. Artificial Intelligence, such as RAPIDx AI, aims to enhance accuracy through clinical and biochemical data integration, but its adoption relies on addressing usability, explainability, and seamless workflow integration without disrupting care.
Objective
Evaluate RAPIDx AI’s integration into clinical workflows, address usability barriers, and optimise its adoption in emergencies.
Methods
The PROLIFERATE_AI framework was implemented across 12 EDs (July 2022–January 2024) with 39 participants: 15 experts co-designed a survey via Expert Knowledge Elicitation (EKE), applied to 24 ED clinicians to assess RAPIDx AI usability and adoption. Bayesian inference, using priors, estimated comprehension, emotional engagement, usage, and preference, while Monte Carlo simulations quantified uncertainty and variability, generating posterior means and 95% bootstrapped confidence intervals. Qualitative thematic analysis identified barriers and optimisation needs, with data triangulated through the PROLIFERATE_AI scoring system to rate RAPIDx AI’s performance by user roles and demographics.
Results
Registrars exhibited the highest comprehension (median: 0.466, 95 % CI: 0.41–0.51) and preference (median: 0.458, 95 % CI: 0.41–0.48), while residents/interns scored the lowest in comprehension (median: 0.198, 95 % CI: 0.17–0.26) and emotional engagement (median: 0.112, 95 % CI: 0.09–0.14). Registered nurses showed strong emotional engagement (median: 0.379, 95 % CI: 0.35–0.45). Novice users faced usability and workflow integration barriers, while experienced clinicians suggested automation and streamlined workflows. RAPIDx AI scored “Good Impact,” excelling with trained users but requiring targeted refinements for novices.
Conclusion
RAPIDx AI enhances diagnostic accuracy and efficiency for experienced users, but usability challenges for novices highlight the need for targeted training and interface refinements. The PROLIFERATE_AI framework offers a robust methodology for evaluating and scaling AI solutions, addressing the evolving needs of sociotechnical systems.}
}
@article{DENEAULT2025723,
title = {Preferential Bayesian optimization improves the efficiency of printing objects with subjective qualities†‡†Data and code from this study are archived at https://osf.io/fqp96/.‡Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00320a},
journal = {Digital Discovery},
volume = {4},
number = {3},
pages = {723-737},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00320a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2500035X},
author = {James R. Deneault and Woojae Kim and Jiseob Kim and Yuzhe Gu and Jorge Chang and Benji Maruyama and Jay I. Myung and Mark A. Pitt},
abstract = {Despite recent advances in closed-loop 3D printing, optimizing subjective and difficult-to-quantify qualities—such as surface finish and clarity of fine detail—remains a significant challenge, often relying on the traditional time-consuming and inefficient trial-and-error process. Preferential Bayesian optimization (PBO) is a machine learning technique that uses human preference judgements to efficiently guide the search for such abstract optimums in a high-dimensional space. We evaluated PBO's ability to identify optimal parameter values in printing profiles of vases and pairs of 3D cones. In semi-autonomous printing campaigns, a human observer ranked triplets of images of these objects with a target object in mind, preferring slender/bulbous vases and cone pairs that were smooth and well-formed. Results show that PBO consistently and quickly identified an optimal parameter combination across repeated testing. Modeling was then used to identify object dimensions responsible for preference judgements and to mimic preference behavior. Findings suggest that PBO is a promising tool for expanding the range of 3D objects that can be printed efficiently.}
}
@article{SOUSA2018156,
title = {Combination of expert decision and learned based Bayesian Networks for multi-scale mechanical analysis of timber elements},
journal = {Expert Systems with Applications},
volume = {93},
pages = {156-168},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306723},
author = {Hélder S. Sousa and Francisco Prieto-Castrillo and José C. Matos and Jorge M. Branco and Paulo B. Lourenço},
keywords = {Bayesian Networks, Timber, Multi-scale analysis, Expert systems, Learning algorithms, Ranking},
abstract = {The use of Bayesian Networks allows to organize and correlate information gathered from different sources and its optimization may incorporate restrictions adjusting the network based on expert knowledge and network operativeness, in such a way that it may satisfactorily represent a given domain. The main goal of this paper is to study if an optimized learned Bayesian Network may be used as a prior structure for an expert based network of an engineering structural material analysis. The methodology is applied to a database of results from an experimental campaign that focused on the mechanical characterization of timber elements recovered from an early 20th century building. To that study case it is evidenced that through a suitable combination of model averaging and supervision steps it is possible to achieve robust and reliable models to underpin the causal structure of a typical multi-scale timber analysis.}
}
@article{PERRIN2024106700,
title = {Towards a configurable and non-hierarchical search space for NAS},
journal = {Neural Networks},
volume = {180},
pages = {106700},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106700},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006245},
author = {Mathieu Perrin and William Guicquero and Bruno Paille and Gilles Sicard},
keywords = {Neural architecture search, Convolutional neural network, Wasserstein autoencoder, Bayesian optimization},
abstract = {Neural Architecture Search (NAS) outperforms handcrafted Neural Network (NN) design. However, current NAS methods generally use hard-coded search spaces, and predefined hierarchical architectures. As a consequence, adapting them to a new problem can be cumbersome, and it is hard to know which of the NAS algorithm or the predefined hierarchical structure impacts performance the most. To improve flexibility, and be less reliant on expert knowledge, this paper proposes a NAS methodology in which the search space is easily customizable, and allows for full network search. NAS is performed with Gaussian Process (GP)-based Bayesian Optimization (BO) in a continuous architecture embedding space. This embedding is built upon a Wasserstein Autoencoder, regularized by both a Maximum Mean Discrepancy (MMD) penalization and a Fully Input Convex Neural Network (FICNN) latent predictor, trained to infer the parameter count of architectures. This paper first assesses the embedding’s suitability for optimization by solving 2 computationally inexpensive problems: minimizing the number of parameters, and maximizing a zero-shot accuracy proxy. Then, two variants of complexity-aware NAS are performed on CIFAR-10 and STL-10, based on two different search spaces, providing competitive NN architectures with limited model sizes.}
}
@article{PAVLOVSKII2022101772,
title = {Hybrid genetic predictive modeling for finding optimal multipurpose multicomponent therapy},
journal = {Journal of Computational Science},
volume = {63},
pages = {101772},
year = {2022},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2022.101772},
url = {https://www.sciencedirect.com/science/article/pii/S1877750322001545},
author = {Vladislav V. Pavlovskii and Ilia V. Derevitskii and Sergey V. Kovalchuk},
keywords = {Optimal drug combinations, Synergy drugs, Drug effect predicting, Chronic disease therapy, Machine learning, Genetic optimization, Bayesian networks, Diabetes mellitus},
abstract = {This article proposes a new approach to calculating optimal therapy for the treatment of chronic diseases. For all patients, this approach can calculate drug combinations with the best probability of achieving treatment goals. This approach is based on the combination of machine learning methods, probability graph models, classical statistical modeling tools, and the authors’ algorithms. The paper shows the advantages of the proposed approach for the practical medical task of selecting drug combinations for type 2 diabetes mellitus treatment. The authors created useful tools for calculating drug combinations for compensating carbohydrate metabolism for diabetes patients. For treatment goals and values, the research team used the main carbohydrate metabolism indicator – glycated hemoglobin, the lipid profile indicator – low-density lipoprotein cholesterol, and also arterial blood pressure which is an important indicator of a patient’s condition. In the virtual implementation, the method showed higher quality in comparison to results obtained without using this approach. For validation, classic metrics and experts’ knowledge were used. Both types of validation showed that the method was of high quality and did not contradict fundamental medicine knowledge. Therefore, this method can be used as part of a decision support system for medical specialists who work with type 2 diabetes mellitus patients. This paper is an extended version of the investigation [1]. The short version also proposed an approach for calculating optimal therapy. Although this approach is of good quality, it had some drawbacks, so some improvements were made in the new version of the algorithm. The authors used Bayesian networks to reduce the search space by selecting only essential groups of drugs. Moreover, a genetic algorithm was used to find a more accurate solution.}
}
@article{HOBBALLAH201895,
title = {Formulating preliminary design optimization problems using expert knowledge: Application to wood-based insulating materials},
journal = {Expert Systems with Applications},
volume = {92},
pages = {95-105},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306401},
author = {Mohamad Hussein Hobballah and Amadou Ndiaye and Franck Michaud and Mark Irle},
keywords = {Knowledge acquisition, Knowledge domain decomposition, Experts knowledge, Causal maps, Bio-based material, Thermal insulation},
abstract = {The design of wood-fiber based thermal insulating material with optimized properties and characteristics requires a good scientific knowledge of the latter. Currently, the technical, economic and ecological characteristics of a wood-fiber based composite mat are not well known. This article presents a methodology for the acquisition of expert knowledge so that the properties and characteristics can be modeled. The knowledge domain is multidisciplinary and it was delimited and decomposed into disciplines and domains of expertise. A panel of seven experts was constituted to cover the various disciplines and domains of expertise of the knowledge domain. Knowledge acquisition sessions, guided by the estimated importance and the availability of knowledge, were conducted using semi-structured interviews and the mapping of the existing causal relations between variables. A causal map was established to represent the causal knowledge of each of the experts and then, the established causal maps were assembled into a unique global causal map, which was subsequently validated by the experts. It contains the information necessary for formulating the properties and characteristics to be optimized, which were: thermal conductivity; thickness recovery of the material; the manufacturing cost and the product's environmental impact. Properties and characteristics are function of raw material type, their morphological properties and the manufacturing process variables. This methodology makes it possible to establish which objectives to optimize and which variables influence each objective. Consequently, the objective functions of the optimization problem can be clarified, specified and modeled.}
}
@incollection{WALLACH2014277,
title = {Chapter 7 - Parameter Estimation with Bayesian Methods},
editor = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
booktitle = {Working with Dynamic Crop Models (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {277-309},
year = {2014},
isbn = {978-0-12-397008-4},
doi = {https://doi.org/10.1016/B978-0-12-397008-4.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970084000071},
author = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
keywords = {Bayes, MCMC, Parameter estimation, Posterior, Prior, Uncertainty},
abstract = {Bayesian methods are becoming increasingly popular for estimating parameters of complex mathematical models because the Bayesian approach provides a coherent framework for dealing with uncertainty. To start with, the principle is a prior probability distribution of the model parameters describing our belief about the parameter values before we use the set of measurements. The Bayesian methods then tell us how to update this belief using the measurements to give the posterior parameter density. In the Bayesian approach, the parameters are defined as random variables and the prior and posterior parameter distributions represent our belief about parameter values before and after using observed data to improve estimates. This approach has several advantages: i) parameters can be estimated from different types of information (data, literature, expert knowledge); ii) the posterior probability distribution can be used to implement uncertainty analysis methods; iii) the posterior probability distribution can be used for optimizing decisions in the face of uncertainty. This chapter presents the basic principles of the Bayesian approach and describes several algorithms to calculate posterior parameter distributions. These algorithms are illustrated in several applications on yield and soil carbon estimation.}
}
@article{LAURENT2014318,
title = {Bayesian object-based estimation of LAI and chlorophyll from a simulated Sentinel-2 top-of-atmosphere radiance image},
journal = {Remote Sensing of Environment},
volume = {140},
pages = {318-329},
year = {2014},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2013.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0034425713003088},
author = {Valérie C.E. Laurent and Michael E. Schaepman and Wout Verhoef and Joerg Weyermann and Roberto O. Chávez},
keywords = {Top-of-atmosphere radiance, Sentinel-2, APEX, Variable estimation, Bayesian optimization, Object-based, Coupled model, Radiative transfer, SLC, MODTRAN4, Li–Ross, Nadir-normalization},
abstract = {Leaf area index (LAI) and chlorophyll content (Cab) are important vegetation variables which can be monitored using remote sensing (RS). Physically-based approaches have higher transferability and are therefore better suited than empirically-based approaches for estimating LAI and Cab at global scales. These approaches, however, require the inversion of radiative transfer (RT) models, which is an ill-posed and underdetermined problem. Four regularization methods have been proposed, allowing finding stable solutions: 1) model coupling, 2) using a priori information (e.g. Bayesian approaches), 3) spatial constraints (e.g. using objects), and 4) temporal constraints. For mono-temporal data, only the first three methods can be applied. In an earlier study, we presented a Bayesian object-based algorithm for inverting the SLC-MODTRAN4 coupled canopy-atmosphere RT model, and compared it with a Bayesian LUT inversion. The results showed that the object-based approach provided more accurate LAI estimates. This study, however, heavily relied on expert knowledge about the objects and vegetation classes. Therefore, in this new contribution, we investigated the applicability of the Bayesian object-based inversion of the SLC-MODTRAN4 model to a situation where no such knowledge was available. The case study used a 16×22km2 simulated top-of-atmosphere image of the upcoming Sentinel-2 sensor, covering the area near the city of Zurich, Switzerland. Seven APEX radiance images were nadir-normalized using the parametric Li–Ross model, spectrally and spatially resampled to Sentinel-2 specifications, geometrically corrected, and mosaicked. The atmospheric effects between APEX flight height and top-of-atmosphere level were added based on two MODTRAN4 simulations. The vegetation objects were identified and delineated using a segmentation algorithm, and classified in four levels of brightness in the visible domain. The LAI and Cab maps obtained from the Bayesian object-based inversion of the coupled SLC-MODTRAN4 model presented realistic spatial patterns. The impact of the parametric Li–Ross nadir-normalization was evaluated by comparing 1) the angular signatures of the SLC-MODTRAN4 and Li–Ross models, and 2) the LAI and Cab maps obtained from a Li–Ross nadir-normalized image (using nadir viewing geometry) and from the original image (using the original viewing geometry). The differences in angular signatures were small but systematic, and the differences between the LAI and Cab maps increased from the center towards the edges of the across-track direction. The results of this study contribute to preparing the RS community for the arrival of Sentinel-2 data in the near future, and generalize the applicability of the Bayesian object-based approach for estimating vegetation variables to cases where no field data are available.}
}
@article{SCHUH20231143,
title = {Heuristic Guided Hierarchical Reinforcement Learning Approach For The Economic Improvement Of Production Lines},
journal = {Procedia CIRP},
volume = {120},
pages = {1143-1148},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.139},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123008715},
author = {Günther Schuh and Seth Schmitz and Jan Maetschke and Benedict Janssen and Hanna Offermanns},
keywords = {Reinforcement Learning, Production Lines, Simulation, Discrete Event Simulation-based Optimization, Performance, Markov Decision Problem},
abstract = {The increasing variety of products poses a challenge for efficient manufacturing on production lines due to resulting small batch sizes and frequent product changes, which lower the average overall effectiveness. Especially for industries such as the Fast Moving Consumer Goods (FMCG) industry that manufacture at high speed on production lines, it is mandatory to increase the performance of production lines in an economical way. Due to the complexity of such production lines, identifying efficient actions and combining them into economic improvement trajectories is challenging. There are numerous approaches based on different concepts, such as simulation-based heuristics, to solve these challenges. In other application areas, reinforcement learning has shown remarkable success in recent years, and first reinforcement learning-based approaches to this specific problem can be found. However, these approaches mostly focus on details instead of providing a holistic view of the possibilities to improve a production line or are limited in their practical application due to lack of integration of existing expert knowledge or limited quality of results. For this reason, this paper proposes a hierarchical reinforcement learning approach that combines discrete event simulation with a heuristically driven multi-agent system. Thus, the selection of the improvement strategy of the production line is performed by one agent, and the dedicated improvement of specific parameters is performed by specialized subordinate other agents. Through this hierarchical multi-agent system, on the one hand, the learning rate can be increased. On the other hand, by guiding the agents through a heuristic based on expert knowledge, the learning quality is increased.}
}
@article{VALVERDE2023106657,
title = {Causal reinforcement learning based on Bayesian networks applied to industrial settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106657},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106657},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008412},
author = {Gabriel Valverde and David Quesada and Pedro Larrañaga and Concha Bielza},
keywords = {Reinforcement learning, Bayesian networks, Causality, Parameter learning, Dynamic simulators, Ordinary differential equations},
abstract = {The increasing amount of real-time data collected from sensors in industrial environments has accelerated the application of machine learning in decision-making. Reinforcement learning (RL) is a powerful tool to find optimal policies for achieving a given goal. However, RL’s typical application is risky and insufficient in environments where actions can have irreversible consequences and require interpretability and fairness. While new trends in RL may provide guidance based on expert knowledge, they do not often consider uncertainty or include prior knowledge in the learning process. We propose a causal reinforcement learning alternative based on Bayesian networks (RLBNs) to address this challenge. The RLBN simultaneously models a policy and takes advantage of the joint distribution of the state and action space, reducing uncertainty in unknown situations. We propose a training algorithm for the network’s parameters and structure based on the reward function and likelihood of the effects and measurements taken. Our experiment with the CartPole benchmark and industrial fouling using ordinary differential equations (ODEs) demonstrates that RLBNs are interpretable, secure, flexible, and more robust than their competitors. Our contributions include a novel method that incorporates expert knowledge into the decision-making engine. It uses Bayesian networks with a predefined structure as a causal graph and a hybrid learning strategy that considers both likelihood and reward. This would avoid losing the virtues of the Bayesian network.}
}
@article{MROWCZYNSKA2022116035,
title = {A new fuzzy model of multi-criteria decision support based on Bayesian networks for the urban areas' decarbonization planning},
journal = {Energy Conversion and Management},
volume = {268},
pages = {116035},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116035},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422008251},
author = {M. Mrówczyńska and M. Skiba and A. Leśniak and A. Bazan-Krzywoszańska and F. Janowiec and M. Sztubecka and R. Grech and J.K. Kazak},
keywords = {Cities' sustainable development, Renewable energy sources, Energy policy scenarios, Bayesian network, Fuzzy Analytical Hierarchy Process, Geographic Information System},
abstract = {The study introduces a framework for forecasting and decision-making in multi-criteria processes and proposes their application in the decarbonization of urban areas. Optimizing the multi-criteria decision-making process is an integrated set of information-processing-decision activities in which actual data, expert knowledge using fuzzy inference rules, Geographic Information System, and Bayesian networks are combined. Using proposed tools leads to designing a new approach to improving the energy efficiency of cities and reducing CO2 emissions using renewable energy. The integration of modern computational methods leads to rational planning of environmentally friendly and energy-conscious smart cities by the provisions of the Fit for 55 packages. The effectiveness of the proposed approach has been demonstrated in the example of three scenarios considering different types of renewable energy sources that can be implemented in urban areas. The success probability of decarbonizing these areas was calculated for defined quarters of the city of Zielona Góra with different parameters. Thereby the usefulness of the method was confirmed. Significantly, the likelihood of a successful deployment of photovoltaics (PV) in urban areas was estimated at 55.25% and for heat pumps at 28.79%. The proposed method enables a clear interpretation of the results, which may be the basis for urban energy efficiency planning.}
}
@article{FANG2020103495,
title = {Human-in-the-loop optimization of wearable robots to reduce the human metabolic energy cost in physical movements},
journal = {Robotics and Autonomous Systems},
volume = {127},
pages = {103495},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103495},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019308036},
author = {Jing Fang and Yuan Yuan},
keywords = {Wearable robotics, Human–robot interaction, Human-in-the-loop design},
abstract = {Most designs of wearable robots are based on human biomechanical statistics, engineering experience or individual experiments. Despite great successes, few of them consider the human–robot integration and individual differences between users. Additionally, the design periods, cost and safety also need to be further improved. Learning from the natural driving mechanism of human body, we propose a general human-in-the-loop (HIL) optimization designing approach for this kind of wearable robots. Firstly, the human–robot coupling model of the personalized wearable robot and the human musculoskeletal model are established. Then, the Computed Muscle Control (CMC) tool embedded in software OpenSim and the Bayesian optimization used in machine learning are combined to find the optimal design scheme for the personalized wearable robots to reduce the human metabolic energy cost in specific physical movement. The HIL approach could not only optimize the control parameters of wearable robots, but also optimize their geometry, material and any other design parameters flexibly and effectively. An application example for the HIL approach is also provided to help designers better understand and use the HIL method proposed in this paper.}
}
@article{PRATIUSH2024252,
title = {Scientific exploration with expert knowledge (SEEK) in autonomous scanning probe microscopy with active learning††This manuscript has been co-authored by UT-Battelle, LLC, under contract DE-AC0500OR22725 with the US Department of Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).},
journal = {Digital Discovery},
volume = {4},
number = {1},
pages = {252-263},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00277f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24002249},
author = {Utkarsh Pratiush and Hiroshi Funakubo and Rama Vasudevan and Sergei V. Kalinin and Yongtao Liu},
abstract = {Microscopy plays a foundational role in materials science, biology, and nanotechnology, offering high-resolution imaging and detailed insights into properties at the nanoscale and atomic level. Microscopy automation via active machine learning approaches is a transformative advancement, offering increased efficiency, reproducibility, and the capability to perform complex experiments. Our previous work on autonomous experimentation with scanning probe microscopy (SPM) demonstrated an active learning framework using deep kernel learning (DKL) for structure–property relationship discovery. Here we extend this approach to a multi-stage decision process to incorporate prior knowledge and human interest into DKL-based workflows, we operationalize these workflows in SPM. By integrating expected rewards from structure libraries or spectroscopic features, we enhanced the exploration efficiency of autonomous microscopy, demonstrating more efficient and targeted exploration in autonomous microscopy. These methods can be seamlessly applied to other microscopy and imaging techniques. Furthermore, the concept can be adapted for general Bayesian optimization in material discovery across a broad range of autonomous experimental fields.}
}
@article{KUMAR2012687,
title = {Integrated modelling for Sustainability Appraisal for Urban River Corridor (re)-development},
journal = {Procedia Environmental Sciences},
volume = {13},
pages = {687-697},
year = {2012},
note = {18th Biennial ISEM Conference on Ecological Modelling for Global Change and Coupled Human and Natural System},
issn = {1878-0296},
doi = {https://doi.org/10.1016/j.proenv.2012.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S1878029612000631},
author = {Vikas Kumar and J.R. Rouquette and D.N. Lerner},
keywords = {Integrated Modelling, Sustainability Appraisal, Urban River Corridor, Bayesian Network},
abstract = {Sustainability Appraisal (SA) is mandatory under the relevant legislation of UK (DCLG, 2008a) and applies to the preparation of Regional Spatial Strategies, Development Plans and Supplementary Planning documents. SA is a complex task that involves integration of social, environmental and economic considerations into formal plans and often requires trade-offs between multiple stakeholders that may not easily be brought to consensus. Classical assessment can facilitate discussion, but these can only partially inform decision makers as many important aspects of sustainability are abstract and not quantifiable. Such abstract criteria however can be modelled using a Bayesian Network (BN), combining expert opinions, empirical evidence and other information such as model simulation, survey etc. This paper discusses the work of the URSULA project at the University of Sheffield, in which a participative and integrative approach to urban river corridor development, incorporating the principal of sustainability was used. The project used a case study site in Sheffield, UK, and three alternative scenarios were developed, incorporating a number of possible riverside design features. Scenarios were fully designed and visualised using a variety of different media and a sustainability appraisal was undertaken using a broad range of environmental, social and economic indicators. Experts’ assessment logics were captured through mind mapping and further expert elicitation was used to develop an integrated model for SA. The BN approach allows model complexity to be reduced to a level appropriate for an assessment process, whilst still taking complex system interactions implicitly into account. The integrated SA model is being used to develop better design by optimising different design elements in order to deliver an optimum (re)-development plan.}
}
@article{ADAMS2024697,
title = {Human-in-the-loop for Bayesian autonomous materials phase mapping},
journal = {Matter},
volume = {7},
number = {2},
pages = {697-709},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2590238524000067},
author = {Felix Adams and Austin McDannald and Ichiro Takeuchi and A. Gilad Kusne},
keywords = {human-in-the-loop, autonomous, X-ray diffraction, machine learning, phase mapping},
abstract = {Summary
Autonomous experimentation combines machine learning and laboratory automation to select and perform experiments toward user goals. Accordingly, materials optimization using autonomous experimentation requires fewer experiments and less time than Edisonian studies. Integrating knowledge from theory, simulations, literature, and human intuition into the machine learning model can further increase this advantage. We present a set of methods for integrating human input into an autonomous materials exploration campaign for composition-structure phase mapping. The methods are demonstrated on X-ray diffraction data collected from a thin-film ternary combinatorial library. During the campaign, the user can provide input by indicating potential phase boundaries or phase regions with their uncertainty or indicate regions of interest. The input is then integrated through probabilistic priors, resulting in a probabilistic distribution over potential phase maps given the data, model, and human input. We demonstrate an improvement in phase-mapping performance given appropriate human input.}
}
@article{DIFIORE2024107302,
title = {Physics-aware multifidelity Bayesian optimization: A generalized formulation},
journal = {Computers & Structures},
volume = {296},
pages = {107302},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107302},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000312},
author = {Francesco {Di Fiore} and Laura Mainini},
keywords = {Domain-aware efficient optimization, Multifidelity Bayesian optimization, Physics-informed machine learning, Structural health monitoring, Simulation-driven design},
abstract = {The adoption of high-fidelity models for many-query optimization problems is majorly limited by the significant computational cost required for their evaluation at every query. Multifidelity Bayesian methods (MFBO) allow to include costly high-fidelity responses for a sub-selection of queries only, and use fast lower-fidelity models to accelerate the optimization process. State-of-the-art methods rely on a purely data-driven search and do not include explicit information about the physical context. This paper acknowledges that prior knowledge about the physical domains of engineering problems can be leveraged to accelerate these data-driven searches, and proposes a generalized formulation for MFBO to embed a form of domain awareness during the optimization procedure. In particular, we formalize a bias as a multifidelity acquisition function that captures the physical structure of the domain. This permits to partially alleviate the data-driven search from learning the domain properties on-the-fly, and sensitively enhances the management of multiple sources of information. The method allows to efficiently include high-fidelity simulations to guide the optimization search while containing the overall computational expense. Our physics-aware multifidelity Bayesian optimization is presented and illustrated for two classes of optimization problems frequently met in science and engineering, namely design optimization and health monitoring problems.}
}
@article{MAIER2022117540,
title = {Autonomous and data-efficient optimization of turning processes using expert knowledge and transfer learning},
journal = {Journal of Materials Processing Technology},
volume = {303},
pages = {117540},
year = {2022},
issn = {0924-0136},
doi = {https://doi.org/10.1016/j.jmatprotec.2022.117540},
url = {https://www.sciencedirect.com/science/article/pii/S0924013622000528},
author = {Markus Maier and Hannes Kunstmann and Ruben Zwicker and Alisa Rupenyan and Konrad Wegener},
keywords = {Turning, Transfer learning, Expert knowledge, Process optimization, Bayesian optimization, Gaussian process models, Machining},
abstract = {Process parameters in machining are predominantly selected by following manual tuning procedures. Using data from the system and dedicated performance indicators combined with learning-based approaches enables automating these procedures while reducing the costs of the machining process. This study investigates efficient data-driven approaches for autonomous parameter selection in turning. The number of experimental trials for finding optimal process parameters is reduced by incorporating expert knowledge and transferring knowledge between different tasks. The turning process costs are modeled using Gaussian process models, and the selection of informative experiments is achieved by Bayesian optimization. In this study, all tested methods using expert knowledge or transfer of knowledge reduced the number of experiments by at least 40% compared to a standard approach for parameter selection based on Bayesian optimization without expert knowledge, confirming the efficiency of the applied methods.}
}
@article{CHEN2020102703,
title = {Systematizing heterogeneous expert knowledge, scenarios and goals via a goal-reasoning artificial intelligence agent for democratic urban land use planning},
journal = {Cities},
volume = {101},
pages = {102703},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102703},
url = {https://www.sciencedirect.com/science/article/pii/S0264275119312466},
author = {Weizhen Chen and Liang Zhao and Qi Kang and Fan Di},
keywords = {Land use planning, Goal reasoning, Artificial intelligence, Markov decision processes, Reinforcement learning, Multicriteria decision analysis},
abstract = {The tasks of democratic urban land use planning, as subjective-objective combined decision-making efforts that require considerable time and energy, have heretofore been accomplished mainly through deep human thought or by voting. In this paper, we introduce a goal-reasoning artificial intelligence (AI) agent that can assist with these tasks by combining traditional scenario planning, multicriteria decision analysis (MCDA) with a novel goal-oriented Monte Carlo tree search (G-MCTS) method. G-MCTS conducts goal-oriented searches to meet the needs of heterogeneous goals and provide the best land use solutions. We evaluated this method on a real-world planning case, and the results show that 1) the goal-reasoning AI agent is good at performing complex goal reasoning tasks with many heterogeneous expert knowledge; 2) different human planning manuscripts could be integrated into a better solution via a goal-reasoning AI agent; and 3) the goal-reasoning AI agent has the potential to make comprehensive decisions during a democratic political agenda. We conclude that the goal-reasoning AI agent, via an improved reinforcement learning (RL) method of G-MCTS, provides vast potential for assisting in subjective-objective combined urban land use planning and many other similar fields by weighing heterogeneous goals, reproducing human inspiration, and acting as a reflexive sociotechnical system.}
}
@article{BOUMA20114497,
title = {Assessing the value of Earth Observation for managing coral reefs: An example from the Great Barrier Reef},
journal = {Science of The Total Environment},
volume = {409},
number = {21},
pages = {4497-4503},
year = {2011},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2011.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0048969711007418},
author = {Jetske A. Bouma and Onno Kuik and Arnold G. Dekker},
keywords = {Value of information, Earth Observation, Cost-effectiveness analysis, Marine water quality management, Bayesian decision theory, Coral reef protection},
abstract = {The Integrated Global Observing Strategy (IGOS, 2003) argues that further investments in Earth Observation information are required to improve coral reef protection worldwide. The IGOS Strategy does not specify what levels of investments are needed nor does it quantify the benefits associated with better-protected reefs. Evaluating costs and benefits is important for determining optimal investment levels and for convincing policy-makers that investments are required indeed. Few studies have quantitatively assessed the economic benefits of Earth Observation information or evaluated the economic value of information for environmental management. This paper uses an expert elicitation approach based on Bayesian Decision Theory to estimate the possible contribution of global Earth Observation to the management of the Great Barrier Reef. The Great Barrier Reef including its lagoon is a World Heritage Area affected by anthropogenic changes in land-use as well as climate change resulting in increased flows of sediments, nutrients and carbon to the GBR lagoon. Since European settlement, nutrient and sediment loads having increased 5–10 times and the change in water quality is causing damages to the reef. Earth Observation information from ocean and coastal color satellite sensors can provide spatially and temporally dense information on sediment flows. We hypothesize that Earth Observation improves decision-making by enabling better-targeted run-off reduction measures and we assess the benefits (cost savings) of this improved targeting by optimizing run-off reductions under different states of the world. The analysis suggests that the benefits of Earth Observation can indeed be substantial, depending on the perceived accuracy of the information and on the prior beliefs of decision-makers. The results indicate that increasing informational accuracy is the most effective way for developers of Earth Observation information to increase the added value of Earth Observation for managing coral reefs.}
}
@article{LIANG2024110033,
title = {Expert knowledge data-driven based actor–critic reinforcement learning framework to solve computationally expensive unit commitment problems with uncertain wind energy},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {159},
pages = {110033},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2024.110033},
url = {https://www.sciencedirect.com/science/article/pii/S0142061524002540},
author = {Huijun Liang and Chenhao Lin and Aokang Pang},
keywords = {Unit commitment, Reinforcement learning, Knowledge data-driven, Surrogate model, Meta-heuristic algorithm},
abstract = {With the expansion of power grid, unaffordable computational cost and time will pose serious challenges of time-efficient scheduling in unit commitment problem (UCP). However, existing optimization methods, i.e., mathematical programming methods and meta-heuristic algorithms, are powerless and time-consuming to handle computationally expensive UCP (CEUCP). Thus, reinforcement learning methods with strong inference and time-saving performances are motivated to solve the computationally expensive challenges in tackling CEUCPs. In this paper, a novel expert knowledge data-driven based actor–critic (AC) reinforcement learning methodology is proposed for solving CEUCPs. Specifically, in the proposed AC reinforcement learning methodology, expert knowledge, data-driven surrogate model, and improved meta-heuristic algorithm are integrated for further performance enhancement. Firstly, a novel action selection mechanism (based on the expert knowledge of thermal units characteristic) is integrated into AC to improve the efficiency of network training. Secondly, an improved extreme learning machine (ELM) data-driven surrogate model is proposed to build reward function in AC. In detail, original cost function in reward is replaced by a lightweight ELM model. Shape distance is integrated into ELM for enhancing accuracy. Finally, original marine predators algorithm (MPA) is improved for obtaining optimal dispatching decisions and rewards of AC method quickly and correctly. Original search pattern is replaced by quantum based representation for boosting convergence. The excellent performances of the proposed AC framework are verified by simulations of 10-units, 100-units, and 100-units with wind energy test systems.}
}
@article{YU20121005,
title = {Iterative learning belief rule-base inference methodology using evidential reasoning for delayed coking unit},
journal = {Control Engineering Practice},
volume = {20},
number = {10},
pages = {1005-1015},
year = {2012},
note = {4th Symposium on Advanced Control of Industrial Processes (ADCONIP)},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2012.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S096706611200041X},
author = {Xiaodong Yu and Dexian Huang and Yongheng Jiang and Yihui Jin},
keywords = {Belief rule-base, Evidential reasoning, Expert system, Iterative learning, Feedforward compensation, Delayed coking unit},
abstract = {The belief rule-base inference methodology using evidential reasoning (RIMER) approach has been proved to be an effective extension of traditional rule-based expert systems and a powerful tool for representing more complicated causal relationships using different types of information with uncertainties. With a predetermined structure of the initial belief rule-base (BRB), the RIMER approach requires the assignment of some system parameters including rule weights, attribute weights, and belief degrees using experts’ knowledge. Although some updating algorithms were proposed to solve this problem, it is still difficult to find an optimal compact BRB. In this paper, a novel updating algorithm is proposed based on iterative learning strategy for delayed coking unit (DCU), which contains both continuous and discrete characteristics. Daily DCU operations under different conditions are modeled by a BRB, which is then updated using iterative learning methodology, based on a novel statistical utility for every belief rule. Compared with the other learning algorithms, our methodology can lead to a more optimal compact final BRB. With the help of this expert system, a feedforward compensation strategy is introduced to eliminate the disturbance caused by the drum-switching operations. The advantages of this approach are demonstrated on the UniSim™ Operations Suite platform through the developed DCU operation expert system modeled and optimized from a real oil refinery.}
}
@incollection{WALLACH2019275,
title = {Chapter 8 - Parameter Estimation With Bayesian Methods},
editor = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
booktitle = {Working with Dynamic Crop Models (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {275-309},
year = {2019},
isbn = {978-0-12-811756-9},
doi = {https://doi.org/10.1016/B978-0-12-811756-9.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117569000083},
author = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
keywords = {Bayes, MCMC, Parameter estimation, Posterior, Prior, Uncertainty},
abstract = {Bayesian methods are becoming increasingly popular for estimating parameters of complex mathematical models because the Bayesian approach provides a coherent framework for dealing with uncertainty. To start with, the principle is a prior probability distribution of the model parameters describing our belief about the parameter values before we use the set of measurements. The Bayesian methods then tell us how to update this belief using the measurements to give the posterior parameter density. In the Bayesian approach, the parameters are defined as random variables and the prior and posterior parameter distributions represent our belief about parameter values before and after using observed data to improve estimates. This approach has several advantages: (i) parameters can be estimated from different types of information (data, literature, expert knowledge); (ii) the posterior probability distribution can be used to implement uncertainty analysis methods; and (iii) the posterior probability distribution can be used for optimizing decisions in the face of uncertainty. This chapter presents the basic principles of the Bayesian approach and describes several algorithms to calculate posterior parameter distributions. These algorithms are illustrated in several applications on yield and soil carbon estimation.}
}
@article{ZHANG2020103493,
title = {TBM performance prediction with Bayesian optimization and automated machine learning},
journal = {Tunnelling and Underground Space Technology},
volume = {103},
pages = {103493},
year = {2020},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103493},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820304478},
author = {Qianli Zhang and Weifei Hu and Zhenyu Liu and Jianrong Tan},
keywords = {Performance prediction, Bayesian optimization, Machine learning},
abstract = {Accurately predicting the performance of a tunnel boring machine (TBM) is important to safe and efficient tunneling. The application of machine learning algorithms to TBM performance prediction creates several challenges. Such prediction is a nontrivial task involving procedures such as data preprocessing, selection of a machine learning algorithm and optimization of the related hyperparameters. The demand for expert knowledge has restricted the application of machine learning methods to TBM performance prediction, and it is meaningful to study predicting TBM performance automatically. In this paper, we explore three approaches to TBM performance prediction using Bayesian optimization and automated machine learning (AutoML). In the first study, Bayesian optimization is used to determine the optimal hyperparameters of various machine learning algorithms, including support vector regression (SVR), decision tree, bagging tree, random forest and AdaBoost. We attain the minimum mean squared error (MSE) values of 3.135×10-2 and 3.177×10-2 for a decision tree and SVR, respectively. In the second approach called the neural architecture search (NAS), the optimal combination of architecture, hyperparameters and the training procedure of an artificial neural network is found in a single operation. We obtain the optimal results of 3.514×10-2 and 3.237×10-2 if complete and simplified NAS are used, respectively. In the third method, the best combination of a data preprocessing method, a machine learning model and the related hyperparameters is found, and an optimal MSE value of 3.148×10-2 is obtained using AutoML. In all three studies, we obtain state-of-the-art prediction results that are superior to a previous best prediction result of 3.500×10-2. The prediction results prove that Bayesian optimization and AutoML are powerful tools that can not only effectively predict TBM performance but also reduce the demand for expert knowledge of machine learning.}
}
@article{XUE2021897,
title = {A novel fuzzy Bayesian network-based MADM model for offshore wind turbine selection in busy waterways: An application to a case in China},
journal = {Renewable Energy},
volume = {172},
pages = {897-917},
year = {2021},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.03.084},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121004407},
author = {Jie Xue and Tsz Leung Yip and Bing Wu and Chaozhong Wu and P.H.A.J.M. {van Gelder}},
keywords = {Wind energy, Offshore wind turbine (OWT), Multiple-attribute decision-making (MADM), Principal component analysis (PCA), Fuzzy bayesian network, Marine traffic safety},
abstract = {Offshore wind power is an important renewable energy source and plays an essential role in optimizing the energy structure worldwide. Simultaneously, offshore wind turbine (OWT) selection is a complicated process since it concerning various variables and optimization scenarios. In this paper, a novel fuzzy Bayesian network-based model for multiple-attribute decision-making (MADM) is proposed. First of all, a three-layer decision-making framework for OWT selection is established through systematically combing previous studies, expert knowledge, and the principal component analysis (PCA) results by treating the wind turbine parameters, wind turbine economy, wind turbine reliability, and navigation safety as the attributes, and the corresponding 11 influencing factors are identified and quantified. Moreover, a triangular fuzzy number is introduced to fuzzify each influencing factor, and the belief degree for different linguistic variables corresponding to the specific influencing factor is employed in the fuzzy IF-THEN rule system. Then, the belief rule base is transformed into the Bayesian network as the conditional probability tables (CPTs), which can directly express the influence relationship of various factors and realize the integration of various influence factors to obtain the optimal scheme. Finally, the proposed model is validated by taking a case study in busy waterways in the Eastern China Sea as an example. This research provides an intuitive, feasible, and practical way for OWT selection.}
}
@article{OUELLET2021105138,
title = {Improve performance and robustness of knowledge-based FUZZY LOGIC habitat models},
journal = {Environmental Modelling & Software},
volume = {144},
pages = {105138},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105138},
url = {https://www.sciencedirect.com/science/article/pii/S136481522100181X},
author = {Valérie Ouellet and Julien Mocq and Salah-Eddine {El Adlouni} and Stefan Krause},
keywords = {Fuzzy logic, Critic, Expert knowledge, Model optimization, Decision framework},
abstract = {Previous criticisms of knowledge-based fuzzy logic modelling have identified some of its limitations and revealed weaknesses regarding the development of fuzzy sets, the integration of expert knowledge, and the outcomes of different defuzzification processes. We show here how expert disagreement and fuzzy logic mechanisms associated with the rule development and combinations can positively or adversely affect model performance and the interpretation of results. We highlight how expert disagreement can induce uncertainty into model outputs when defining fuzzy sets and selecting a defuzzification method. We present a framework to account for sources of error and bias and improve the performance and robustness of knowledge-based fuzzy logic models. We recommend to 1) provide clear/unambiguous instructions on model development, processes and objectives, including the definition of input variables and fuzzy sets, 2) incorporate the disagreement among experts into the analysis, 3) increase the use of short rules and the OR operator to reduce complexity, and 4) improve model performance and robustness by using narrow fuzzy sets for extreme values of input variables to expand the universe of discourse adequately. Our framework is focused on fuzzy logic models but can be applied to all knowledge-based models that require expert judgment, including expert systems, decision trees and (fuzzy) Bayesian inference systems.}
}
@article{SOUSA2025102892,
title = {Human-in-the-loop Multi-objective Bayesian Optimization for Directed Energy Deposition with in-situ monitoring},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {92},
pages = {102892},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102892},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524001790},
author = {João Sousa and Armando Sousa and Frank Brueckner and Luís Paulo Reis and Ana Reis},
keywords = {Additive manufacturing, Digital twin, Robot operating system, Surrogate modeling, Process optimization},
abstract = {Directed Energy Deposition (DED) is a free-form metal additive manufacturing process characterized as toolless, flexible, and energy-efficient compared to traditional processes. However, it is a complex system with a highly dynamic nature that presents challenges for modeling and optimization due to its multiphysics and multiscale characteristics. Additionally, multiple factors such as different machine setups and materials require extensive testing through single-track depositions, which can be time and resource-intensive. Single-track experiments are the foundation for establishing optimal initial parameters and comprehensively characterizing bead geometry, ensuring the accuracy and efficiency of computer-aided design and process quality validation. We digitized a DED setup using the Robot Operating System (ROS 2) and employed a thermal camera for real-time monitoring and evaluation to streamline the experimentation process. With the laser power and velocity as inputs, we optimized the dimensions and stability of the melt pool and evaluated different objective functions and approaches using a Response Surface Model (RSM). The three-objective approach achieved better rewards in all iterations and, when implemented in a real setup, allowed to reduce the number of experiments and shorten setup time. Our approach can minimize waste, increase the quality and reliability of DED, and enhance and simplify human-process interaction by leveraging the collaboration between human knowledge and model predictions.}
}
@article{WANG2024102811,
title = {Clinical knowledge-guided deep reinforcement learning for sepsis antibiotic dosing recommendations},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102811},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102811},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000538},
author = {Yuan Wang and Anqi Liu and Jucheng Yang and Lin Wang and Ning Xiong and Yisong Cheng and Qin Wu},
keywords = {Deep reinforcement learning, Sepsis, Antibiotic, Clinical},
abstract = {Sepsis is the third leading cause of death worldwide. Antibiotics are an important component in the treatment of sepsis. The use of antibiotics is currently facing the challenge of increasing antibiotic resistance (Evans et al., 2021). Sepsis medication prediction can be modeled as a Markov decision process, but existing methods fail to integrate with medical knowledge, making the decision process potentially deviate from medical common sense and leading to underperformance. (Wang et al., 2021). In this paper, we use Deep Q-Network (DQN) to construct a Sepsis Anti-infection DQN (SAI-DQN) model to address the challenge of determining the optimal combination and duration of antibiotics in sepsis treatment. By setting sepsis clinical knowledge as reward functions to guide DQN complying with medical guidelines, we formed personalized treatment recommendations for antibiotic combinations. The results showed that our model had a higher average value for decision-making than clinical decisions. For the test set of patients, our model predicts that 79.07% of patients will achieve a favorable prognosis with the recommended combination of antibiotics. By statistically analyzing decision trajectories and drug action selection, our model was able to provide reasonable medication recommendations that comply with clinical practices. Our model was able to improve patient outcomes by recommending appropriate antibiotic combinations in line with certain clinical knowledge.}
}
@article{GUAYHOTTIN2025113039,
title = {Robust prior-biased acquisition function for human-in-the-loop Bayesian optimization},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113039},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113039},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000863},
author = {Rose Guay-Hottin and Lison Kardassevitch and Hugo Pham and Guillaume Lajoie and Marco Bonizzato},
keywords = {Bayesian optimization, Domain knowledge integration, Prior-weighted acquisition function, Region of interest, Human-in-the-loop},
abstract = {In diverse fields of application, Bayesian Optimization (BO) has been proposed to find the optimum of black-box functions, surpassing human-driven searches. BO’s appeal lies in its data efficiency, making it suitable for optimizing costly-to-evaluate functions without requiring extensive training data. While BO can perform well in closed-loop, domain experts frequently have hypotheses about which parameter combinations are more likely to yield optimal results. Hence, for BO to be truly relevant and adopted by practitioners, such prior knowledge needs to be efficiently and seamlessly integrated into the optimization framework. Some methods were recently developed to address this challenge, but they suffer from robustness issues when provided erroneous insight. Building on the idea of element-wise prior-weighted acquisition function, we propose to use a fixed-weight effective prior that distills expert user knowledge with minimal computational cost. Comprehensive investigation across diverse task conditions and prior quality levels revealed that our method, α-πBO, surpasses Vanilla BO when provided with insights of good quality while maintaining robustness against misleading information. Moreover, unlike other methods, α-πBO typically requires no hyperparameter tuning, largely simplifying its implementation in diverse tasks.}
}
@article{AMMANN20252428,
title = {Automated Knowledge Graph Learning in Industrial Processes},
journal = {Procedia Computer Science},
volume = {253},
pages = {2428-2437},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003114},
author = {Lolita Ammann and Jorge Martinez-Gil and Michael Mayr and Georgios C. Chasparis},
keywords = {Knowledge graphs, Graph learning, Knowledge engineering, Granger causality},
abstract = {Industrial processes generate vast amounts of time series data, yet extracting meaningful relationships and insights remains challenging. Knowledge graphs allow to uniquely store and present information, enabling novel capabilities for identifying clusters, temporal relationships, and hidden connections. Building a knowledge graph either requires considerable manual effort and deep expert knowledge of the process or computationally intensive machine learning that provides little transparency in how process structures are identified. This paper introduces a framework for automated knowledge graph learning from time series data, specifically tailored for industrial applications. Moving beyond traditional black-box models process knowledge retrieved from various transparent analytical approaches are amalgamated into a knowledge graph. Similarity analysis is employed to explore simultaneous process relationships, while Granger-causality analysis provides insights into temporally disjoint parameter interactions and causal dependencies between parameters. To illustrate the practical utility of our approach, we present a motivating use case demonstrating the benefits of our framework in a real-world industrial scenario. The user is provided with an intuitive visualization of the process and information that can be enriched with expert knowledge improving decision-making, process optimization, and knowledge discovery.}
}
@article{DEVOL2025118788,
title = {Methodology for the automated selection of time-frequency representations},
journal = {Journal of Sound and Vibration},
volume = {596},
pages = {118788},
year = {2025},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2024.118788},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X24005509},
author = {Nathaniel DeVol and Christopher Saldaña and Katherine Fu},
keywords = {Time-frequency analysis, Bayesian optimization, Machine learning},
abstract = {Data preprocessing is a key step in extracting useful information from sound and vibration data and often involves selecting a time-frequency representation. No single time-frequency representation is always optimal, and no standard method exists for selecting the appropriate time-frequency representation, so selecting the time-frequency representation requires expert knowledge and is susceptible to human bias. To address this, this work introduces a methodology to automate the selection of a time-frequency representation for a dataset using only a subset of the healthy, or normal, class of data. To select the parameters for each type of time-frequency representation, Bayesian optimization is used. With a candidate from each type of time-frequency representation, the average similarity is used to select the final candidate. Additionally, the use of multiple time-frequency representations within a single model is explored. Because there is currently no objective method to compare the selected time frequency representations against, the proposed methodology is evaluated in two case studies. In the case studies, the time frequency representations are used as inputs to a simple convolutional neural network that achieved 100% accuracy in classifying bearing faults and 94% accuracy in classifying the contact tip to workpiece distance in wire arc additive manufacturing. Additionally, the proposed methodology presents a 75% and 94% reduction in the data size for the two case studies. This offers further benefits for reducing costs of data transmission and storage in modern digital manufacturing architectures.}
}
@article{LASH2024114304,
title = {HEX: Human-in-the-loop explainability via deep reinforcement learning},
journal = {Decision Support Systems},
volume = {187},
pages = {114304},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114304},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624001374},
author = {Michael T. Lash},
keywords = {Explainability, Interpretability, Human-in-the-loop, Deep reinforcement learning, Machine learning, Behavioral machine learning, Decision support},
abstract = {The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider’s preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider’s trust and tendency to rely on trusted features.}
}
@article{DIAZSECADES2023114747,
title = {Waste heat recovery system for marine engines optimized through a preference learning rank function embedded into a Bayesian optimizer},
journal = {Ocean Engineering},
volume = {281},
pages = {114747},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114747},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011319},
author = {Luis Alfonso Díaz-Secades and R. González and N. Rivera and Elena Montañés and José Ramón Quevedo},
keywords = {Waste heat recovery, Marine diesel engine, Desalination, Steam rankine cycle, Organic rankine cycle, Thermoelectric generators, Preference learning, Bayesian Optimization},
abstract = {Waste heat recovery is a proven process to improve efficiency on engines and meets current necessities of the maritime industry. Since January 1, 2023, already built vessels must meet the energy efficiency indicators known as EEXI and CII. Aiming to reduce fuel consumption and mitigate pollution emissions, a novel waste heat recovery system composed of steam Rankine cycle, organic Rankine cycle, thermoelectric harvesters and desalination is presented. High, medium and low-grade waste heat from exhaust gas, jacket water, lubricating oil and engine block radiation are targeted for recovery. Performance assessment of each subsystem when implemented on a real case study 6-cylinder medium speed marine engine is analyzed. The equivalent electricity production concept was used for the assessment of the desalination subsystem. The proposed system effectively recovers waste energy, offering economic benefits, reducing pollution and satisfying the daily demand of fresh water. Also, optimal states of the waste heat recovery are provided via Bayesian optimization, which requires an evaluation function for the system to be optimized. However, this function is not available and cannot be straightforwardly established, since the quality of waste heat recovery depends on some indicators with a trade-off among them. Hence, a preference learning procedure that exploits expert knowledge is proposed to induce a function of this kind from those indicators in order to be embedded into the Bayesian optimization procedure afterward. Applied to the case study engine, a fuel consumption reduction of 15.04% is achieved. Fuel savings lead to an improvement in energy efficiency indicators, achieving a reduction of 6.98% on the EEXI and a 13.85% on the CII.}
}
@article{PARK20124240,
title = {Evolutionary attribute ordering in Bayesian networks for predicting the metabolic syndrome},
journal = {Expert Systems with Applications},
volume = {39},
number = {4},
pages = {4240-4249},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411014333},
author = {Han-Saem Park and Sung-Bae Cho},
keywords = {Bayesian network, Prognostic modeling, Attribute ordering optimization, Metabolic syndrome},
abstract = {The metabolic syndrome is a set of risk factors that include abdominal obesity, insulin resistance, dyslipidemia and hypertension. It has affected around 25% of adults in the US and become a serious problem in Asian countries recently due to the change in dietary habit and life style. On the other hand, Bayesian networks that are the models to solve the problems of uncertainty provide a robust and transparent formalism for probabilistic modeling, so they have been used as a method for diagnostic or prognostic model in medical domain. Since the K2 algorithm, a well-known algorithm for Bayesian networks structure learning, is influenced by an input order of the attributes, an optimization of BN attribute ordering has been studied as a research issue. This paper proposes a novel ordering optimization method using a genetic algorithm based on medical expert knowledge in order to solve this problem. For experiments, we use the dataset examined twice in 1993 and 1995 in Yonchon County of Korea. It has 18 attributes of 1193 subjects participated in both surveys. Using this dataset, we make the prognostic model of the metabolic syndrome using Bayesian networks with an optimized ordering by evolutionary approach. Through an ordering optimization, the prognostic model of higher performance is constructed, and the optimized Bayesian network model by the proposed method outperforms the conventional BN model as well as neural networks and k-nearest neighbors. Finally, we present the application program using the prognostic model of the metabolic syndrome in order to show the usefulness of the proposed method.}
}
@article{LI2021107850,
title = {A knowledge-guided and data-driven method for building HVAC systems fault diagnosis},
journal = {Building and Environment},
volume = {198},
pages = {107850},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321002560},
author = {Tingting Li and Yang Zhao and Chaobo Zhang and Jing Luo and Xuejun Zhang},
keywords = {Diagnostic bayesian networks, Genetic algorithm, Fault diagnosis, Fault interpretation, HVAC systems, Energy conservation},
abstract = {Fault diagnosis is crucial for energy conversation of building HVAC systems. Generally, knowledge-driven fault diagnosis methods have good interpretability, whereas data-driven fault diagnosis methods have high diagnosis accuracy. With the aim of integrating the advantages of both types of methods, this paper proposes a knowledge-guided and data-driven fault diagnosis method. The proposed method develops a diagnostic Bayesian network (DBN) based on both expert knowledge and operational data. A probabilistic framework is developed for determining the prior DBN structures based on expert knowledge. An improved genetic algorithm-based approach is raised for further optimizing the DBN structures based on the operational data. Local casual graphs are generated from the DBN for visually interpreting the fault action mechanisms. Experts can evaluate the reliability of the diagnosis results using the local casual graphs, and then make reliable decisions. The proposed method is evaluated using the experimental data from the ASHARE Project 1312-RP. The results show that the performance of the proposed method is promising. Six typical faults are interpreted by the local casual graphs. It is demonstrated that the local casual graphs can effectively reveal the action mechanisms behind the six faults.}
}
@article{CHEN2025107318,
title = {HCPI-HRL: Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning},
journal = {Neural Networks},
volume = {187},
pages = {107318},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107318},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025001972},
author = {Bin Chen and Zehong Cao and Wolfgang Mayer and Markus Stumptner and Ryszard Kowalczyk},
keywords = {Deep reinforcement learning, Hierarchical reinforcement learning, Causal inference, Subgoal discovery},
abstract = {The dependency on extensive expert knowledge for defining subgoals in hierarchical reinforcement learning (HRL) restricts the training efficiency and adaptability of HRL agents in complex, dynamic environments. Inspired by human-guided causal discovery skills, we proposed a novel method, Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning (HCPI-HRL), designed to infer diverse, effective subgoal structures as intrinsic rewards and incorporate critical objects from dynamic environmental states using stable causal relationships. The HCPI-HRL method is supposed to guide an agent’s exploration direction and promote the reuse of learned subgoal structures across different tasks. Our designed HCPI-HRL comprises two levels: the top level operates as a meta controller, assigning subgoals discovered based on human-driven causal critical object perception and causal structure inference; the bottom level employs the Proximal Policy Optimisation (PPO) algorithm to accomplish the assigned subgoals. Experiments conducted across discrete and continuous control environments demonstrated that HCPI-HRL outperforms benchmark methods such as hierarchical and adjacency PPO in terms of training efficiency, exploration capability, and transferability. Our research extends the potential of HRL methods incorporating human-guided causal modelling to infer the effective relationships across subgoals, enhancing the agent’s capability to learn efficient policies in dynamic environments with sparse reward signals.}
}
@article{BLONDET2019289,
title = {A knowledge-based system for numerical design of experiments processes in mechanical engineering},
journal = {Expert Systems with Applications},
volume = {122},
pages = {289-302},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300120},
author = {Gaëtan Blondet and Julien Le Duigou and Nassim Boudaoud},
keywords = {Knowledge based system, Numerical design of experiments, Bayesian network},
abstract = {This paper describes a specific knowledge-based system (KBS) to assist designers in configuring numerical design of experiments (NDoE) processes efficiently. NDoE processes are applied in product design to improve the quality of product, by taking into account variabilities and uncertainties. NDoE processes are defined by various and complex methodologies to achieve several objectives, as optimization, surrogate modeling or sensitivity analysis. On the other hand, NDoE processes may demand huge computing resources to execute hundreds simulations, and also advanced expert knowledge to set the best configuration amongst numerous possibilities. Designers aim to obtain most useful results with a minimal computational cost as soon as possible. Thus, the configuration step must be as fast as possible, and it must lead to an efficient combination of complex methods, algorithms and hyper-parameters, to obtain valuable information on the product. The proposed KBS and its inference engine, a bayesian network, is detailed and applied to a product developed by automotive industry. The KBS propose new efficient configurations to achieve designers' goal. This application shorten the configuration step of the NDoE process, and enables designers to use more complex methods. It also allows designers to capitalize knowledge and learn from each past NDoE process.}
}
@article{JAFRASTEH2021104674,
title = {Objective functions from Bayesian optimization to locate additional drillholes},
journal = {Computers & Geosciences},
volume = {147},
pages = {104674},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104674},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420306464},
author = {Bahram Jafrasteh and Alberto Suárez},
keywords = {Bayesian optimization, Cross-validation, Objective function, Rank comparison, Gaussian process, Infill drillhole},
abstract = {The key available information to choose new locations for drilling are the estimated ore grade values and the corresponding uncertainties at the tentative locations. These pieces of information are combined to generate a single objective function. The mathematical form of the objective function should reflect the effect of these values and their relative importance. Traditional objective function use multiplication of these parameters by different powering values. In this study, we develop two novel objective functions from the Bayesian optimization: the probability of improvement (PI), and the expected improvement (EI). These two objective functions seek new drillholes while considering the effect of the used value and their relative importance. Therefore, they can provide a trade-off between exploration and exploitation. All the objective functions have adjustable parameters. These parameters are typically tuned using expert knowledge or heuristic rules. Here, a statistical method based on cross-validation is proposed to adjust the parameters of the traditional and novel objective functions. The performance of the novel objective functions is validated against other ones using a distance based ranking method, in a phosphate deposit. The obtained results demonstrate the robustness of the EI and PI, the newly introduced objective functions from the Bayesian optimization framework.}
}
@article{LIU2025110279,
title = {Context-aware inverse reinforcement learning for modeling individuals’ daily activity schedules},
journal = {Engineering Applications of Artificial Intelligence},
volume = {146},
pages = {110279},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110279},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625002799},
author = {Dongjie Liu and Dawei Li and Kun Gao and Yuchen Song and Zijie Zhou},
keywords = {Travel demand modeling, Activity generation, Artificial Intelligence, Activity-based models, Inverse reinforcement learning},
abstract = {Understanding individual and crowd dynamics in urban environments is critical for numerous applications, such as urban planning, traffic forecasting, and location-based services. Therefore, accurately modeling individuals' daily activity schedules is essential. Traditional methods, like utility-based and rule-based approaches, rely on expert knowledge and presumed model structures. While machine learning methods offer flexibility, they often ignore explicit behavioral mechanisms, particularly comprehensive discussion and integration of context related to individuals' daily travel. To address these, we propose a framework that integrates travel context with deep Inverse Reinforcement Learning (IRL), learning temporal patterns from sociodemographics, start time and duration of the current activity, travel modes, and land use. Specifically, individuals' activity schedules are initially formulated as a Markov Decision Process to simulate travelers’ sequential decision-making processes, laying the groundwork for the IRL framework; Then, a context-aware IRL method is proposed to model individuals' travel decision-making from observed temporal trajectories. Finally, we validate the proposed model by demonstrating its superior performance over discrete choice model and several well-known imitation learning benchmarks in tasks such as policy performance comparison, reward recovery, model generalizability, and computational efficiency using travel behavior datasets. This approach provides meaningful behavioral insights and paves the way for Artificial Intelligence-driven activity schedulers modeling.}
}
@article{LEE2012469,
title = {Multi-agent knowledge integration mechanism using particle swarm optimization},
journal = {Technological Forecasting and Social Change},
volume = {79},
number = {3},
pages = {469-484},
year = {2012},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2011.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0040162511001715},
author = {Kun Chang Lee and Namho Lee and Habin Lee},
keywords = {Agent-based model (ABM), Particle swarm optimization (PSO), Fuzzy cognitive map (FCM), Expert knowledge, Knowledge integration, IT project risk assessment},
abstract = {Unstructured group decision-making is burdened with several central difficulties: unifying the knowledge of multiple experts in an unbiased manner and computational inefficiencies. In addition, a proper means of storing such unified knowledge for later use has not yet been established. Storage difficulties stem from of the integration of the logic underlying multiple experts' decision-making processes and the structured quantification of the impact of each opinion on the final product. To address these difficulties, this paper proposes a novel approach called the multiple agent-based knowledge integration mechanism (MAKIM), in which a fuzzy cognitive map (FCM) is used as a knowledge representation and storage vehicle. In this approach, we use particle swarm optimization (PSO) to adjust causal relationships and causality coefficients from the perspective of global optimization. Once an optimized FCM is constructed an agent based model (ABM) is applied to the inference of the FCM to solve real world problem. The final aggregate knowledge is stored in FCM form and is used to produce proper inference results for other target problems. To test the validity of our approach, we applied MAKIM to a real-world group decision-making problem, an IT project risk assessment, and found MAKIM to be statistically robust.}
}
@article{FORIO2020101124,
title = {Bayesian Belief Network models as trade-off tools of ecosystem services in the Guayas River Basin in Ecuador},
journal = {Ecosystem Services},
volume = {44},
pages = {101124},
year = {2020},
issn = {2212-0416},
doi = {https://doi.org/10.1016/j.ecoser.2020.101124},
url = {https://www.sciencedirect.com/science/article/pii/S2212041620300668},
author = {Marie Anne Eurie Forio and Gonzalo Villa-Cox and Wout {Van Echelpoel} and Helena Ryckebusch and Koen Lock and Pieter Spanoghe and Arne Deknock and Niels {De Troyer} and Indira Nolivos-Alvarez and Luis Dominguez-Granda and Stijn Speelman and Peter L.M. Goethals},
keywords = {Integrated water management, Agricultural intensification, Land use cover, Trade-off tool, Land use cover optimization, Agricultural crops},
abstract = {Food production often leads to environmental degradation. Consequently, insights into ecosystem functioning in relation to exploitation are needed as a basis for socioeconomically acceptable mitigation of these impacts. A Bayesian Belief Network (BBN) model is developed to link three major ecosystem services (ES), i.e. food production, water provision and ecotourism, and determine the effect of local agricultural practices and management on the ES in the Guayas Basin (Ecuador). Several data sources were integrated into the BBN model, including processed spatial data from primary and secondary sources, sampling and survey data, and expert knowledge. The model suggests that banana and sugar cane generate the highest yield but provide low ecotourism benefits. In contrast, cacao produces the lowest yields but contributes to better water quality. Scenario analyses suggest that environmental gains are possible by optimising the land use (LU) based on the edaphoclimatic requirements of crops. Moreover, the integration of LU optimisation with upscaling and farming intensification can allow for additional advantages in water provision and ecotourism while mitigating productivity losses. The BBN development approach can serve as a reference for other case studies, where data scarcity plays a limiting factor in the assessment of interactions between key ES.}
}
@article{RAMANATHAN2018422,
title = {Smart controller for conical tank system using reinforcement learning algorithm},
journal = {Measurement},
volume = {116},
pages = {422-428},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117307091},
author = {Prabhu Ramanathan and Kamal Kant Mangla and Sitanshu Satpathy},
keywords = {Non-linear process, Machine learning, Reinforcement learning, Data acquisition, Conical tank control},
abstract = {The objective of the paper is to study the implementation of machine learning based controller to control non-linear systems. A smart controller based on reinforcement learning algorithm is proposed, and its performance is demonstrated by using it to control the level of liquid in a non-linear conical tank system. The system is represented in terms of a Markov Decision Process (MDP), and a reinforcement learning technique based on Q-learning algorithm is used to control the process. The advantage is that a standalone controller is designed on its own without prior knowledge of the environment or the system. The hardware implementation of the designed controller showed that the controller controlled the level of fluid in the conical tank efficiently, and rejected random disturbances introduced in the system. This controller provides an edge over PID, fuzzy, and other neural network based controllers, by eliminating the need for linearizing non-linear characteristics, tuning PID parameters, designing transfer functions, and developing fuzzy membership functions.}
}
@article{HOLLOS2022105556,
title = {Conditional interval reduction method: A possible new direction for the optimization of process based models},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105556},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105556},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002560},
author = {R. Hollós and N. Fodor and K. Merganičová and D. Hidy and T. Árendás and T. Grünwald and Z. Barcza},
keywords = {Model optimization, Bayesian calibration, Parameter constraints, Decision tree},
abstract = {Application of process-based models at different spatial scales requires their proper parameterization. This task is typically executed using trial-and-error parameter adjustment or a probabilistic method. Practical application of the probabilistic methods is hampered by methodological complexity and lack of interpretability. Here we present a novel approach for the parameterization of process-based models that we call as conditional interval refinement method (CIRM). The method can be best described as the combination of a probabilistic approach and the advantages of the expert-based parameter adjustment. CIRM was demonstrated by optimizing the Biome-BGCMuSo biogeochemical model using maize yield observations. The proposed approach uses the General Likelihood Uncertainty Estimation (GLUE) method with additional expert knowledge, supplemented by the construction and interpretation of decision trees. It was demonstrated that the iterative, fully automatic method successfully constrained the parameter intervals meanwhile our confidence on the parameters increased. The algorithm can easily be implemented with other process-based models.}
}
@article{LANGEMEYER2020135487,
title = {Creating urban green infrastructure where it is needed – A spatial ecosystem service-based decision analysis of green roofs in Barcelona},
journal = {Science of The Total Environment},
volume = {707},
pages = {135487},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.135487},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719354804},
author = {Johannes Langemeyer and Diego Wedgwood and Timon McPhearson and Francesc Baró and Anders L. Madsen and David N. Barton},
keywords = {Cities, Urban, Nature-based solutions (NBS), Green infrastructure (GI), Bayesian Belief Networks (BBN), Multi-criteria decision analysis (MCDA)},
abstract = {As cities face increasing pressure from densification trends, green roofs represent a valuable source of ecosystem services for residents of compact metropolises where available green space is scarce. However, to date little research has been conducted regarding the holistic benefits of green roofs at a citywide scale, with local policymakers lacking practical guidance to inform expansion of green roofs coverage. The study addresses this issue by developing a spatial multi-criteria screening tool applied in Barcelona, Spain to determine: 1) where green roofs should be prioritized in Barcelona based on expert elicited demand for a wide range of ecosystem services and 2) what type of design of potential green roofs would optimize the ecosystem service provision. As inputs to the model, fifteen spatial indicators were selected as proxies for ecosystem service deficits and demands (thermal regulation, runoff control, habitat and pollination, food production, recreation, and social cohesion) along with five decision alternatives for green roof design (extensive, semi-intensive, intensive, naturalized, and allotment). These indicators and alternatives were analyzed probabilistically and spatially, then weighted according to feedback from local experts. Results of the assessment indicate that there is high demand across Barcelona for the ecosystem services that green roofs potentially might provide, particularly in dense residential neighborhoods and the industrial south. Experts identified habitat, pollination and thermal regulation as the most needed ES with runoff control and food production as the least demanded. Naturalized roofs generated the highest potential ecosystem service provision levels for 87.5% of rooftop area, apart from smaller areas of central Barcelona where intensive rooftops were identified as the preferable green roof design. Overall, the spatial model developed in this study offers a flexible screening based on spatial multi-criteria decision analysis that can be easily adjusted to guide municipal policy in other cities considering the effectiveness of green infrastructure as source of ecosystem services.}
}
@article{XU2023112055,
title = {Bayesian Optimization of photonic curing process for flexible perovskite photovoltaic devices},
journal = {Solar Energy Materials and Solar Cells},
volume = {249},
pages = {112055},
year = {2023},
issn = {0927-0248},
doi = {https://doi.org/10.1016/j.solmat.2022.112055},
url = {https://www.sciencedirect.com/science/article/pii/S092702482200472X},
author = {Weijie Xu and Zhe Liu and Robert T. Piper and Julia W.P. Hsu},
keywords = {Perovskite solar cell, Bayesian optimization, Photonic curing, SHarply additive explanation, Machine learning},
abstract = {Photonic curing is a thin-film processing technique that can enable high-throughput perovskite solar cell (PSC) manufacturing. However, photonic curing has many variables that can affect the processing outcome, making optimization challenging. Here, we introduce Bayesian Optimization (BO), a machine-learning framework, to optimize the power conversion efficiency (PCE) of photonically cured MAPbI3 PSCs on ITO-coated Willow Glass. We apply BO with four input variables—MAPbI3 concentration, additive CH2I2 volume, pulse voltage, and pulse length. These input variables were dynamically adjusted in response to the new data, an example of a human-machine partnership. With the limited experimental budget of 48 conditions, we achieved a champion PCE of 11.42% and predicted 14 new conditions resulting in >10% PCE. Beyond simple optimization, we examined the relationships between pairs of inputs with two-dimensional contour plots and investigated the relative importance of each input to gain insight into photonic curing. We demonstrate that BO is a powerful tool in process optimization and can be adapted to other PSC manufacturing cases.}
}
@article{WEICHERT2023514,
title = {Explainable production planning under partial observability in high-precision manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {514-524},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001590},
author = {Dorina Weichert and Alexander Kister and Peter Volbach and Sebastian Houben and Marcus Trost and Stefan Wrobel},
keywords = {Explainability, Manufacturing, Reinforcement Learning, Monte Carlo Tree Search, Partially Observable Markov Decision Process},
abstract = {Conceptually, high-precision manufacturing is a sequence of production and measurement steps, where both kinds of steps require to use non-deterministic models to represent production and measurement tolerances. This paper demonstrates how to effectively represent these manufacturing processes as Partially Observable Markov Decision Processes (POMDP) and derive an offline strategy with state-of-the-art Monte Carlo Tree Search (MCTS) approaches. In doing so, we face two challenges: a continuous observation space and explainability requirements from the side of the process engineers. As a result, we find that a tradeoff between the quantitative performance of the solution and its explainability is required. In a nutshell, the paper elucidates the entire process of explainable production planning: We design and validate a white-box simulation from expert knowledge, examine state-of-the-art POMDP solvers, and discuss our results from both the perspective of machine learning research and as an illustration for high-precision manufacturing practitioners.}
}
@article{DUNLAP20238061,
title = {Continuous flow synthesis of pyridinium salts accelerated by multi-objective Bayesian optimization with active learning††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3sc01303k},
journal = {Chemical Science},
volume = {14},
number = {30},
pages = {8061-8069},
year = {2023},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc01303k},
url = {https://www.sciencedirect.com/science/article/pii/S204165202300010X},
author = {John H. Dunlap and Jeffrey G. Ethier and Amelia A. Putnam-Neeb and Sanjay Iyer and Shao-Xiong Lennon Luo and Haosheng Feng and Jose Antonio {Garrido Torres} and Abigail G. Doyle and Timothy M. Swager and Richard A. Vaia and Peter Mirau and Christopher A. Crouse and Luke A. Baldwin},
abstract = {ABSTRACT
We report a human-in-the-loop implementation of the multi-objective experimental design via a Bayesian optimization platform (EDBO+) towards the optimization of butylpyridinium bromide synthesis under continuous flow conditions. The algorithm simultaneously optimized reaction yield and production rate (or space-time yield) and generated a well defined Pareto front. The versatility of EDBO+ was demonstrated by expanding the reaction space mid-campaign by increasing the upper temperature limit. Incorporation of continuous flow techniques enabled improved control over reaction parameters compared to common batch chemistry processes, while providing a route towards future automated syntheses and improved scalability. To that end, we applied the open-source Python module, nmrglue, for semi-automated nuclear magnetic resonance (NMR) spectroscopy analysis, and compared the acquired outputs against those obtained through manual processing methods from spectra collected on both low-field (60 MHz) and high-field (400 MHz) NMR spectrometers. The EDBO+ based model was retrained with these four different datasets and the resulting Pareto front predictions provided insight into the effect of data analysis on model predictions. Finally, quaternization of poly(4-vinylpyridine) with bromobutane illustrated the extension of continuous flow chemistry to synthesize functional materials.}
}
@article{GUO201620774,
title = {A new fault diagnosis method based on Bayesian network model in a wastewater treatment plant of northern China},
journal = {Desalination and Water Treatment},
volume = {57},
number = {44},
pages = {20774-20783},
year = {2016},
issn = {1944-3986},
doi = {https://doi.org/10.1080/19443994.2015.1110047},
url = {https://www.sciencedirect.com/science/article/pii/S1944398624164478},
author = {Liang Guo and Ying Zhao and Fu-yi Cui},
keywords = {Bayesian network model, Fault diagnosis, Wastewater treatment plant, A2/O process, Bayesian inference},
abstract = {A novel diagnosing faults method is presented using a Bayesian network (BNT) model to optimize system diagnosis for a wastewater treatment plant (WTP) in northern China. The BNT model is established according to the expert knowledge based on local conditions. The historical data of the WTP are employed to implement the parameter learning of the BNT model. Some practical cases are carried out by the BNT model based on the Bayesian inference. The diagnostic results are compared with the monitoring data of that day to verify accuracy of the BNT model. Meanwhile, several fault diagnosis results and improvement measures are given in this study. The results show that the proposed method is robust enough to diagnose the faults quickly and accurately so as to optimize the operation of the WTP.}
}
@article{WU2024101,
title = {Data and knowledge fusion-driven Bayesian networks for interpretable fault diagnosis of HVAC systems},
journal = {International Journal of Refrigeration},
volume = {161},
pages = {101-112},
year = {2024},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2024.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140700724000732},
author = {Daibiao Wu and Haidong Yang and Kangkang Xu and Xianbing Meng and Sihua Yin and Chengjiu Zhu and Xi Jin},
keywords = {Chillers, Fault diagnosis, Bayesian networks, Broad learning system, Fuzzy logic system, Mult-sources information, Refroidisseurs, Diagnostic des défaillances, Réseaux bayésiens, Système d'apprentissage étendu, Système à logique floue, Information multi-sources},
abstract = {Timely diagnosis and maintenance of faults in chiller units is beneficial for reducing energy consumption in buildings. In order to improve the diagnostic accuracy and provide reasonable explanations for fault occurrences of the heating, ventilation, and air conditioning (HVAC) systems in buildings, a fusion-driven Bayesian network fault diagnosis method is proposed based on broad learning system and fuzzy expert knowledge. Firstly, from the data-driven perspective, an efficient broad learning system is developed as benchmarking model to build the prior Bayesian network structure. Secondly, from the knowledge-driven perspective, the fuzzy logic system is used to fuzz expert knowledge, which then used to optimize the Bayesian network. Finally, the information obtained by experts on-site is incorporated into the optimized Bayesian network as new evidence nodes to determine the Bayesian network. The parameters of the Bayesian network are learned through Noisy-MAX processing of the conditional probability table. The performance of the proposed method is evaluated based on the fault diagnosis of a chiller system. The results demonstrate that the Bayesian network established through this method combines the advantages of data-driven and knowledge-driven approaches. It not only performs well in terms of diagnostic accuracy, with accuracy rates above 98 % for six typical faults, but also provides effective explanations for the underlying mechanisms of the faults through causal relationship diagrams.}
}
@article{MONTEWKA201361,
title = {A probabilistic model estimating oil spill clean-up costs – A case study for the Gulf of Finland},
journal = {Marine Pollution Bulletin},
volume = {76},
number = {1},
pages = {61-71},
year = {2013},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2013.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X13005821},
author = {Jakub Montewka and Mia Weckström and Pentti Kujala},
keywords = {Oil spill, Clean-up costs, The Gulf of Finland, Maritime traffic, Bayesian Belief Networks, Risk analysis},
abstract = {Existing models estimating oil spill costs at sea are based on data from the past, and they usually lack a systematic approach. This make them passive, and limits their ability to forecast the effect of the changes in the oil combating fleet or location of a spill on the oil spill costs. In this paper we make an attempt towards the development of a probabilistic and systematic model estimating the costs of clean-up operations for the Gulf of Finland. For this purpose we utilize expert knowledge along with the available data and information from literature. Then, the obtained information is combined into a framework with the use of a Bayesian Belief Networks. Due to lack of data, we validate the model by comparing its results with existing models, with which we found good agreement. We anticipate that the presented model can contribute to the cost-effective oil-combating fleet optimization for the Gulf of Finland. It can also facilitate the accident consequences estimation in the framework of formal safety assessment (FSA).}
}
@article{KAISER2020352,
title = {Bayesian decomposition of multi-modal dynamical systems for reinforcement learning},
journal = {Neurocomputing},
volume = {416},
pages = {352-359},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.12.132},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305026},
author = {Markus Kaiser and Clemens Otte and Thomas A. Runkler and Carl Henrik Ek},
keywords = {Bayesian machine learning, Gaussian processes, Hierarchical gaussian processes, Reinforcement learning, Model-based reinforcement learning, Stochastic policy search, Data-efficiency},
abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.}
}
@article{JIANG2025720,
title = {Multiple financial analyst opinions aggregation based on uncertainty-aware quality evaluation},
journal = {European Journal of Operational Research},
volume = {320},
number = {3},
pages = {720-738},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724006660},
author = {Shuai Jiang and Wenjun Zhou and Yanhong Guo and Hui Xiong},
keywords = {Analyst opinion, Collective wisdom, Uncertainty quantification, Financial technology},
abstract = {Financial analysts’ opinions are pivotal in investment decision-making, as they provide valuable expert knowledge. Aggregating these opinions offers a promising way to unlock their collective wisdom. However, existing opinion aggregation methods are hindered by their inability to effectively assess differences in opinion quality, resulting in suboptimal outcomes. This Study introduces a novel model called SmartMOA, which addresses this limitation by automatically evaluating the quality of each opinion and integrating this evaluation into the aggregation process. Our model begins with a novel Bayesian neural network that leverages the implicit knowledge embedded in the interactions between analysts and stock characteristics. This methodology produces an assessment of individual opinions that accounts for uncertainties. We then formulate a bi-objective combinatorial optimization problem to determine optimal weights for combining multiple analysts’ opinions, simultaneously minimizing the error and uncertainty of the aggregated outcome. Therefore, SmartMOA systematically highlights high-quality opinions during the aggregation process. Using a real dataset spanning eight years, we present comprehensive empirical evidence that demonstrates the superior performance of SmartMOA in heterogeneous analyst opinion aggregation.}
}
@article{COUTINHO202413,
title = {Human-in-the-loop controller tuning using Preferential Bayesian Optimization},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {14},
pages = {13-18},
year = {2024},
note = {12th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.306},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324010486},
author = {João P.L. Coutinho and Ivan Castillo and Marco S. Reis},
keywords = {Bayesian Optimization, Preference Learning, Multi-objective optimization, PID tuning},
abstract = {The development of human-centric platforms that are able to combine computational resources and advanced analytics with human judgment and qualitative processing ability is a key driver of the Industry 5.0 movement. In this setting, humans are not only active in the loop but also play a key role in the decision-making process. In this work, we propose the use of Preferential Bayesian Optimization (PBO) for human-in-the-loop controller tuning. PBO relies on pairwise comparisons and preference feedback (A is better than B) to search for the optimal trade-of between different performance criteria from the user’s perspective. The advantages of PBO are demonstrated in a simulated Proportional Integral (PI) controller tuning example with real user feedback under a reduced number of experiments. The results show that PBO leads to a greater emphasis on closed-loop responses closer to the user’s desired behavior when compared with multi-objective alternatives, while being straightforward to implement from the user’s perspective.}
}
@article{HOQUETANIA2022,
title = {Thinking Aloud or Screaming Inside: Exploratory Study of Sentiment Around Work},
journal = {JMIR Formative Research},
volume = {6},
number = {9},
year = {2022},
issn = {2561-326X},
doi = {https://doi.org/10.2196/30113},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X22008496},
author = {Marzia {Hoque Tania} and Md Razon Hossain and Nuzhat Jahanara and Ilya Andreev and David A Clifton},
keywords = {work-related mental health, sentiment analysis, natural language processing, occupational health, Bayesian inference, machine learning, artificial intelligence, mobile phone},
abstract = {Background
Millions of workers experience work-related ill health every year. The loss of working days often accounts for poor well-being because of discomfort and stress caused by the workplace. The ongoing pandemic and postpandemic shift in socioeconomic and work culture can continue to contribute to adverse work-related sentiments. Critically investigating state-of-the-art technologies, this study identifies the research gaps in recognizing workers’ need for well-being support, and we aspire to understand how such evidence can be collected to transform the workforce and workplace.
Objective
Building on recent advances in sentiment analysis, this study aims to closely examine the potential of social media as a tool to assess workers’ emotions toward the workplace.
Methods
This study collected a large Twitter data set comprising both pandemic and prepandemic tweets facilitated through a human-in-the-loop approach in combination with unsupervised learning and meta-heuristic optimization algorithms. The raw data preprocessed through natural language processing techniques were assessed using a generative statistical model and a lexicon-assisted rule-based model, mapping lexical features to emotion intensities. This study also assigned human annotations and performed work-related sentiment analysis.
Results
A mixed methods approach, including topic modeling using latent Dirichlet allocation, identified the top topics from the corpus to understand how Twitter users engage with discussions on work-related sentiments. The sorted aspects were portrayed through overlapped clusters and low intertopic distances. However, further analysis comprising the Valence Aware Dictionary for Sentiment Reasoner suggested a smaller number of negative polarities among diverse subjects. By contrast, the human-annotated data set created for this study contained more negative sentiments. In this study, sentimental juxtaposition revealed through the labeled data set was supported by the n-gram analysis as well.
Conclusions
The developed data set demonstrates that work-related sentiments are projected onto social media, which offers an opportunity to better support workers. The infrastructure of the workplace, the nature of the work, the culture within the industry and the particular organization, employers, colleagues, person-specific habits, and upbringing all play a part in the health and well-being of any working adult who contributes to the productivity of the organization. Therefore, understanding the origin and influence of the complex underlying factors both qualitatively and quantitatively can inform the next generation of workplaces to drive positive change by relying on empirically grounded evidence. Therefore, this study outlines a comprehensive approach to capture deeper insights into work-related health.}
}
@article{HAAS2024772,
title = {Improving the weld seam quality in laser welding processes by means of Bayesian optimization},
journal = {Procedia CIRP},
volume = {124},
pages = {772-775},
year = {2024},
note = {13th CIRP Conference on Photonic Technologies [LANE 2024], 15-19 September 2024, Fürth, Germany},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124005766},
author = {Michael Haas and Volkher Onuseit and John Powell and Felix Zaiß and Johannes Wahl and Tobias Menold and Christian Hagenlocher and Andreas Michalowski},
keywords = {laser welding, machine learning, process optimization, pore formation, X-ray imaging},
abstract = {The determination of appropriate process parameters is crucial for the development of laser welding processes. This usually requires extensive and time-consuming experimentation combined with expert knowledge. To reduce the number of experiments required to determine appropriate process parameters, Bayesian optimization was used in this work. Bead on plate laser welding of AA5754 samples was performed while optimizing the laser power, the welding speed, the focus position and the power distribution in the core-ring fiber laser system with the objective of achieving welds with a specific weld depth and low number of defects at high welding speeds. The welds were evaluated using X-ray imaging and height measurements. A cost function was developed to quantify the overall weld quality based on the weld properties. It is demonstrated that the Bayesian optimizer can determine appropriate process parameters for the given objective, based on a cost function, within a comparatively small number of 29 experiments.}
}
@article{PITIOT2010830,
title = {Hybridation of Bayesian networks and evolutionary algorithms for multi-objective optimization in an integrated product design and project management context},
journal = {Engineering Applications of Artificial Intelligence},
volume = {23},
number = {5},
pages = {830-843},
year = {2010},
note = {Advances in metaheuristics for hard optimization: new trends and case studies},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2010.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197610000370},
author = {Paul Pitiot and Thierry Coudert and Laurent Geneste and Claude Baron},
keywords = {Project management, Product preliminary design, Evolutionary algorithm, Experience feedback, Bayesian network, Learning},
abstract = {A better integration of preliminary product design and project management processes at early steps of system design is nowadays a key industrial issue. Therefore, the aim is to make firms evolve from classical sequential approach (first product design the project design and management) to new integrated approaches. In this paper, a model for integrated product/project optimization is first proposed which allows taking into account simultaneously decisions coming from the product and project managers. However, the resulting model has an important underlying complexity, and a multi-objective optimization technique is required to provide managers with appropriate scenarios in a reasonable amount of time. The proposed approach is based on an original evolutionary algorithm called evolutionary algorithm oriented by knowledge (EAOK). This algorithm is based on the interaction between an adapted evolutionary algorithm and a model of knowledge (MoK) used for giving relevant orientations during the search process. The evolutionary operators of the EA are modified in order to take into account these orientations. The MoK is based on the Bayesian Network formalism and is built both from expert knowledge and from individuals generated by the EA. A learning process permits to update probabilities of the BN from a set of selected individuals. At each cycle of the EA, probabilities contained into the MoK are used to give some bias to the new evolutionary operators. This method ensures both a faster and effective optimization, but it also provides the decision maker with a graphic and interactive model of knowledge linked to the studied project. An experimental platform has been developed to experiment the algorithm and a large campaign of tests permits to compare different strategies as well as the benefits of this novel approach in comparison with a classical EA.}
}
@article{PEREZCOLO2023120959,
title = {Intelligent approach for the industrialization of deep learning solutions applied to fault detection},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120959},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120959},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014616},
author = {Ivo {Perez Colo} and Carolina {Saavedra Sueldo} and Mariano {De Paula} and Gerardo G. Acosta},
keywords = {Artificial intelligence, Deep neural networks, Bayesian optimization, Industrialization, Fault detection},
abstract = {Early fault detection, both in equipment and the products in process, is of paramount importance in industrial processes to ensure the quality of the final product, avoid abnormal operating conditions, expensive repairs, and even production process shutdown. The growing complexity of industrial systems and the increase in the amount of available data have encouraged the development of intelligent systems for automatic fault prediction/detection, mainly based on Industry 4.0 technologies and, particularly, those based on deep learning methodologies. However, the vast majority of proposals and research carried out to date define specific solutions for specific cases, which still requires a high level of expert knowledge for scaling the solutions to industrial environments. Actually, one of the major issues towards the industrialization of deep learning solutions is the determination of the optimal, or near-optimal, hyper-parameters. In this paper, we propose a low-level set-up effort intelligent failure detection system that integrates deep neural networks with a Bayesian Optimization algorithm for self-tuning of the system hyper-parameters. In addition, to facilitate the industrialization of the proposal and its incorporation into current industrial systems, we embedded the proposal in our previously formulated and tested Simulai architecture which allows for containing and interaction with multiple and heterogeneous technological components of manufacturing processes. Finally, our proposal is tested in two real cases of a different nature. The obtained results show a successful performance and demonstrate the easy online integration and interaction in a real production system.}
}
@article{SAHIN2007124,
title = {Fault diagnosis for airplane engines using Bayesian networks and distributed particle swarm optimization},
journal = {Parallel Computing},
volume = {33},
number = {2},
pages = {124-143},
year = {2007},
note = {Trends in Parallel Computing},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2006.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106001013},
author = {Ferat Sahin and M. Çetin Yavuz and Ziya Arnavut and Önder Uluyol},
keywords = {Bayesian networks, Fault diagnosis, Particle swarm optimization, Parallel computing},
abstract = {This paper presents a fault diagnosis system for airplane engines using Bayesian networks (BN) and distributed particle swarm optimization (PSO). The PSO is inherently parallel, works for large domains and does not trap into local maxima. We implemented the algorithm on a computer cluster with 48 processors using message passing interface (MPI) in Linux. Our implementation has the advantages of being general, robust, and scalable. Unlike existing BN-based fault diagnosis methods, neither expert knowledge nor node ordering is necessary prior to the Bayesian Network discovery. The raw datasets obtained from airplane engines during actual flights are preprocessed using equal frequency binning histogram and used to generate Bayesian networks fault diagnosis for the engines. We studied the performance of the distributed PSO algorithm and generated a BN that can detect faults in the test data successfully.}
}
@article{WANG2025101343,
title = {A clinical treatment recommender system optimizing adjuvant chemoradiotherapy benefit in Chinese women with breast cancer using interpretable causal Bayesian networks},
journal = {The Lancet Regional Health - Western Pacific},
volume = {55},
pages = {101343},
year = {2025},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2024.101343},
url = {https://www.sciencedirect.com/science/article/pii/S2666606524003377},
author = {Ruoyu Wang and Simiao Tian and Zhe Wang and Yiou Wang and Dianlong Zhang and Jianing Jiang and Yongqiang Yao and Hongshen Chen and Hong Fang and Mingqian Cao},
abstract = {Background
Breast cancer (BC) has surpassed lung cancer in becoming the most common cancer among women worldwide. However, few studies have investigated multivariate BC prognosis outcomes. This study aimed to develop and validate a causal model based on Bayesian networks (BN) that can simultaneously predict long-term risk of all-cause mortality and recurrence among Chinese women with BC, and further to estimate individualised expected benefits of model-based treatment recommendations.
Methods
This study used data from adult women who were diagnosed with primary invasive T1-4 N0-3 M0 BC at the Affiliated Zhongshan Hospital of Dalian University, Dalian, Liaoning, China, between January 2011 and December 2017. After the whole cohort was randomly divided into 70% development and 30% validation cohorts, a causal BN (CBN) model was built by aggregating extensive expert knowledge and data-driven approaches to arrive at a consensus structure for causal inference, and subsequently validated its ability to predict both overall survival (OS) and recurrence free survival (RFS) by using the area under the curve (AUC), Brier score (BS), accuracy and predicted/observation ratio. Variables included sociodemographics; preoperative clinical, histopathological and molecular biomarkers; operative variables; and postoperative treatment variables. OS and RFS in women with BC were the main outcomes of this study and were assessed at the most recent follow-up on December 31, 2022.
Findings
A total of 3512 patients, which consisted of 2458 patients in the development cohort (mean [SD] age, 54.60 [10.68] years) and 1054 patients in the validation cohort (mean [SD] age, 53.77 [11.08] years), were eligible for this study. A total of 175 (5.0%) patients with BC died, and 344 (9.8%) experienced recurrence after at least 5-year follow-up. A CBN model was developed that included the following predictors: age group; preoperative CEA, CA 125 and CA15-3 serum levels; neoadjuvant chemoradiotherapy; surgery type; tumour grade and histology; immunohistochemical expression of oestrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) amplification, and Ki-67; pT and pN stages; and postoperative treatment with chemotherapy, radiotherapy and endocrine therapy. In the validation cohort, the CBN model had excellent performance with desirable AUCs of 0.92 (95% CI 0.87-0.98) and 0.73 (0.62-0.83) and low BSs of 0.025 and 0.062 for OS and RFS, respectively. The model also achieved excellent accuracy and model calibration for OS and RFS, with accuracies of 96.4% and 92.8% and predicted/observed ratios of 1.01 (0.92-1.11) and 1.05 (0.95-1.15), respectively. Furthermore, the chemoradiotherapy treatment according to our CBN model recommendations was associated with notably better prognosis, with a hazard ratio of 0.63 (0.44-0.90) for OS and 0.81 for RFS (0.75-0.87), respectively.
Interpretation
In this prognostic study based on clinical real-world data, a CBN model was developed that accurately predicted two important outcomes for women with BC, and it has the potential to identify patients who could benefit from chemoradiotherapy. The CBN can effectively capture complex interrelationships among risk factors and identify potential causal pathways underlying tumour progression through its graphical representation, and could also provide an adjuvant treatment recommendation system for BC women.}
}
@article{FATEHIKARJOU2025100490,
title = {Human-in-the-loop control strategy for IoT-based smart thermostats with Deep Reinforcement Learning},
journal = {Energy and AI},
volume = {20},
pages = {100490},
year = {2025},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2025.100490},
url = {https://www.sciencedirect.com/science/article/pii/S2666546825000229},
author = {Payam {Fatehi Karjou} and Fabian Stupperich and Phillip Stoffel and Dirk Müller},
keywords = {Human-in-the-loop control, AI, TRV, RLC, IoT},
abstract = {Thermostatic Radiator Valves (TRVs) are a widely used technology for regulating room heating in Europe countries. Smart TRVs can provide significant energy savings, often ranging from 20–40% compared to conventional heating systems. They use sensors and algorithms to learn user behavior and optimize heating schedules accordingly. They can often be easily retrofitted to existing heating systems, making them a practical option for enhancing energy efficiency in present buildings, especially in office buildings due to their highly dynamic operational patterns. This work presents a novel human-in-the-loop control strategy for Internet of Things (IoT)-based TRVs using Deep Reinforcement Learning (DRL). A key focus of this research is enhancing the adaptability of agents’ behavior by implementing a more generic and flexible Markov Decision Process (MDP) to promote policy generalization across diverse scenarios. The study explores the challenges of transferring control behaviors from simulation environments to real-world settings, examining the performance across different thermal zones and evaluating the integration flexibility of the control strategy within building systems. Real-world occupant behavior is incorporated, including dynamic comfort preferences and occupancy predictions, to better align thermostat operation with user preferences. Furthermore, this paper discusses the practical challenges encountered during implementation, including battery consumption of IoT devices, integration of occupancy detection and prediction systems, and maintenance requirements. By addressing these issues, the proposed control strategy seeks to improve the scalability and feasibility of IoT-based TRVs, thereby providing a viable solution for their widespread deployment in buildings.}
}
@article{YANG2020113424,
title = {A physics-informed Run-to-Run control framework for semiconductor manufacturing},
journal = {Expert Systems with Applications},
volume = {155},
pages = {113424},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113424},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420302487},
author = {Wei-Ting Yang and Jakey Blue and Agnès Roussy and Jacques Pinaton and Marco S. Reis},
keywords = {Advanced Process Control (APC), Chemical-Mechanical Polishing (CMP), Dynamic Bayesian Network (DBN), Fault Detection and Classification (FDC), Physics-informed, Run-to-Run (R2R) control},
abstract = {For decades, Run-to-Run (R2R) controllers have been widely implemented in semiconductor manufacturing. They operate over key process parameters on the basis of the metrological measurements acquired from the process and their deviations from the target setpoints. Conventionally, R2R controllers have been implemented independently of the actual equipment condition, which is obviously affecting the process stability and performance. Therefore, both equipment signals and process states shall be considered to make the R2R controllers more robust to the equipment condition drifts. In this paper, we propose a novel physics-informed framework to integrate the real-time equipment condition, based on the Fault Detection and Classification (FDC) data, into the R2R controllers. By utilizing Dynamic Bayesian Networks (DBN), the implicit relationship structure between metrology measurements, FDC indicators, and R2R regulators can be learned and reviewed explicitly. The structure shall be further reviewed to valid with the existing relationships and expert knowledge. Infeasible causalities on the structure will be constrained via setting up the blacklist at the structure learning stage. The proposed framework consists of the offline modeling stage, which incorporates the process, equipment variables, and the expert knowledge in the structure learning, and the online control stage, which constructs the Structured R2R controller (SRC) based on the relationship structure. As a result, the model is consistent by design with empirically known relationships and fundamental physical laws. The proposed SRC not only optimizes the operation with respect to the target control values but also considers the equipment and process states simultaneously. The effectiveness of SRC and the derivative control strategy are validated through a real dataset of a Chemical-Mechanical Polishing (CMP) process, and two simulated studies.}
}
@article{SAVAGE2024108810,
title = {Human-algorithm collaborative Bayesian optimization for engineering systems},
journal = {Computers & Chemical Engineering},
volume = {189},
pages = {108810},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108810},
url = {https://www.sciencedirect.com/science/article/pii/S009813542400228X},
author = {Tom Savage and Ehecatl Antonio {del Rio Chanona}},
keywords = {Bayesian optimization, Experimental design, Human-in-the-loop, Domain knowledge},
abstract = {Bayesian optimization has proven effective for optimizing expensive-to-evaluate functions in Chemical Engineering. However, valuable physical insights from domain experts are often overlooked. This article introduces a collaborative Bayesian optimization approach that re-integrates human input into the data-driven decision-making process. By combining high-throughput Bayesian optimization with discrete decision theory, experts can influence the selection of experiments via a discrete choice. We propose a multi-objective approach togenerate a set of high-utility and distinct solutions, from which the expert selects the desired solution for evaluation at each iteration. Our methodology maintains the advantages of Bayesian optimization while incorporating expert knowledge and improving accountability. The approach is demonstrated across various case studies, including bioprocess optimization and reactor geometry design, demonstrating that even with an uninformed practitioner, the algorithm recovers the regret of standard Bayesian optimization. By including continuous expert opinion, the proposed method enables faster convergence and improved accountability for Bayesian optimization in engineering systems.}
}
