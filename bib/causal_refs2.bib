@article{COUTINHO2023108211,
title = {Bayesian Optimization for automatic tuning of digital multi-loop PID controllers},
journal = {Computers & Chemical Engineering},
volume = {173},
pages = {108211},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108211},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423000807},
author = {João P.L. Coutinho and Lino O. Santos and Marco S. Reis},
keywords = {Multi-loop PID tuning, Automatic tuning, Bayesian optimization, Internal model control, Sequential loop closing},
abstract = {In recent years, the use of Bayesian optimization (BO) for efficient automatic tuning of general controller structures through iterative closed-loop experiments, has been attracting increasing interest. However, its potential for tuning interactive multi-loop PID controllers in Multi Input Multi Output (MIMO) processes remains largely unexplored. Even though the optimization domain greatly affects closed-loop performance and safety, it is usually defined manually, through expert knowledge or experimentation. This paper presents a novel systematic methodology for defining the optimization domain for automatic multi-loop PID tuning using BO. Sequential loop closing, system identification and tuning relations are used to constrain the bounds on controller parameters to meaningful ranges, including gains and sampling times. This provides an effective way to improve the convergence of BO and secure process safety during closed-loop experiments, without requiring a MIMO process model or extensive prior knowledge. The methodology can be applied ”as is” to single-loop PID controllers.}
}
@article{ZHANG2023677,
title = {Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system},
journal = {Applied Mathematical Modelling},
volume = {119},
pages = {677-697},
year = {2023},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23001269},
author = {Yiming Zhang and Dingyang Zhang and Xiaoge Zhang and Lemiao Qiu and Felix T.S. Chan and Zili Wang and Shuyou Zhang},
keywords = {Deep Reinforcement Learning, Multi-component System, Probabilistic Machine Learning, Maintenance Scheduling, Sampling-Efficient Learning},
abstract = {In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.}
}
@article{HENNEBOLD20241296,
title = {Combination of Process Mining and Causal Discovery Generated Graph Models for Comprehensive Process Modeling},
journal = {Procedia CIRP},
volume = {130},
pages = {1296-1302},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.242},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013994},
author = {Christoph Hennebold and Muhammad M. Islam and Jonas Krauß and Marco F. Huber},
keywords = {Process Mining, Causal Discovery, Domain Knowledge, Hybrid Modeling},
abstract = {The extraction of process knowledge and its use for modeling and subsequent analysis is a very valuable approach to optimizing processes. Process Mining (PM) methods are widely used knowledge extraction techniques based on process knowledge recorded in event logs. One shortcoming is that PM approaches cannot easily access data in non-event log format, and secondly, that models are generated using directly-follows relations, which in the case of processes with concurrent activities means that process transitions may not be modeled correctly, and the resulting models do not correspond to reality as well as being unnecessarily complex. In addition, increasing digitization in manufacturing enables an ever greater collection of various data sources throughout the process with varying levels of information. Besides PM, in recent years great progress has been made in learning causal structures via causal discovery (CD) as well as in explicitly using additional expert knowledge. We argue that CD and expert knowledge are valuable additions to existing PM based approaches, as they allow knowledge extraction and modeling on different process levels. To combine these levels of information, this paper uses a manufacturing use case to show heterogeneous data sources and expert knowledge are used to create hybrid models. The results show that the combination of data-driven modeling methods with expert knowledge help to compensate for the weaknesses of the individual methods and achieve better overall results.}
}
@article{HE2022939,
title = {Multi-objective optimization of the textile manufacturing process using deep-Q-network based multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {939-949},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000728},
author = {Zhenglei He and Kim Phuc Tran and Sebastien Thomassey and Xianyi Zeng and Jie Xu and Changhai Yi},
keywords = {Deep reinforcement learning, Deep Q-networks, Multi-objective, Optimization, Decision, Process, Textile Manufacturing},
abstract = {Multi-objective optimization, such as quality, productivity, and cost, of the textile manufacturing process is increasingly challenging because of the growing complexity involved in the development of textile industry in the upcoming big data era. It is hard for traditional methods to deal with high-dimension decision space in this issue, and prior experts’ knowledge is required as well as human intervention. This paper proposed a novel framework that transformed the textile process optimization problem into a stochastic game, and introduced deep Q-networks algorithm instead of current methods to approach it in a multi-agent system. The developed multi-agent reinforcement learning system applied a utilitarian selection mechanism to maximize the sum of all agents’ rewards (obeying the increasing ε-greedy policy) in each state, to avoid the interruption of multiple equilibria and achieve the correlated equilibrium optimal solutions of the textile process. The case study result reflects that the proposed MARL system can achieve the optimal solutions for the textile ozonation process, and it performs better than the traditional approaches.}
}
@article{LUOMA2021112614,
title = {Developing a conceptual influence diagram for socio-eco-technical systems analysis of biofouling management in shipping – A Baltic Sea case study},
journal = {Marine Pollution Bulletin},
volume = {170},
pages = {112614},
year = {2021},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2021.112614},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X21006482},
author = {Emilia Luoma and Lauri Nevalainen and Elias Altarriba and Inari Helle and Annukka Lehikoinen},
keywords = {Systems analysis, Risk management, Influence diagram, Alien species, Biofouling management, Baltic Sea},
abstract = {Ship hulls create a vector for the transportation of harmful non-indigenous species (NIS) all over the world. To sustainably prevent NIS introductions, the joint consideration of environmental, economic and social aspects in the search of optimal biofouling management strategies is needed. This article presents a multi-perspective soft systems analysis of the biofouling management problem, based on an extensive literature review and expert knowledge collected in the Baltic Sea area during 2018–2020. The resulting conceptual influence diagram (CID) reveals the multidimensionality of the problem by visualizing the causal relations between the key elements and demonstrating the entanglement of social, ecological and technical aspects. Seen as a boundary object, we suggest the CID can support open dialogue and better risk communication among stakeholders by providing an illustrative and directly applicable starting point for the discussions. It also provides a basis for quantitative management optimization in the future.}
}
@article{GONZALEZ2023108110,
title = {New paradigms for exploiting parallel experiments in Bayesian optimization},
journal = {Computers & Chemical Engineering},
volume = {170},
pages = {108110},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108110},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422004434},
author = {Leonardo D. González and Victor M. Zavala},
keywords = {Bayesian optimization, High-throughput experiments, Parallelization},
abstract = {Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential algorithm (one experiment is proposed per round) and thus cannot directly exploit high-throughput (parallel) experiments. Diverse modifications to the BO framework have been proposed in the literature to enable exploitation of parallel experiments but such approaches are limited in the degree of parallelization that they can achieve and can lead to redundant experiments (thus wasting resources and potentially compromising performance). In this work, we present new parallel BO paradigms that exploit the structure of the system to partition the design space. Specifically, we propose an approach that partitions the design space by following the level sets of the performance function and an approach that exploits the partially separable structure of the performance function found. We conduct extensive numerical experiments using a reactor case study to benchmark the effectiveness of these approaches against a variety of state-of-the-art parallel algorithms reported in the literature. Our computational results show that our approaches significantly reduce the required search time and increase the probability of finding a global (rather than local) solution.}
}
@article{ZHOU201669,
title = {An empirical study of Bayesian network parameter learning with monotonic influence constraints},
journal = {Decision Support Systems},
volume = {87},
pages = {69-79},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616300744},
author = {Yun Zhou and Norman Fenton and Cheng Zhu},
keywords = {BN parameter learning, Monotonic influences, Exterior constraints, Experiments on publicly available BNs, Real medical study},
abstract = {Learning the conditional probability table (CPT) parameters of Bayesian networks (BNs) is a key challenge in real-world decision support applications, especially when there are limited data available. A conventional way to address this challenge is to introduce domain knowledge/expert judgments that are encoded as qualitative parameter constraints. In this paper we focus on a class of constraints which is naturally encoded in the edges of BNs with monotonic influences. Experimental results indicate that such monotonic influence constraints are widespread in practical BNs (all BNs used in the study contain such monotonic influences). To exploit expert knowledge about such constraints we have developed an improved constrained optimization algorithm, which achieves good parameter learning performance using these constraints, especially when data are limited. Specifically, this algorithm outperforms the previous state-of-the-art and is also robust to errors in labelling the monotonic influences. The method is applied to a real world medical decision support BN where we had access to expert-provided constraints and real hospital data. The results suggest that incorporating expert judgments about monotonic influence constraints can lead to more accurate BNs for decision support and risk analysis.}
}
@article{AV2025113138,
title = {Accelerated experimental design using a human–AI teaming framework},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113138},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113138},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001856},
author = {Arun Kumar A.V. and Alistair Shilton and Sunil Gupta and Shannon Ryan and Majid Abdolshah and Hung Le and Santu Rana and Julian Berk and Mahad Rashid and Svetha Venkatesh},
keywords = {Machine learning, Experimental design, Sample-efficiency, Gaussian process, Bayesian optimization, Human–AI teaming},
abstract = {In this paper we propose a human–AI teaming framework for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our proposed algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. We validate our proposed algorithm using synthetic data and with human experts performing real-world experiments.}
}
@article{ZHANG201265,
title = {Robust hyperspectral vision-based classification for multi-season weed mapping},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {69},
pages = {65-73},
year = {2012},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2012.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271612000421},
author = {Yun Zhang and David C. Slaughter and Erik S. Staab},
keywords = {Computer vision, Plant recognition, Machine learning, Multiclassifier system, Weed control, Seasonal variability},
abstract = {This study investigated the robustness of hyperspectral image-based plant recognition to seasonal variability in a natural farming environment in the context of automated in-row weed control. A machine vision system was developed and equipped with a CCD camera integrated with a line-imaging spectrograph for close-range weed sensing and mapping. Three canonical Bayesian classifiers were developed using canopy reflectance (400–795nm) collected over three seasons for tomato and weeds. The performance of the three season-specific classifiers was tested by changing environmental conditions, resulting in an increase in total error rate of up to 36%. Global calibration across the complete span of the three seasons produced overall classification accuracies of 85.0%, 90.0% and 92.7%, respectively, for 2005, 2006 and 2008. To improve the stability of global classifier over multiple seasons, a multiclassifier system was constructed with three canonical Bayesian classifiers optimized for the three seasons individually. This system was tested on a data set simulating an upcoming season with field conditions similar to that in 2005. The system increased the total discrimination accuracy to 95.8% for the tested season under simulation. This method provided an innovative direction for achieving robust plant recognition over multiple seasons by integrating expert knowledge from historical data that most closely matched the new field environment.}
}
@article{KANGAS2000157,
title = {Improving the quality of landscape ecological forest planning by utilising advanced decision-support tools},
journal = {Forest Ecology and Management},
volume = {132},
number = {2},
pages = {157-171},
year = {2000},
issn = {0378-1127},
doi = {https://doi.org/10.1016/S0378-1127(99)00221-2},
url = {https://www.sciencedirect.com/science/article/pii/S0378112799002212},
author = {Jyrki Kangas and Ron Store and Pekka Leskinen and Lauri Mehtätalo},
keywords = {Bayesian statistics, Ecological modelling, Forest management, GIS, Multi-criteria decision support, Uncertainty},
abstract = {The quality of landscape ecological analyses and their integration with the multi-objective comparison of forest plans can be improved by making use of the decision-support methods, techniques, and tools produced by recent research on forest planning, as demonstrated in this study. Special attention is given to strengthening the ecological grounds of calculations through modelling expert knowledge, quantification of ecological evaluations, integration of different objectives and different phases of the planning process, and analysing the effects of uncertainty in ecological judgments on the final results. The planning process is illustrated by a case study. The landscape ecological approach is finding increasing application in practical forest planning, especially in boreal forestry. Unfortunately, gaps in the available ecological knowledge, and the inefficiency of the planning methods and tools used often lead to vague planning processes. In many cases, only methods originally developed for wood-production planning are still applied, and planning advances (e.g. multi-objective optimisation, Geographical Information Systems (GIS) tools, and modelling expert knowledge) are under-utilised. In this study, HERO heuristic multi-objective optimisation, GIS operations, pairwise comparisons techniques, and Bayesian analysis are applied in an integrated planning process. Efficient forest plan alternatives are generated for further consideration by utilising heuristic optimization and GIS. Given the multi-objective choice situation, the plans generated are holistically evaluated by means of multiple decision-support tools and techniques.}
}
@article{TRUONG2013390,
title = {Web-based tool for expert elicitation of the variogram},
journal = {Computers & Geosciences},
volume = {51},
pages = {390-399},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2012.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300412002890},
author = {Phuong N. Truong and Gerard B.M. Heuvelink and John Paul Gosling},
keywords = {Spatial variability, Expert knowledge, Geostatistics, Subjective prior information},
abstract = {The variogram is the keystone of geostatistics. Estimation of the variogram is deficient and difficult when there are no or too few observations available due to budget constraints or physical and temporal obstacles. In such cases, expert knowledge can be an important source of information. Expert knowledge can also fulfil the increasing demand for an a priori variogram in Bayesian geostatistics and spatial sampling optimization. Formal expert elicitation provides a sound scientific basis to reliably and consistently extract knowledge from experts. In this study, we aimed at applying existing statistical expert elicitation techniques to extract the variogram of a regionalized variable that is assumed to have either a multivariate normal or lognormal spatial probability distribution from expert knowledge. To achieve this, we developed an elicitation protocol and implemented it as a web-based tool to facilitate the elicitation of beliefs from multiple experts. Our protocol has two main rounds: elicitation of the marginal probability distribution and elicitation of the variogram. The web-based tool has three main components: a web interface for expert elicitation and feedback; a component for statistical computation and mathematical pooling of multiple experts’ knowledge; and a database management component. Results from a test case study show that the protocol is adequate and that the online elicitation tool functions satisfactorily. The web-based tool is free to use and supports scientists to conveniently elicit the variogram of spatial random variables from experts. The source code is available from the journal FTP site under the GNU General Public License.}
}
@article{POESCHL2016247,
title = {Situation-based Methodology for Planning the Commissioning of Special Machinery Using Bayesian Networks},
journal = {Procedia CIRP},
volume = {57},
pages = {247-252},
year = {2016},
note = {Factories of the Future in the digital environment - Proceedings of the 49th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.11.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116311970},
author = {Sebastian Poeschl and Frank Wirth and Thomas Bauernhansl},
keywords = {Special Machinery, Bayesian Networks, Process Heuristics, Process Optimization},
abstract = {In German mechanical engineering customized systems and integration solutions are the biggest trends which are mainly applied in special machinery. This paper shows a method to decrease test and commissioning time by using expert knowledge and by considering the risk of failing processes. In literature and practice there is a wide research on virtual commissioning. However, research on methods to optimize production is very rare for complex machinery. In the proposed method, for planning and adapting processes, the authors use heuristics because of their ability to optimize processes using expert knowledge. For the decision of the right application of a heuristic, Bayesian Networks are applied to rate and compare different alternatives. Thus, the result is a method which allows to rate the processes with the needed time and the possible risk for an elimination and a substitution of these processes. Using this method the throughput time of a laser system in production in one single commissioning process is decreased in the validation example by approximately three days.}
}
@article{SOARES2018715,
title = {Experiences of Structured Elicitation for Model-Based Cost-Effectiveness Analyses},
journal = {Value in Health},
volume = {21},
number = {6},
pages = {715-723},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518302274},
author = {Marta O. Soares and Linda Sharples and Alec Morton and Karl Claxton and Laura Bojke},
keywords = {Bayesian, cost effectiveness, decision modeling, elicitation, expert judgment, subjective},
abstract = {Background
Empirical evidence supporting the cost-effectiveness estimates of particular health care technologies may be limited, or it may even be missing entirely. In these situations, additional information, often in the form of expert judgments, is needed to reach a decision. There are formal methods to quantify experts’ beliefs, termed as structured expert elicitation (SEE), but only limited research is available in support of methodological choices. Perhaps as a consequence, the use of SEE in the context of cost-effectiveness modelling is limited.
Objectives
This article reviews applications of SEE in cost-effectiveness modelling with the aim of summarizing the basis for methodological choices made in each application and recording the difficulties and challenges reported by the authors in the design, conduct, and analyses.
Methods
The methods used in each application were extracted along with the criteria used to support methodological and practical choices and any issues or challenges discussed in the text. Issues and challenges were extracted using an open field, and then categorised and grouped for reporting.
Results
The review demonstrates considerable heterogeneity in methods used, and authors acknowledge great methodological uncertainty in justifying their choices. Specificities of the context area emerging as potentially important in determining further methodological research in elicitation are between- expert variation and its interpretation, the fact that substantive experts in the area may not be trained in quantitative subjects, that judgments are often needed on various parameter types, the need for some form of assessment of validity, and the need for more integration with behavioural research to devise relevant debiasing strategies.
Conclusions
This review of experiences of SEE highlights a number of specificities/constraints that can shape the development of guidance and target future research efforts in this area.}
}
@article{SEIXAS2014140,
title = {A Bayesian network decision model for supporting the diagnosis of dementia, Alzheimer׳s disease and mild cognitive impairment},
journal = {Computers in Biology and Medicine},
volume = {51},
pages = {140-158},
year = {2014},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2014.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0010482514000961},
author = {Flávio Luiz Seixas and Bianca Zadrozny and Jerson Laks and Aura Conci and Débora Christina {Muchaluat Saade}},
keywords = {Clinical decision support system, Bayesian network, Dementia, Alzheimer׳s disease, Mild cognitive impairment},
abstract = {Population aging has been occurring as a global phenomenon with heterogeneous consequences in both developed and developing countries. Neurodegenerative diseases, such as Alzheimer׳s Disease (AD), have high prevalence in the elderly population. Early diagnosis of this type of disease allows early treatment and improves patient quality of life. This paper proposes a Bayesian network decision model for supporting diagnosis of dementia, AD and Mild Cognitive Impairment (MCI). Bayesian networks are well-suited for representing uncertainty and causality, which are both present in clinical domains. The proposed Bayesian network was modeled using a combination of expert knowledge and data-oriented modeling. The network structure was built based on current diagnostic criteria and input from physicians who are experts in this domain. The network parameters were estimated using a supervised learning algorithm from a dataset of real clinical cases. The dataset contains data from patients and normal controls from the Duke University Medical Center (Washington, USA) and the Center for Alzheimer׳s Disease and Related Disorders (at the Institute of Psychiatry of the Federal University of Rio de Janeiro, Brazil). The dataset attributes consist of predisposal factors, neuropsychological test results, patient demographic data, symptoms and signs. The decision model was evaluated using quantitative methods and a sensitivity analysis. In conclusion, the proposed Bayesian network showed better results for diagnosis of dementia, AD and MCI when compared to most of the other well-known classifiers. Moreover, it provides additional useful information to physicians, such as the contribution of certain factors to diagnosis.}
}
@article{DANG201930,
title = {A Bayesian Belief Network – Based approach to link ecosystem functions with rice provisioning ecosystem services},
journal = {Ecological Indicators},
volume = {100},
pages = {30-44},
year = {2019},
note = {Sven Erik Jørgensen - Memorial Issue},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X1830308X},
author = {Kinh Bac Dang and Wilhelm Windhorst and Benjamin Burkhard and Felix Müller},
keywords = {Agriculture, Ecosystem service demand, Ecosystem service supply, Ecosystem service budget, Socio-ecological system, Scenario},
abstract = {The complex interactions between environmental and anthropogenic components have significantly influenced rice cultivation. The clear understanding of these interactions is important to (i) optimize rice provisioning ecosystem service (ES) supply, (ii) minimize negative impacts on other ES and (iii) choose suitable strategies for sustainable agriculture. Impacts of environmental and anthropogenic components on rice provisioning ES supply largely depend on site selection and farming practices. The demand for rice can be determined by the size of the population and imports/exports of rice products. Rice provisioning ES supply and demand need to be balanced if the goal is an import-independent and sustainable agriculture. As a decision support tool, Bayesian Belief Networks (BBN) are used for quantifying various ES supply types, demands as well as their budgets. The BBN network presented in this study was developed through interviews, expert knowledge, geographical information systems and statistical models. The results show that the capacity of rice provision can be optimized through site selection and farming practice. The results can help to reduce crop failures and to choose suitable areas for the use of new practices and technologies. Moreover, the presented BBN has been used to forecast future patterns of rice provision through effective or ineffective options of the environmental and human-derived components in eight scenarios. Thereby, the BBN turns out to be a promising decision support tool for agricultural managers in predicting probabilities of success in scenarios of agricultural planning.}
}
@article{JENSEN2023102005,
title = {Machine learning guided development of high-performance nano-structured nickel electrodes for alkaline water electrolysis},
journal = {Applied Materials Today},
volume = {35},
pages = {102005},
year = {2023},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2023.102005},
url = {https://www.sciencedirect.com/science/article/pii/S2352940723002743},
author = {Veronica Humlebæk Jensen and Enzo Raffaele Moretti and Jonas Busk and Emil Howaldt Christiansen and Sofie Marie Skov and Emilie Jacobsen and Mikkel Rykær Kraglund and Arghya Bhowmik and Ragnar Kiebach},
keywords = {Water electrolysis, Nano catalyst, Hydrogen evolution reaction, Bayesian optimization, Technical electrodes, Human in the loop},
abstract = {Utilizing a human in the loop Bayesian optimisation paradigm based on Gaussian process regression, we optimized an Ni electrodeposition method to synthesize nano-structured, high-performance hydrogen evolution reaction electrodes. Via exploration-exploitation stages, the synthesis process variables current density, temperature, ligand concentration and deposition time were optimized influencing the deposition layer morphology and, consequently, hydrogen evolution reaction activity. The resulting structures range from micrometre-sized, star-shaped features to nano-sized sandpaper-like structures with very high specific surface areas and good hydrogen evolution reaction activity. Using the overpotential at 10 mA cm−2 as the figure of merit, hydrogen evolution reaction overpotentials as low as -117 mV were reached, approaching the best known technical high surface area electrodes (e.g. Raney Ni). This is achieved with considerably fewer experiments than what would have been necessary with a linear grid search, as the machine learning model could capture the unintuitive interdependencies of the synthesis variables.}
}
@article{LEE2011868,
title = {Grammatical error simulation for computer-assisted language learning},
journal = {Knowledge-Based Systems},
volume = {24},
number = {6},
pages = {868-876},
year = {2011},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2011.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S095070511100058X},
author = {Sungjin Lee and Jonghoon Lee and Hyungjong Noh and Kyusong Lee and Gary Geunbae Lee},
keywords = {Intelligent grammar tutoring, Corrective feedback, Grammatical error detection, Language learner simulation, Grammar quiz},
abstract = {This paper presents an automated method to generate realistic grammatical errors that can perform crucial functions for advanced technologies in computer-assisted language learning (CALL), including generating corrective feedback in dialog-based CALL (DB-CALL) systems, simulating a language learner to optimize tutoring strategies, and generating context-dependent grammar quizzes as educational materials. The goal of this study is to make grammatical errors generated by automatic simulators more realistic. To generate realistic errors, expert knowledge of language learners’ error characteristics was imported into a statistical modeling system that uses Markov logic, which provides a theoretically sound way to encode knowledge into probabilistic first-order logic. We learned the weights of first-order formulas from a learner corpus. The improved quality of the proposed method was demonstrated through comparative experiments using automatic evaluations (precision and recall rate and Kullback–Leibler divergence between error distributions) and human assessments. The proposed method increased precision by 6% and recall by 8.33% averaged across all proficiency levels. It also exhibited a relative improvement of 37.5% in the average Kullback–Leibler divergence. Judgment by human evaluators showed that the proposed method increased the average scores in two different evaluation tasks by 7 and by 0.411. Finally, we present a measure of labor savings to help predict the time and cost associated with this method for those who plan to exploit grammatical error simulation for their applications. The results indicate that using the proposed method could reduce the grammatical error generation time by 59% in average.}
}
@article{TANG2023102989,
title = {Towards a model of human-cyber–physical automata and a synthesis framework for control policies},
journal = {Journal of Systems Architecture},
volume = {144},
pages = {102989},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102989},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001686},
author = {Xiaochen Tang and Miaomiao Zhang and Wanwei Liu and Bowen Du and Zhiming Liu},
keywords = {HCPS, HCPA, Markov decision process, Model-free reinforcement learning, Model synthesis},
abstract = {Advances in research and increasing applications of Cyber–Physical Systems (CPSs) show the need to consider factors of humans in the loop. This has led to the growing research focus on Human-Cyber–Physical Systems (HCPSs). In general, humans in an HCPS interact with both the cyber and physical systems, as well as among the humans themselves. For a better understanding, correct design, development, operation, and maintenance of HCPSs, a computational theory based on a computational model is required. This paper presents our initial work towards a model of human-cyber–physical automata (HCPA). We consider an HCPS as a combination of a human-physical system (HPS) and a CPS in which the control switches between the humans and the machines. We define an HCPA by connecting the automaton of the HPS and the automaton of the CPS through a switch control automaton. The switch control automaton makes switching decision in some critical states shared by the HPS and the CPS. Our theorem shows that the control switching between the HPS and the CPS increases the probability of satisfying a given property. We model the behaviour of a human in specified applications or even in carry out specific tasks, instead of general human intelligence. Therefore, a human can make mistakes to decision making and thus it is a probabilistic automaton with learning ability. The switching between the human and the machine is modelled by an oracle. The oracle learns about the human behaviour, the machine behaviour, as well as the environment to make the control decisions. To generate the control policies of the human and the oracle, we propose a synthesis framework to maximize the probability of the satisfaction of a property specified in Linear Temporal Logic (LTL) by the HCPA. We present a prototype implementation of the framework by extending the model-free reinforcement learning (RL) algorithm and model-free deep-RL algorithm, and our experiment shows that our synthesis framework is effective in obtaining switch policies.}
}
@article{LENG2024111713,
title = {An adaptive convolutional neural network based on transmissibility grayscale image for online identification of offshore platform damage pattern},
journal = {Mechanical Systems and Signal Processing},
volume = {221},
pages = {111713},
year = {2024},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111713},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024006113},
author = {Jiancheng Leng and Jinyong Ma and Huiyu Feng},
keywords = {Damage pattern recognition, Convolutional neural network, Transmissibility grayscale image, Attention mechanism, Variational Bayesian},
abstract = {In response to the challenges in offshore platform damage pattern recognition, where traditional methods lack adaptability owing to relying on feature extraction and expert knowledge, a novel method for online recognition of offshore platform damage patterns using an improved convolutional neural network (CNN) based on transmissibility grayscale image is proposed. The original transmissibility function is converted into a two-dimensional grayscale image to capture the essential features in the original signal, which is then used as the input for the deep learning model. Subsequently, an attention mechanism is introduced to enable the CNN to focus on key areas of the image, emphasize important feature channels, dynamically adjust feature representations, and enhance its overall performance. Meanwhile, the model is optimized by virtue of variational Bayesian algorithm and early stopping criteria, and finally, the trained offline model is applied to online damage pattern recognition of offshore platforms. Through on indoor vibration monitoring experiments, seven different damage patterns of the platform model are recognized accurately, which show that the accuracy of the improved CNN model is increased by an average of 8.5 % as compared to traditional intelligent methods. Furthermore, the proposed method is applied to the online damage identification of offshore platforms in the field, and the corresponding prediction accuracy reaches 98.9 %, demonstrating the feasibility and generalizability of the proposed method in adaptive feature extraction and precise identification of damage patterns.}
}
@article{MAHMOOD2024205283,
title = {Optimizing natural gas pipeline risk assessment using hybrid fuzzy bayesian networks and expert elicitation for effective decision-making strategies},
journal = {Gas Science and Engineering},
volume = {125},
pages = {205283},
year = {2024},
issn = {2949-9089},
doi = {https://doi.org/10.1016/j.jgsce.2024.205283},
url = {https://www.sciencedirect.com/science/article/pii/S2949908924000797},
author = {Yasir Mahmood and Jessica Chen and Nita Yodo and Ying Huang},
keywords = {Risk assessment, Fuzzy set theory, Cause and effect, Fuzzy bayesian network, Pipeline failure, External and internal failure factors},
abstract = {Natural gas pipelines are susceptible to external and internal risk factors, such as corrosion, environmental conditions, external interferences, construction and design faults, and equipment failures. Bayesian Networks (BN) is a promising risk assessment approach widely used to evaluate these risk factors. One of BN's inherent limitations is its inability to accurately capture statistical dependencies and causal relationships, which can be overcome by incorporating expert elicitation into BN. To account for uncertainty and vagueness in assessing pipeline failure risks, fuzzy set theory (FST) can be combined with BN, commonly known as Fuzzy Bayesian Networks (FBN). This study developed an FBN framework that uses linguistic variables to calculate fuzzy probability (FPr) through domain expert elicitation, and crisp probabilities (CPr) are computed using historical incident data from the Pipeline and Hazardous Materials Safety Administration (PHMSA). Based on the findings from the case study of the Midwest region of the USA, external factors, i.e., third-party interference, outside force, and other incidents, significantly impact pipeline performance and reliability. Diagnosis inference indicates that in the Midwest region of the USA, pipeline material and age are critical factors leading to corrosion failure by threatening pipeline integrity. The findings from this study suggested that a targeted risk mitigation strategy is paramount for minimizing the risks associated with pipeline networks.}
}
@article{BOUYAKHSAINE2024114519,
title = {Prediction of residential building occupancy using Machine learning with integrated sensor and survey Data: Insights from a living lab in Morocco},
journal = {Energy and Buildings},
volume = {319},
pages = {114519},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114519},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824006352},
author = {Khadija Bouyakhsaine and Abderrahim Brakez and Mohcine Draou},
keywords = {Occupancy prediction, Machine learning, Data-driven, Internet of Things, Survey data, Living lab},
abstract = {Building occupancy information is essential for effective energy management in buildings through the adoption of energy conservation and occupant-centric control strategies. These strategies endeavor to contribute to optimizing energy consumption while ensuring occupant comfort. This study focuses on advanced occupancy modeling techniques to enhance energy efficiency in residential buildings, utilizing various data-driven techniques. Various Machine Learning models, such as Random Forest, Bayesian Network, Decision Trees, Support Vector Machines, K-Nearest Neighbors, eXtreme Gradient Boosting, and Regularized Greedy Forest, have been evaluated for predicting and classifying building occupancy. Employing a living lab approach within a residential setting, the research evaluates model performance using two ground truth datasets: IoT sensor and survey data. Experimental tests use Accuracy and F1_score as evaluation criteria, demonstrating accuracy rates ranging from 70% to 95.96%. The results highlight the performance of these models in predicting residential occupancy, offering valuable insight into two approaches to occupancy modeling. The Random Forest model performs exceptionally well in capturing occupancy trends, while the Bayesian Network model, combined with expert knowledge, provides detailed predictions of occupancy types and zones. The research also addresses challenges related to data collection, including privacy concerns. It presents effective occupancy modeling strategies for residential energy management.}
}
@article{SOARES20241469,
title = {Recommendations on the Use of Structured Expert Elicitation Protocols for Healthcare Decision Making: A Good Practices Report of an ISPOR Task Force},
journal = {Value in Health},
volume = {27},
number = {11},
pages = {1469-1478},
year = {2024},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524028432},
author = {Marta Soares and Abigail Colson and Laura Bojke and Salah Ghabri and Osvaldo Ulises Garay and Jenna K. Felli and Karen Lee and Elizabeth Molsen-David and Oswaldo Morales-Napoles and Victoria A. Shaffer and Maarten J. IJzerman},
keywords = {Bayesian estimation, IDEA protocol, modified Delphi, SHELF, structured expert elicitation, uncertainty},
abstract = {Healthcare decision making, including regulatory and reimbursement decisions, is based on uncertain assessments of clinical and economic value. This arises from the evidence supporting those assessments being uncertain, incomplete, or even absent. Qualitative, structured expert elicitation (SEE) is a valuable tool for extracting expert knowledge about an uncertain quantity and formulating that knowledge as a probability distribution. This creates a useful input to decision modeling and support, particularly in areas with limited evidence, such as advanced therapy products, precision medicine, rare diagnoses, and other areas with high uncertainty. Structured SEE protocols are used to improve the transparency, accuracy, and consistency of quantitative judgments from experts, limiting the effect of heuristics and biases. This task force report introduces 5 commonly used protocols for SEE (Sheffield elicitation framework; modified Delphi method; Cooke’s classical method; investigate, discuss, estimate, aggregate protocol; and the Medical Research Council reference protocol). It describes the common elements of SEE, discusses how these protocols differ in their implementation of those elements and illustrates the use of the protocols. The report then reviews the relevant constraints on implementing SEE within the context of healthcare decision making and considers the strengths and weaknesses of these protocols in light of those considerations. Because this is an introductory report on an emerging topic, specific recommendations on practice are not made. However, there are broad recommendations based on the suitability of the different protocols in various decision contexts. The report concludes with recommendations for further research to better guide future practice.}
}
@article{NEIL2025110948,
title = {Calibrating multi-constraint ensemble ecosystem models using genetic algorithms and Approximate Bayesian Computation: A case study of rewilding at the Knepp Estate, UK},
journal = {Ecological Modelling},
volume = {500},
pages = {110948},
year = {2025},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110948},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024003363},
author = {Emily Neil and Ernesto Carrella and Richard Bailey},
keywords = {Ensemble ecosystem modelling, Lotka-Volterra, Rewilding, Species reintroductions, Calibration, Genetic algorithms, Optimisation},
abstract = {This paper presents a new ensemble ecosystem model (EEM) which predicts the impacts of species reintroductions and optimises potential future management interventions at the Knepp Estate rewilding project, UK. Compared to other EEMs, Knepp has a relatively high level of data availability that can be used to constrain the model, including time-series abundance data and expert knowledge. This could improve the realism of outputs and enable more nuanced and context-specific management intervention recommendations. Calibrating EEMs can be challenging, however, and as the number of constraints increases, so does the complexity of the model fitting process. We use a new Genetic Algorithm-Approximate Bayesian Computation (GA-ABC) approach wherein GA outputs are used to inform the prior distributions for ABC. To reduce the parameter search space, we fixed twelve parameters - the consumer self-interaction strengths αi,iand negative growth rates – based on theoretical assumptions. While the GA-ABC method proved effective at efficiently searching the parameter space and optimising multiple constraints, it was computationally intensive and struggled to identify a broad range of outputs. Ultimately, this led to an ensemble of models with similar trajectories. Several potential ways to address this are discussed. Our results reinforce the findings of previous studies that the EEM methodology has potential for guiding conservation management and decision-making. Outputs suggest that reintroducing large herbivores was key to maintaining a diverse grassland-scrubland-woodland ecosystem, and optimisation experiments informed species characteristics and stocking densities needed to achieve specific goals. Ultimately, refining the EEM methodology to improve calibration and facilitate the integration of additional data will enhance its utility for ecosystem management, helping to achieve more effective and informed outcomes.}
}
@article{DENG2021106854,
title = {Build complementary models on human feedback for simulation to the real world},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106854},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106854},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001179},
author = {Zixuan Deng and Yanping Xiang and Zhongfeng Kang},
keywords = {Safe reinforcement learning, Human-in-the-loop reinforcement learning, Markov decision processes, Supervised learning},
abstract = {Using simulators is a cost-effective way to meet human needs. Nevertheless, inevitable errors derived from the gap between simulation and the real world sometimes cause great losses and must be taken seriously. This paper focuses on one cause of the gap, which is the incomplete state representation in simulation, and proposes a supervised learning approach, correcting human-unacceptable policies calculated by simulators, based on human feedback. The approach first detects the related blind spots by classifiers which are trained on data from aggregation of noisy human feedback. Then, it corrects the human-unacceptable policies through the complementary model presented based on linear function approximation (LFA) and a policy iteration algorithm FRU-SADPP that uses radial basis functions (RBFs). We evaluate our approach on two simulated domains and demonstrate its higher accuracy of policies than two baselines, in terms of three typical kinds of human suboptimality and human errors, and three types of human feedback. Experiments also show the scalability of our approach.}
}
@article{ECHEVESTE202443,
title = {Enhancing Hip Exoskeleton Tuning Performance with Machine Learning: An Anthropometric Data-Driven Approach},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {28},
pages = {43-48},
year = {2024},
note = {The 4th Modeling, Estimation, and Control Conference – 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324023413},
author = {Salvador Echeveste and Md. Safwan Mondal and Subramanian Ramasay and Pranav A. Bhounsule},
keywords = {Assistive, Rehabilitation Robotics, Optimal Control, Machine Learning in modeling, estimation, control},
abstract = {Hip exoskeletons offer significant potential for enhancing human movement, especially for those with mobility impairments. However, optimizing their performance typically involves lengthy discrete and continuous optimization methods. To address this, we propose a novel approach using machine learning to predict controller parameter classes, aiming to improve the tuning process. Our method relies on subject-specific anthropometric data to predict optimal controller parameters for hip exoskeletons. Through a machine learning framework, we develop predictive models to determine the most effective parameter settings tailored to individual users. By employing feature engineering, data synthesis techniques, and model training, we enhance the initialization of Bayesian Human-in-the-loop (HIL) optimization. Results indicate that our machine learning models can predict control parameter classes with 75% accuracy, leading to a 9.98% improvement in optimized exoskeleton performance for users.}
}
@article{GRAY2015404,
title = {An Abductive Diagnosis and Modeling Concept for Wind Power Plants},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {21},
pages = {404-409},
year = {2015},
note = {9th IFAC Symposium on Fault Detection, Supervision andSafety for Technical Processes SAFEPROCESS 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.09.560},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315016894},
author = {Christopher S. Gray and Roxane Koitz and Siegfried Psutka and Franz Wotawa},
keywords = {Fault diagnosis, Model-based diagnosis, ATMS, Abductive diagnosis, Wind turbines},
abstract = {The number and complexity of industrial wind turbine installations have increased significantly over the last decades. As maintenance costs are high and down-times lead to substantial revenue loss, increasing the reliability and optimizing the maintenance process are crucial tasks from an industrial perspective. However, many of the proposed diagnosis systems merely focus on parts of the turbine or only locate a portion of the faults. Model-based diagnosis has been applied successfully in industrial settings and further provides a solid theoretical background. Therefore, we propose a model-based approach depending on automatically retrieved health variables and on an extensive expert knowledge on specific component-oriented failure modes as well as their effects on measurable signals. As the expert assessment provides causal links between faults and their manifestations, we formally create a Propositional Horn Clause Abduction Problem (PHCAP). In this paper, we present a modeling concept taking advantage of existing expert knowledge and show how it can be used for wind turbine diagnosis employing already existing algorithms and structures. Our models enable us to directly determine root causes from the links between malfunctions to observable turbine signals on a system level with a relatively low effort compared to other approaches.}
}
@article{LIU2024545,
title = {Task graph offloading via deep reinforcement learning in mobile edge computing},
journal = {Future Generation Computer Systems},
volume = {158},
pages = {545-555},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001638},
author = {Jiagang Liu and Yun Mi and Xinyu Zhang and Xiaocui Li},
keywords = {Mobile edge computing, Task graph, Computation offloading, Reinforcement learning},
abstract = {Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Markov Decision Process (MDP). Then, we design a deep reinforcement learning algorithm (SATA-DRL) to learn the task scheduling strategy from the interaction with the environment, to improve user experience. Extensive simulations validate that SATA-DRL is superior to existing strategies in terms of reducing average makespan and deadline violation.}
}
@article{RIVAS200727,
title = {Application of Bayesian networks to the evaluation of roofing slate quality},
journal = {Engineering Geology},
volume = {94},
number = {1},
pages = {27-37},
year = {2007},
issn = {0013-7952},
doi = {https://doi.org/10.1016/j.enggeo.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0013795207001299},
author = {T. Rivas and J.M. Matías and J. Taboada and A. Argüelles},
keywords = {Roofing slate, Bayesian networks, Quality index, Ornamental stone},
abstract = {We present the results of an application of Bayesian networks to the evaluation of the quality of roofing slate. Using data from borehole samples of a slate deposit, two networks constructed with different levels of expert knowledge input were evaluated for their capacities for inference and prediction of the quality of slate for roofing. It can be concluded from the results that Bayesian networks are an extremely useful automated tool for evaluating the quality of a resource such as slate for the following reasons: they allow final quality to be assessed immediately and in probabilistic terms with a tolerable degree of uncertainty; they enable the probability of obtaining different final qualities to be estimated when new data is introduced into the network; they speed up the evaluation process by simulating and optimising the work of the expert (during field data collection and borehole description) in identifying the parameters with the greatest influence on final quality; and finally, they have a satisfactory capacity for prediction.}
}
@article{PINERODEPLAZA2025105810,
title = {Human-centred AI for emergency cardiac care: Evaluating RAPIDx AI with PROLIFERATE_AI},
journal = {International Journal of Medical Informatics},
volume = {196},
pages = {105810},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.105810},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625000279},
author = {Maria Alejandra {Pinero de Plaza} and Kristina Lambrakis and Fernando Marmolejo-Ramos and Alline Beleigoli and Mandy Archibald and Lalit Yadav and Penelope McMillan and Robyn Clark and Michael Lawless and Erin Morton and Jeroen Hendriks and Alison Kitson and Renuka Visvanathan and Derek P. Chew and Carlos Javier {Barrera Causil}},
keywords = {Artificial intelligence, Emergency medicine, Decision support, Cardiac biomarkers, Usability, Adoption, Human-centred evaluation},
abstract = {Background
Chest pain diagnosis in emergency care is hindered by overlapping cardiac and non-cardiac symptoms, causing diagnostic uncertainty. Artificial Intelligence, such as RAPIDx AI, aims to enhance accuracy through clinical and biochemical data integration, but its adoption relies on addressing usability, explainability, and seamless workflow integration without disrupting care.
Objective
Evaluate RAPIDx AI’s integration into clinical workflows, address usability barriers, and optimise its adoption in emergencies.
Methods
The PROLIFERATE_AI framework was implemented across 12 EDs (July 2022–January 2024) with 39 participants: 15 experts co-designed a survey via Expert Knowledge Elicitation (EKE), applied to 24 ED clinicians to assess RAPIDx AI usability and adoption. Bayesian inference, using priors, estimated comprehension, emotional engagement, usage, and preference, while Monte Carlo simulations quantified uncertainty and variability, generating posterior means and 95% bootstrapped confidence intervals. Qualitative thematic analysis identified barriers and optimisation needs, with data triangulated through the PROLIFERATE_AI scoring system to rate RAPIDx AI’s performance by user roles and demographics.
Results
Registrars exhibited the highest comprehension (median: 0.466, 95 % CI: 0.41–0.51) and preference (median: 0.458, 95 % CI: 0.41–0.48), while residents/interns scored the lowest in comprehension (median: 0.198, 95 % CI: 0.17–0.26) and emotional engagement (median: 0.112, 95 % CI: 0.09–0.14). Registered nurses showed strong emotional engagement (median: 0.379, 95 % CI: 0.35–0.45). Novice users faced usability and workflow integration barriers, while experienced clinicians suggested automation and streamlined workflows. RAPIDx AI scored “Good Impact,” excelling with trained users but requiring targeted refinements for novices.
Conclusion
RAPIDx AI enhances diagnostic accuracy and efficiency for experienced users, but usability challenges for novices highlight the need for targeted training and interface refinements. The PROLIFERATE_AI framework offers a robust methodology for evaluating and scaling AI solutions, addressing the evolving needs of sociotechnical systems.}
}
@article{DENEAULT2025723,
title = {Preferential Bayesian optimization improves the efficiency of printing objects with subjective qualities†‡†Data and code from this study are archived at https://osf.io/fqp96/.‡Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00320a},
journal = {Digital Discovery},
volume = {4},
number = {3},
pages = {723-737},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00320a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2500035X},
author = {James R. Deneault and Woojae Kim and Jiseob Kim and Yuzhe Gu and Jorge Chang and Benji Maruyama and Jay I. Myung and Mark A. Pitt},
abstract = {Despite recent advances in closed-loop 3D printing, optimizing subjective and difficult-to-quantify qualities—such as surface finish and clarity of fine detail—remains a significant challenge, often relying on the traditional time-consuming and inefficient trial-and-error process. Preferential Bayesian optimization (PBO) is a machine learning technique that uses human preference judgements to efficiently guide the search for such abstract optimums in a high-dimensional space. We evaluated PBO's ability to identify optimal parameter values in printing profiles of vases and pairs of 3D cones. In semi-autonomous printing campaigns, a human observer ranked triplets of images of these objects with a target object in mind, preferring slender/bulbous vases and cone pairs that were smooth and well-formed. Results show that PBO consistently and quickly identified an optimal parameter combination across repeated testing. Modeling was then used to identify object dimensions responsible for preference judgements and to mimic preference behavior. Findings suggest that PBO is a promising tool for expanding the range of 3D objects that can be printed efficiently.}
}
@article{SOUSA2018156,
title = {Combination of expert decision and learned based Bayesian Networks for multi-scale mechanical analysis of timber elements},
journal = {Expert Systems with Applications},
volume = {93},
pages = {156-168},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306723},
author = {Hélder S. Sousa and Francisco Prieto-Castrillo and José C. Matos and Jorge M. Branco and Paulo B. Lourenço},
keywords = {Bayesian Networks, Timber, Multi-scale analysis, Expert systems, Learning algorithms, Ranking},
abstract = {The use of Bayesian Networks allows to organize and correlate information gathered from different sources and its optimization may incorporate restrictions adjusting the network based on expert knowledge and network operativeness, in such a way that it may satisfactorily represent a given domain. The main goal of this paper is to study if an optimized learned Bayesian Network may be used as a prior structure for an expert based network of an engineering structural material analysis. The methodology is applied to a database of results from an experimental campaign that focused on the mechanical characterization of timber elements recovered from an early 20th century building. To that study case it is evidenced that through a suitable combination of model averaging and supervision steps it is possible to achieve robust and reliable models to underpin the causal structure of a typical multi-scale timber analysis.}
}
@article{PERRIN2024106700,
title = {Towards a configurable and non-hierarchical search space for NAS},
journal = {Neural Networks},
volume = {180},
pages = {106700},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106700},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006245},
author = {Mathieu Perrin and William Guicquero and Bruno Paille and Gilles Sicard},
keywords = {Neural architecture search, Convolutional neural network, Wasserstein autoencoder, Bayesian optimization},
abstract = {Neural Architecture Search (NAS) outperforms handcrafted Neural Network (NN) design. However, current NAS methods generally use hard-coded search spaces, and predefined hierarchical architectures. As a consequence, adapting them to a new problem can be cumbersome, and it is hard to know which of the NAS algorithm or the predefined hierarchical structure impacts performance the most. To improve flexibility, and be less reliant on expert knowledge, this paper proposes a NAS methodology in which the search space is easily customizable, and allows for full network search. NAS is performed with Gaussian Process (GP)-based Bayesian Optimization (BO) in a continuous architecture embedding space. This embedding is built upon a Wasserstein Autoencoder, regularized by both a Maximum Mean Discrepancy (MMD) penalization and a Fully Input Convex Neural Network (FICNN) latent predictor, trained to infer the parameter count of architectures. This paper first assesses the embedding’s suitability for optimization by solving 2 computationally inexpensive problems: minimizing the number of parameters, and maximizing a zero-shot accuracy proxy. Then, two variants of complexity-aware NAS are performed on CIFAR-10 and STL-10, based on two different search spaces, providing competitive NN architectures with limited model sizes.}
}
@article{PAVLOVSKII2022101772,
title = {Hybrid genetic predictive modeling for finding optimal multipurpose multicomponent therapy},
journal = {Journal of Computational Science},
volume = {63},
pages = {101772},
year = {2022},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2022.101772},
url = {https://www.sciencedirect.com/science/article/pii/S1877750322001545},
author = {Vladislav V. Pavlovskii and Ilia V. Derevitskii and Sergey V. Kovalchuk},
keywords = {Optimal drug combinations, Synergy drugs, Drug effect predicting, Chronic disease therapy, Machine learning, Genetic optimization, Bayesian networks, Diabetes mellitus},
abstract = {This article proposes a new approach to calculating optimal therapy for the treatment of chronic diseases. For all patients, this approach can calculate drug combinations with the best probability of achieving treatment goals. This approach is based on the combination of machine learning methods, probability graph models, classical statistical modeling tools, and the authors’ algorithms. The paper shows the advantages of the proposed approach for the practical medical task of selecting drug combinations for type 2 diabetes mellitus treatment. The authors created useful tools for calculating drug combinations for compensating carbohydrate metabolism for diabetes patients. For treatment goals and values, the research team used the main carbohydrate metabolism indicator – glycated hemoglobin, the lipid profile indicator – low-density lipoprotein cholesterol, and also arterial blood pressure which is an important indicator of a patient’s condition. In the virtual implementation, the method showed higher quality in comparison to results obtained without using this approach. For validation, classic metrics and experts’ knowledge were used. Both types of validation showed that the method was of high quality and did not contradict fundamental medicine knowledge. Therefore, this method can be used as part of a decision support system for medical specialists who work with type 2 diabetes mellitus patients. This paper is an extended version of the investigation [1]. The short version also proposed an approach for calculating optimal therapy. Although this approach is of good quality, it had some drawbacks, so some improvements were made in the new version of the algorithm. The authors used Bayesian networks to reduce the search space by selecting only essential groups of drugs. Moreover, a genetic algorithm was used to find a more accurate solution.}
}
@article{HOBBALLAH201895,
title = {Formulating preliminary design optimization problems using expert knowledge: Application to wood-based insulating materials},
journal = {Expert Systems with Applications},
volume = {92},
pages = {95-105},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306401},
author = {Mohamad Hussein Hobballah and Amadou Ndiaye and Franck Michaud and Mark Irle},
keywords = {Knowledge acquisition, Knowledge domain decomposition, Experts knowledge, Causal maps, Bio-based material, Thermal insulation},
abstract = {The design of wood-fiber based thermal insulating material with optimized properties and characteristics requires a good scientific knowledge of the latter. Currently, the technical, economic and ecological characteristics of a wood-fiber based composite mat are not well known. This article presents a methodology for the acquisition of expert knowledge so that the properties and characteristics can be modeled. The knowledge domain is multidisciplinary and it was delimited and decomposed into disciplines and domains of expertise. A panel of seven experts was constituted to cover the various disciplines and domains of expertise of the knowledge domain. Knowledge acquisition sessions, guided by the estimated importance and the availability of knowledge, were conducted using semi-structured interviews and the mapping of the existing causal relations between variables. A causal map was established to represent the causal knowledge of each of the experts and then, the established causal maps were assembled into a unique global causal map, which was subsequently validated by the experts. It contains the information necessary for formulating the properties and characteristics to be optimized, which were: thermal conductivity; thickness recovery of the material; the manufacturing cost and the product's environmental impact. Properties and characteristics are function of raw material type, their morphological properties and the manufacturing process variables. This methodology makes it possible to establish which objectives to optimize and which variables influence each objective. Consequently, the objective functions of the optimization problem can be clarified, specified and modeled.}
}
@incollection{WALLACH2014277,
title = {Chapter 7 - Parameter Estimation with Bayesian Methods},
editor = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
booktitle = {Working with Dynamic Crop Models (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {277-309},
year = {2014},
isbn = {978-0-12-397008-4},
doi = {https://doi.org/10.1016/B978-0-12-397008-4.00007-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970084000071},
author = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
keywords = {Bayes, MCMC, Parameter estimation, Posterior, Prior, Uncertainty},
abstract = {Bayesian methods are becoming increasingly popular for estimating parameters of complex mathematical models because the Bayesian approach provides a coherent framework for dealing with uncertainty. To start with, the principle is a prior probability distribution of the model parameters describing our belief about the parameter values before we use the set of measurements. The Bayesian methods then tell us how to update this belief using the measurements to give the posterior parameter density. In the Bayesian approach, the parameters are defined as random variables and the prior and posterior parameter distributions represent our belief about parameter values before and after using observed data to improve estimates. This approach has several advantages: i) parameters can be estimated from different types of information (data, literature, expert knowledge); ii) the posterior probability distribution can be used to implement uncertainty analysis methods; iii) the posterior probability distribution can be used for optimizing decisions in the face of uncertainty. This chapter presents the basic principles of the Bayesian approach and describes several algorithms to calculate posterior parameter distributions. These algorithms are illustrated in several applications on yield and soil carbon estimation.}
}
@article{LAURENT2014318,
title = {Bayesian object-based estimation of LAI and chlorophyll from a simulated Sentinel-2 top-of-atmosphere radiance image},
journal = {Remote Sensing of Environment},
volume = {140},
pages = {318-329},
year = {2014},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2013.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0034425713003088},
author = {Valérie C.E. Laurent and Michael E. Schaepman and Wout Verhoef and Joerg Weyermann and Roberto O. Chávez},
keywords = {Top-of-atmosphere radiance, Sentinel-2, APEX, Variable estimation, Bayesian optimization, Object-based, Coupled model, Radiative transfer, SLC, MODTRAN4, Li–Ross, Nadir-normalization},
abstract = {Leaf area index (LAI) and chlorophyll content (Cab) are important vegetation variables which can be monitored using remote sensing (RS). Physically-based approaches have higher transferability and are therefore better suited than empirically-based approaches for estimating LAI and Cab at global scales. These approaches, however, require the inversion of radiative transfer (RT) models, which is an ill-posed and underdetermined problem. Four regularization methods have been proposed, allowing finding stable solutions: 1) model coupling, 2) using a priori information (e.g. Bayesian approaches), 3) spatial constraints (e.g. using objects), and 4) temporal constraints. For mono-temporal data, only the first three methods can be applied. In an earlier study, we presented a Bayesian object-based algorithm for inverting the SLC-MODTRAN4 coupled canopy-atmosphere RT model, and compared it with a Bayesian LUT inversion. The results showed that the object-based approach provided more accurate LAI estimates. This study, however, heavily relied on expert knowledge about the objects and vegetation classes. Therefore, in this new contribution, we investigated the applicability of the Bayesian object-based inversion of the SLC-MODTRAN4 model to a situation where no such knowledge was available. The case study used a 16×22km2 simulated top-of-atmosphere image of the upcoming Sentinel-2 sensor, covering the area near the city of Zurich, Switzerland. Seven APEX radiance images were nadir-normalized using the parametric Li–Ross model, spectrally and spatially resampled to Sentinel-2 specifications, geometrically corrected, and mosaicked. The atmospheric effects between APEX flight height and top-of-atmosphere level were added based on two MODTRAN4 simulations. The vegetation objects were identified and delineated using a segmentation algorithm, and classified in four levels of brightness in the visible domain. The LAI and Cab maps obtained from the Bayesian object-based inversion of the coupled SLC-MODTRAN4 model presented realistic spatial patterns. The impact of the parametric Li–Ross nadir-normalization was evaluated by comparing 1) the angular signatures of the SLC-MODTRAN4 and Li–Ross models, and 2) the LAI and Cab maps obtained from a Li–Ross nadir-normalized image (using nadir viewing geometry) and from the original image (using the original viewing geometry). The differences in angular signatures were small but systematic, and the differences between the LAI and Cab maps increased from the center towards the edges of the across-track direction. The results of this study contribute to preparing the RS community for the arrival of Sentinel-2 data in the near future, and generalize the applicability of the Bayesian object-based approach for estimating vegetation variables to cases where no field data are available.}
}
@article{SCHUH20231143,
title = {Heuristic Guided Hierarchical Reinforcement Learning Approach For The Economic Improvement Of Production Lines},
journal = {Procedia CIRP},
volume = {120},
pages = {1143-1148},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.139},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123008715},
author = {Günther Schuh and Seth Schmitz and Jan Maetschke and Benedict Janssen and Hanna Offermanns},
keywords = {Reinforcement Learning, Production Lines, Simulation, Discrete Event Simulation-based Optimization, Performance, Markov Decision Problem},
abstract = {The increasing variety of products poses a challenge for efficient manufacturing on production lines due to resulting small batch sizes and frequent product changes, which lower the average overall effectiveness. Especially for industries such as the Fast Moving Consumer Goods (FMCG) industry that manufacture at high speed on production lines, it is mandatory to increase the performance of production lines in an economical way. Due to the complexity of such production lines, identifying efficient actions and combining them into economic improvement trajectories is challenging. There are numerous approaches based on different concepts, such as simulation-based heuristics, to solve these challenges. In other application areas, reinforcement learning has shown remarkable success in recent years, and first reinforcement learning-based approaches to this specific problem can be found. However, these approaches mostly focus on details instead of providing a holistic view of the possibilities to improve a production line or are limited in their practical application due to lack of integration of existing expert knowledge or limited quality of results. For this reason, this paper proposes a hierarchical reinforcement learning approach that combines discrete event simulation with a heuristically driven multi-agent system. Thus, the selection of the improvement strategy of the production line is performed by one agent, and the dedicated improvement of specific parameters is performed by specialized subordinate other agents. Through this hierarchical multi-agent system, on the one hand, the learning rate can be increased. On the other hand, by guiding the agents through a heuristic based on expert knowledge, the learning quality is increased.}
}
@article{VALVERDE2023106657,
title = {Causal reinforcement learning based on Bayesian networks applied to industrial settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106657},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106657},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008412},
author = {Gabriel Valverde and David Quesada and Pedro Larrañaga and Concha Bielza},
keywords = {Reinforcement learning, Bayesian networks, Causality, Parameter learning, Dynamic simulators, Ordinary differential equations},
abstract = {The increasing amount of real-time data collected from sensors in industrial environments has accelerated the application of machine learning in decision-making. Reinforcement learning (RL) is a powerful tool to find optimal policies for achieving a given goal. However, RL’s typical application is risky and insufficient in environments where actions can have irreversible consequences and require interpretability and fairness. While new trends in RL may provide guidance based on expert knowledge, they do not often consider uncertainty or include prior knowledge in the learning process. We propose a causal reinforcement learning alternative based on Bayesian networks (RLBNs) to address this challenge. The RLBN simultaneously models a policy and takes advantage of the joint distribution of the state and action space, reducing uncertainty in unknown situations. We propose a training algorithm for the network’s parameters and structure based on the reward function and likelihood of the effects and measurements taken. Our experiment with the CartPole benchmark and industrial fouling using ordinary differential equations (ODEs) demonstrates that RLBNs are interpretable, secure, flexible, and more robust than their competitors. Our contributions include a novel method that incorporates expert knowledge into the decision-making engine. It uses Bayesian networks with a predefined structure as a causal graph and a hybrid learning strategy that considers both likelihood and reward. This would avoid losing the virtues of the Bayesian network.}
}
@article{MROWCZYNSKA2022116035,
title = {A new fuzzy model of multi-criteria decision support based on Bayesian networks for the urban areas' decarbonization planning},
journal = {Energy Conversion and Management},
volume = {268},
pages = {116035},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116035},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422008251},
author = {M. Mrówczyńska and M. Skiba and A. Leśniak and A. Bazan-Krzywoszańska and F. Janowiec and M. Sztubecka and R. Grech and J.K. Kazak},
keywords = {Cities' sustainable development, Renewable energy sources, Energy policy scenarios, Bayesian network, Fuzzy Analytical Hierarchy Process, Geographic Information System},
abstract = {The study introduces a framework for forecasting and decision-making in multi-criteria processes and proposes their application in the decarbonization of urban areas. Optimizing the multi-criteria decision-making process is an integrated set of information-processing-decision activities in which actual data, expert knowledge using fuzzy inference rules, Geographic Information System, and Bayesian networks are combined. Using proposed tools leads to designing a new approach to improving the energy efficiency of cities and reducing CO2 emissions using renewable energy. The integration of modern computational methods leads to rational planning of environmentally friendly and energy-conscious smart cities by the provisions of the Fit for 55 packages. The effectiveness of the proposed approach has been demonstrated in the example of three scenarios considering different types of renewable energy sources that can be implemented in urban areas. The success probability of decarbonizing these areas was calculated for defined quarters of the city of Zielona Góra with different parameters. Thereby the usefulness of the method was confirmed. Significantly, the likelihood of a successful deployment of photovoltaics (PV) in urban areas was estimated at 55.25% and for heat pumps at 28.79%. The proposed method enables a clear interpretation of the results, which may be the basis for urban energy efficiency planning.}
}
@article{FANG2020103495,
title = {Human-in-the-loop optimization of wearable robots to reduce the human metabolic energy cost in physical movements},
journal = {Robotics and Autonomous Systems},
volume = {127},
pages = {103495},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103495},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019308036},
author = {Jing Fang and Yuan Yuan},
keywords = {Wearable robotics, Human–robot interaction, Human-in-the-loop design},
abstract = {Most designs of wearable robots are based on human biomechanical statistics, engineering experience or individual experiments. Despite great successes, few of them consider the human–robot integration and individual differences between users. Additionally, the design periods, cost and safety also need to be further improved. Learning from the natural driving mechanism of human body, we propose a general human-in-the-loop (HIL) optimization designing approach for this kind of wearable robots. Firstly, the human–robot coupling model of the personalized wearable robot and the human musculoskeletal model are established. Then, the Computed Muscle Control (CMC) tool embedded in software OpenSim and the Bayesian optimization used in machine learning are combined to find the optimal design scheme for the personalized wearable robots to reduce the human metabolic energy cost in specific physical movement. The HIL approach could not only optimize the control parameters of wearable robots, but also optimize their geometry, material and any other design parameters flexibly and effectively. An application example for the HIL approach is also provided to help designers better understand and use the HIL method proposed in this paper.}
}
@article{PRATIUSH2024252,
title = {Scientific exploration with expert knowledge (SEEK) in autonomous scanning probe microscopy with active learning††This manuscript has been co-authored by UT-Battelle, LLC, under contract DE-AC0500OR22725 with the US Department of Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).},
journal = {Digital Discovery},
volume = {4},
number = {1},
pages = {252-263},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00277f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24002249},
author = {Utkarsh Pratiush and Hiroshi Funakubo and Rama Vasudevan and Sergei V. Kalinin and Yongtao Liu},
abstract = {Microscopy plays a foundational role in materials science, biology, and nanotechnology, offering high-resolution imaging and detailed insights into properties at the nanoscale and atomic level. Microscopy automation via active machine learning approaches is a transformative advancement, offering increased efficiency, reproducibility, and the capability to perform complex experiments. Our previous work on autonomous experimentation with scanning probe microscopy (SPM) demonstrated an active learning framework using deep kernel learning (DKL) for structure–property relationship discovery. Here we extend this approach to a multi-stage decision process to incorporate prior knowledge and human interest into DKL-based workflows, we operationalize these workflows in SPM. By integrating expected rewards from structure libraries or spectroscopic features, we enhanced the exploration efficiency of autonomous microscopy, demonstrating more efficient and targeted exploration in autonomous microscopy. These methods can be seamlessly applied to other microscopy and imaging techniques. Furthermore, the concept can be adapted for general Bayesian optimization in material discovery across a broad range of autonomous experimental fields.}
}
@article{KUMAR2012687,
title = {Integrated modelling for Sustainability Appraisal for Urban River Corridor (re)-development},
journal = {Procedia Environmental Sciences},
volume = {13},
pages = {687-697},
year = {2012},
note = {18th Biennial ISEM Conference on Ecological Modelling for Global Change and Coupled Human and Natural System},
issn = {1878-0296},
doi = {https://doi.org/10.1016/j.proenv.2012.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S1878029612000631},
author = {Vikas Kumar and J.R. Rouquette and D.N. Lerner},
keywords = {Integrated Modelling, Sustainability Appraisal, Urban River Corridor, Bayesian Network},
abstract = {Sustainability Appraisal (SA) is mandatory under the relevant legislation of UK (DCLG, 2008a) and applies to the preparation of Regional Spatial Strategies, Development Plans and Supplementary Planning documents. SA is a complex task that involves integration of social, environmental and economic considerations into formal plans and often requires trade-offs between multiple stakeholders that may not easily be brought to consensus. Classical assessment can facilitate discussion, but these can only partially inform decision makers as many important aspects of sustainability are abstract and not quantifiable. Such abstract criteria however can be modelled using a Bayesian Network (BN), combining expert opinions, empirical evidence and other information such as model simulation, survey etc. This paper discusses the work of the URSULA project at the University of Sheffield, in which a participative and integrative approach to urban river corridor development, incorporating the principal of sustainability was used. The project used a case study site in Sheffield, UK, and three alternative scenarios were developed, incorporating a number of possible riverside design features. Scenarios were fully designed and visualised using a variety of different media and a sustainability appraisal was undertaken using a broad range of environmental, social and economic indicators. Experts’ assessment logics were captured through mind mapping and further expert elicitation was used to develop an integrated model for SA. The BN approach allows model complexity to be reduced to a level appropriate for an assessment process, whilst still taking complex system interactions implicitly into account. The integrated SA model is being used to develop better design by optimising different design elements in order to deliver an optimum (re)-development plan.}
}
@article{ADAMS2024697,
title = {Human-in-the-loop for Bayesian autonomous materials phase mapping},
journal = {Matter},
volume = {7},
number = {2},
pages = {697-709},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2590238524000067},
author = {Felix Adams and Austin McDannald and Ichiro Takeuchi and A. Gilad Kusne},
keywords = {human-in-the-loop, autonomous, X-ray diffraction, machine learning, phase mapping},
abstract = {Summary
Autonomous experimentation combines machine learning and laboratory automation to select and perform experiments toward user goals. Accordingly, materials optimization using autonomous experimentation requires fewer experiments and less time than Edisonian studies. Integrating knowledge from theory, simulations, literature, and human intuition into the machine learning model can further increase this advantage. We present a set of methods for integrating human input into an autonomous materials exploration campaign for composition-structure phase mapping. The methods are demonstrated on X-ray diffraction data collected from a thin-film ternary combinatorial library. During the campaign, the user can provide input by indicating potential phase boundaries or phase regions with their uncertainty or indicate regions of interest. The input is then integrated through probabilistic priors, resulting in a probabilistic distribution over potential phase maps given the data, model, and human input. We demonstrate an improvement in phase-mapping performance given appropriate human input.}
}
@article{DIFIORE2024107302,
title = {Physics-aware multifidelity Bayesian optimization: A generalized formulation},
journal = {Computers & Structures},
volume = {296},
pages = {107302},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107302},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000312},
author = {Francesco {Di Fiore} and Laura Mainini},
keywords = {Domain-aware efficient optimization, Multifidelity Bayesian optimization, Physics-informed machine learning, Structural health monitoring, Simulation-driven design},
abstract = {The adoption of high-fidelity models for many-query optimization problems is majorly limited by the significant computational cost required for their evaluation at every query. Multifidelity Bayesian methods (MFBO) allow to include costly high-fidelity responses for a sub-selection of queries only, and use fast lower-fidelity models to accelerate the optimization process. State-of-the-art methods rely on a purely data-driven search and do not include explicit information about the physical context. This paper acknowledges that prior knowledge about the physical domains of engineering problems can be leveraged to accelerate these data-driven searches, and proposes a generalized formulation for MFBO to embed a form of domain awareness during the optimization procedure. In particular, we formalize a bias as a multifidelity acquisition function that captures the physical structure of the domain. This permits to partially alleviate the data-driven search from learning the domain properties on-the-fly, and sensitively enhances the management of multiple sources of information. The method allows to efficiently include high-fidelity simulations to guide the optimization search while containing the overall computational expense. Our physics-aware multifidelity Bayesian optimization is presented and illustrated for two classes of optimization problems frequently met in science and engineering, namely design optimization and health monitoring problems.}
}
@article{MAIER2022117540,
title = {Autonomous and data-efficient optimization of turning processes using expert knowledge and transfer learning},
journal = {Journal of Materials Processing Technology},
volume = {303},
pages = {117540},
year = {2022},
issn = {0924-0136},
doi = {https://doi.org/10.1016/j.jmatprotec.2022.117540},
url = {https://www.sciencedirect.com/science/article/pii/S0924013622000528},
author = {Markus Maier and Hannes Kunstmann and Ruben Zwicker and Alisa Rupenyan and Konrad Wegener},
keywords = {Turning, Transfer learning, Expert knowledge, Process optimization, Bayesian optimization, Gaussian process models, Machining},
abstract = {Process parameters in machining are predominantly selected by following manual tuning procedures. Using data from the system and dedicated performance indicators combined with learning-based approaches enables automating these procedures while reducing the costs of the machining process. This study investigates efficient data-driven approaches for autonomous parameter selection in turning. The number of experimental trials for finding optimal process parameters is reduced by incorporating expert knowledge and transferring knowledge between different tasks. The turning process costs are modeled using Gaussian process models, and the selection of informative experiments is achieved by Bayesian optimization. In this study, all tested methods using expert knowledge or transfer of knowledge reduced the number of experiments by at least 40% compared to a standard approach for parameter selection based on Bayesian optimization without expert knowledge, confirming the efficiency of the applied methods.}
}
@article{CHEN2020102703,
title = {Systematizing heterogeneous expert knowledge, scenarios and goals via a goal-reasoning artificial intelligence agent for democratic urban land use planning},
journal = {Cities},
volume = {101},
pages = {102703},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102703},
url = {https://www.sciencedirect.com/science/article/pii/S0264275119312466},
author = {Weizhen Chen and Liang Zhao and Qi Kang and Fan Di},
keywords = {Land use planning, Goal reasoning, Artificial intelligence, Markov decision processes, Reinforcement learning, Multicriteria decision analysis},
abstract = {The tasks of democratic urban land use planning, as subjective-objective combined decision-making efforts that require considerable time and energy, have heretofore been accomplished mainly through deep human thought or by voting. In this paper, we introduce a goal-reasoning artificial intelligence (AI) agent that can assist with these tasks by combining traditional scenario planning, multicriteria decision analysis (MCDA) with a novel goal-oriented Monte Carlo tree search (G-MCTS) method. G-MCTS conducts goal-oriented searches to meet the needs of heterogeneous goals and provide the best land use solutions. We evaluated this method on a real-world planning case, and the results show that 1) the goal-reasoning AI agent is good at performing complex goal reasoning tasks with many heterogeneous expert knowledge; 2) different human planning manuscripts could be integrated into a better solution via a goal-reasoning AI agent; and 3) the goal-reasoning AI agent has the potential to make comprehensive decisions during a democratic political agenda. We conclude that the goal-reasoning AI agent, via an improved reinforcement learning (RL) method of G-MCTS, provides vast potential for assisting in subjective-objective combined urban land use planning and many other similar fields by weighing heterogeneous goals, reproducing human inspiration, and acting as a reflexive sociotechnical system.}
}
@article{BOUMA20114497,
title = {Assessing the value of Earth Observation for managing coral reefs: An example from the Great Barrier Reef},
journal = {Science of The Total Environment},
volume = {409},
number = {21},
pages = {4497-4503},
year = {2011},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2011.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0048969711007418},
author = {Jetske A. Bouma and Onno Kuik and Arnold G. Dekker},
keywords = {Value of information, Earth Observation, Cost-effectiveness analysis, Marine water quality management, Bayesian decision theory, Coral reef protection},
abstract = {The Integrated Global Observing Strategy (IGOS, 2003) argues that further investments in Earth Observation information are required to improve coral reef protection worldwide. The IGOS Strategy does not specify what levels of investments are needed nor does it quantify the benefits associated with better-protected reefs. Evaluating costs and benefits is important for determining optimal investment levels and for convincing policy-makers that investments are required indeed. Few studies have quantitatively assessed the economic benefits of Earth Observation information or evaluated the economic value of information for environmental management. This paper uses an expert elicitation approach based on Bayesian Decision Theory to estimate the possible contribution of global Earth Observation to the management of the Great Barrier Reef. The Great Barrier Reef including its lagoon is a World Heritage Area affected by anthropogenic changes in land-use as well as climate change resulting in increased flows of sediments, nutrients and carbon to the GBR lagoon. Since European settlement, nutrient and sediment loads having increased 5–10 times and the change in water quality is causing damages to the reef. Earth Observation information from ocean and coastal color satellite sensors can provide spatially and temporally dense information on sediment flows. We hypothesize that Earth Observation improves decision-making by enabling better-targeted run-off reduction measures and we assess the benefits (cost savings) of this improved targeting by optimizing run-off reductions under different states of the world. The analysis suggests that the benefits of Earth Observation can indeed be substantial, depending on the perceived accuracy of the information and on the prior beliefs of decision-makers. The results indicate that increasing informational accuracy is the most effective way for developers of Earth Observation information to increase the added value of Earth Observation for managing coral reefs.}
}
@article{LIANG2024110033,
title = {Expert knowledge data-driven based actor–critic reinforcement learning framework to solve computationally expensive unit commitment problems with uncertain wind energy},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {159},
pages = {110033},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2024.110033},
url = {https://www.sciencedirect.com/science/article/pii/S0142061524002540},
author = {Huijun Liang and Chenhao Lin and Aokang Pang},
keywords = {Unit commitment, Reinforcement learning, Knowledge data-driven, Surrogate model, Meta-heuristic algorithm},
abstract = {With the expansion of power grid, unaffordable computational cost and time will pose serious challenges of time-efficient scheduling in unit commitment problem (UCP). However, existing optimization methods, i.e., mathematical programming methods and meta-heuristic algorithms, are powerless and time-consuming to handle computationally expensive UCP (CEUCP). Thus, reinforcement learning methods with strong inference and time-saving performances are motivated to solve the computationally expensive challenges in tackling CEUCPs. In this paper, a novel expert knowledge data-driven based actor–critic (AC) reinforcement learning methodology is proposed for solving CEUCPs. Specifically, in the proposed AC reinforcement learning methodology, expert knowledge, data-driven surrogate model, and improved meta-heuristic algorithm are integrated for further performance enhancement. Firstly, a novel action selection mechanism (based on the expert knowledge of thermal units characteristic) is integrated into AC to improve the efficiency of network training. Secondly, an improved extreme learning machine (ELM) data-driven surrogate model is proposed to build reward function in AC. In detail, original cost function in reward is replaced by a lightweight ELM model. Shape distance is integrated into ELM for enhancing accuracy. Finally, original marine predators algorithm (MPA) is improved for obtaining optimal dispatching decisions and rewards of AC method quickly and correctly. Original search pattern is replaced by quantum based representation for boosting convergence. The excellent performances of the proposed AC framework are verified by simulations of 10-units, 100-units, and 100-units with wind energy test systems.}
}
@article{YU20121005,
title = {Iterative learning belief rule-base inference methodology using evidential reasoning for delayed coking unit},
journal = {Control Engineering Practice},
volume = {20},
number = {10},
pages = {1005-1015},
year = {2012},
note = {4th Symposium on Advanced Control of Industrial Processes (ADCONIP)},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2012.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S096706611200041X},
author = {Xiaodong Yu and Dexian Huang and Yongheng Jiang and Yihui Jin},
keywords = {Belief rule-base, Evidential reasoning, Expert system, Iterative learning, Feedforward compensation, Delayed coking unit},
abstract = {The belief rule-base inference methodology using evidential reasoning (RIMER) approach has been proved to be an effective extension of traditional rule-based expert systems and a powerful tool for representing more complicated causal relationships using different types of information with uncertainties. With a predetermined structure of the initial belief rule-base (BRB), the RIMER approach requires the assignment of some system parameters including rule weights, attribute weights, and belief degrees using experts’ knowledge. Although some updating algorithms were proposed to solve this problem, it is still difficult to find an optimal compact BRB. In this paper, a novel updating algorithm is proposed based on iterative learning strategy for delayed coking unit (DCU), which contains both continuous and discrete characteristics. Daily DCU operations under different conditions are modeled by a BRB, which is then updated using iterative learning methodology, based on a novel statistical utility for every belief rule. Compared with the other learning algorithms, our methodology can lead to a more optimal compact final BRB. With the help of this expert system, a feedforward compensation strategy is introduced to eliminate the disturbance caused by the drum-switching operations. The advantages of this approach are demonstrated on the UniSim™ Operations Suite platform through the developed DCU operation expert system modeled and optimized from a real oil refinery.}
}
@incollection{WALLACH2019275,
title = {Chapter 8 - Parameter Estimation With Bayesian Methods},
editor = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
booktitle = {Working with Dynamic Crop Models (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {275-309},
year = {2019},
isbn = {978-0-12-811756-9},
doi = {https://doi.org/10.1016/B978-0-12-811756-9.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117569000083},
author = {Daniel Wallach and David Makowski and James W. Jones and François Brun},
keywords = {Bayes, MCMC, Parameter estimation, Posterior, Prior, Uncertainty},
abstract = {Bayesian methods are becoming increasingly popular for estimating parameters of complex mathematical models because the Bayesian approach provides a coherent framework for dealing with uncertainty. To start with, the principle is a prior probability distribution of the model parameters describing our belief about the parameter values before we use the set of measurements. The Bayesian methods then tell us how to update this belief using the measurements to give the posterior parameter density. In the Bayesian approach, the parameters are defined as random variables and the prior and posterior parameter distributions represent our belief about parameter values before and after using observed data to improve estimates. This approach has several advantages: (i) parameters can be estimated from different types of information (data, literature, expert knowledge); (ii) the posterior probability distribution can be used to implement uncertainty analysis methods; and (iii) the posterior probability distribution can be used for optimizing decisions in the face of uncertainty. This chapter presents the basic principles of the Bayesian approach and describes several algorithms to calculate posterior parameter distributions. These algorithms are illustrated in several applications on yield and soil carbon estimation.}
}
@article{ZHANG2020103493,
title = {TBM performance prediction with Bayesian optimization and automated machine learning},
journal = {Tunnelling and Underground Space Technology},
volume = {103},
pages = {103493},
year = {2020},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103493},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820304478},
author = {Qianli Zhang and Weifei Hu and Zhenyu Liu and Jianrong Tan},
keywords = {Performance prediction, Bayesian optimization, Machine learning},
abstract = {Accurately predicting the performance of a tunnel boring machine (TBM) is important to safe and efficient tunneling. The application of machine learning algorithms to TBM performance prediction creates several challenges. Such prediction is a nontrivial task involving procedures such as data preprocessing, selection of a machine learning algorithm and optimization of the related hyperparameters. The demand for expert knowledge has restricted the application of machine learning methods to TBM performance prediction, and it is meaningful to study predicting TBM performance automatically. In this paper, we explore three approaches to TBM performance prediction using Bayesian optimization and automated machine learning (AutoML). In the first study, Bayesian optimization is used to determine the optimal hyperparameters of various machine learning algorithms, including support vector regression (SVR), decision tree, bagging tree, random forest and AdaBoost. We attain the minimum mean squared error (MSE) values of 3.135×10-2 and 3.177×10-2 for a decision tree and SVR, respectively. In the second approach called the neural architecture search (NAS), the optimal combination of architecture, hyperparameters and the training procedure of an artificial neural network is found in a single operation. We obtain the optimal results of 3.514×10-2 and 3.237×10-2 if complete and simplified NAS are used, respectively. In the third method, the best combination of a data preprocessing method, a machine learning model and the related hyperparameters is found, and an optimal MSE value of 3.148×10-2 is obtained using AutoML. In all three studies, we obtain state-of-the-art prediction results that are superior to a previous best prediction result of 3.500×10-2. The prediction results prove that Bayesian optimization and AutoML are powerful tools that can not only effectively predict TBM performance but also reduce the demand for expert knowledge of machine learning.}
}
@article{XUE2021897,
title = {A novel fuzzy Bayesian network-based MADM model for offshore wind turbine selection in busy waterways: An application to a case in China},
journal = {Renewable Energy},
volume = {172},
pages = {897-917},
year = {2021},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.03.084},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121004407},
author = {Jie Xue and Tsz Leung Yip and Bing Wu and Chaozhong Wu and P.H.A.J.M. {van Gelder}},
keywords = {Wind energy, Offshore wind turbine (OWT), Multiple-attribute decision-making (MADM), Principal component analysis (PCA), Fuzzy bayesian network, Marine traffic safety},
abstract = {Offshore wind power is an important renewable energy source and plays an essential role in optimizing the energy structure worldwide. Simultaneously, offshore wind turbine (OWT) selection is a complicated process since it concerning various variables and optimization scenarios. In this paper, a novel fuzzy Bayesian network-based model for multiple-attribute decision-making (MADM) is proposed. First of all, a three-layer decision-making framework for OWT selection is established through systematically combing previous studies, expert knowledge, and the principal component analysis (PCA) results by treating the wind turbine parameters, wind turbine economy, wind turbine reliability, and navigation safety as the attributes, and the corresponding 11 influencing factors are identified and quantified. Moreover, a triangular fuzzy number is introduced to fuzzify each influencing factor, and the belief degree for different linguistic variables corresponding to the specific influencing factor is employed in the fuzzy IF-THEN rule system. Then, the belief rule base is transformed into the Bayesian network as the conditional probability tables (CPTs), which can directly express the influence relationship of various factors and realize the integration of various influence factors to obtain the optimal scheme. Finally, the proposed model is validated by taking a case study in busy waterways in the Eastern China Sea as an example. This research provides an intuitive, feasible, and practical way for OWT selection.}
}
@article{OUELLET2021105138,
title = {Improve performance and robustness of knowledge-based FUZZY LOGIC habitat models},
journal = {Environmental Modelling & Software},
volume = {144},
pages = {105138},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105138},
url = {https://www.sciencedirect.com/science/article/pii/S136481522100181X},
author = {Valérie Ouellet and Julien Mocq and Salah-Eddine {El Adlouni} and Stefan Krause},
keywords = {Fuzzy logic, Critic, Expert knowledge, Model optimization, Decision framework},
abstract = {Previous criticisms of knowledge-based fuzzy logic modelling have identified some of its limitations and revealed weaknesses regarding the development of fuzzy sets, the integration of expert knowledge, and the outcomes of different defuzzification processes. We show here how expert disagreement and fuzzy logic mechanisms associated with the rule development and combinations can positively or adversely affect model performance and the interpretation of results. We highlight how expert disagreement can induce uncertainty into model outputs when defining fuzzy sets and selecting a defuzzification method. We present a framework to account for sources of error and bias and improve the performance and robustness of knowledge-based fuzzy logic models. We recommend to 1) provide clear/unambiguous instructions on model development, processes and objectives, including the definition of input variables and fuzzy sets, 2) incorporate the disagreement among experts into the analysis, 3) increase the use of short rules and the OR operator to reduce complexity, and 4) improve model performance and robustness by using narrow fuzzy sets for extreme values of input variables to expand the universe of discourse adequately. Our framework is focused on fuzzy logic models but can be applied to all knowledge-based models that require expert judgment, including expert systems, decision trees and (fuzzy) Bayesian inference systems.}
}
@article{SOUSA2025102892,
title = {Human-in-the-loop Multi-objective Bayesian Optimization for Directed Energy Deposition with in-situ monitoring},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {92},
pages = {102892},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102892},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524001790},
author = {João Sousa and Armando Sousa and Frank Brueckner and Luís Paulo Reis and Ana Reis},
keywords = {Additive manufacturing, Digital twin, Robot operating system, Surrogate modeling, Process optimization},
abstract = {Directed Energy Deposition (DED) is a free-form metal additive manufacturing process characterized as toolless, flexible, and energy-efficient compared to traditional processes. However, it is a complex system with a highly dynamic nature that presents challenges for modeling and optimization due to its multiphysics and multiscale characteristics. Additionally, multiple factors such as different machine setups and materials require extensive testing through single-track depositions, which can be time and resource-intensive. Single-track experiments are the foundation for establishing optimal initial parameters and comprehensively characterizing bead geometry, ensuring the accuracy and efficiency of computer-aided design and process quality validation. We digitized a DED setup using the Robot Operating System (ROS 2) and employed a thermal camera for real-time monitoring and evaluation to streamline the experimentation process. With the laser power and velocity as inputs, we optimized the dimensions and stability of the melt pool and evaluated different objective functions and approaches using a Response Surface Model (RSM). The three-objective approach achieved better rewards in all iterations and, when implemented in a real setup, allowed to reduce the number of experiments and shorten setup time. Our approach can minimize waste, increase the quality and reliability of DED, and enhance and simplify human-process interaction by leveraging the collaboration between human knowledge and model predictions.}
}
@article{WANG2024102811,
title = {Clinical knowledge-guided deep reinforcement learning for sepsis antibiotic dosing recommendations},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102811},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102811},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000538},
author = {Yuan Wang and Anqi Liu and Jucheng Yang and Lin Wang and Ning Xiong and Yisong Cheng and Qin Wu},
keywords = {Deep reinforcement learning, Sepsis, Antibiotic, Clinical},
abstract = {Sepsis is the third leading cause of death worldwide. Antibiotics are an important component in the treatment of sepsis. The use of antibiotics is currently facing the challenge of increasing antibiotic resistance (Evans et al., 2021). Sepsis medication prediction can be modeled as a Markov decision process, but existing methods fail to integrate with medical knowledge, making the decision process potentially deviate from medical common sense and leading to underperformance. (Wang et al., 2021). In this paper, we use Deep Q-Network (DQN) to construct a Sepsis Anti-infection DQN (SAI-DQN) model to address the challenge of determining the optimal combination and duration of antibiotics in sepsis treatment. By setting sepsis clinical knowledge as reward functions to guide DQN complying with medical guidelines, we formed personalized treatment recommendations for antibiotic combinations. The results showed that our model had a higher average value for decision-making than clinical decisions. For the test set of patients, our model predicts that 79.07% of patients will achieve a favorable prognosis with the recommended combination of antibiotics. By statistically analyzing decision trajectories and drug action selection, our model was able to provide reasonable medication recommendations that comply with clinical practices. Our model was able to improve patient outcomes by recommending appropriate antibiotic combinations in line with certain clinical knowledge.}
}
@article{GUAYHOTTIN2025113039,
title = {Robust prior-biased acquisition function for human-in-the-loop Bayesian optimization},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113039},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113039},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000863},
author = {Rose Guay-Hottin and Lison Kardassevitch and Hugo Pham and Guillaume Lajoie and Marco Bonizzato},
keywords = {Bayesian optimization, Domain knowledge integration, Prior-weighted acquisition function, Region of interest, Human-in-the-loop},
abstract = {In diverse fields of application, Bayesian Optimization (BO) has been proposed to find the optimum of black-box functions, surpassing human-driven searches. BO’s appeal lies in its data efficiency, making it suitable for optimizing costly-to-evaluate functions without requiring extensive training data. While BO can perform well in closed-loop, domain experts frequently have hypotheses about which parameter combinations are more likely to yield optimal results. Hence, for BO to be truly relevant and adopted by practitioners, such prior knowledge needs to be efficiently and seamlessly integrated into the optimization framework. Some methods were recently developed to address this challenge, but they suffer from robustness issues when provided erroneous insight. Building on the idea of element-wise prior-weighted acquisition function, we propose to use a fixed-weight effective prior that distills expert user knowledge with minimal computational cost. Comprehensive investigation across diverse task conditions and prior quality levels revealed that our method, α-πBO, surpasses Vanilla BO when provided with insights of good quality while maintaining robustness against misleading information. Moreover, unlike other methods, α-πBO typically requires no hyperparameter tuning, largely simplifying its implementation in diverse tasks.}
}
@article{AMMANN20252428,
title = {Automated Knowledge Graph Learning in Industrial Processes},
journal = {Procedia Computer Science},
volume = {253},
pages = {2428-2437},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003114},
author = {Lolita Ammann and Jorge Martinez-Gil and Michael Mayr and Georgios C. Chasparis},
keywords = {Knowledge graphs, Graph learning, Knowledge engineering, Granger causality},
abstract = {Industrial processes generate vast amounts of time series data, yet extracting meaningful relationships and insights remains challenging. Knowledge graphs allow to uniquely store and present information, enabling novel capabilities for identifying clusters, temporal relationships, and hidden connections. Building a knowledge graph either requires considerable manual effort and deep expert knowledge of the process or computationally intensive machine learning that provides little transparency in how process structures are identified. This paper introduces a framework for automated knowledge graph learning from time series data, specifically tailored for industrial applications. Moving beyond traditional black-box models process knowledge retrieved from various transparent analytical approaches are amalgamated into a knowledge graph. Similarity analysis is employed to explore simultaneous process relationships, while Granger-causality analysis provides insights into temporally disjoint parameter interactions and causal dependencies between parameters. To illustrate the practical utility of our approach, we present a motivating use case demonstrating the benefits of our framework in a real-world industrial scenario. The user is provided with an intuitive visualization of the process and information that can be enriched with expert knowledge improving decision-making, process optimization, and knowledge discovery.}
}
@article{DEVOL2025118788,
title = {Methodology for the automated selection of time-frequency representations},
journal = {Journal of Sound and Vibration},
volume = {596},
pages = {118788},
year = {2025},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2024.118788},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X24005509},
author = {Nathaniel DeVol and Christopher Saldaña and Katherine Fu},
keywords = {Time-frequency analysis, Bayesian optimization, Machine learning},
abstract = {Data preprocessing is a key step in extracting useful information from sound and vibration data and often involves selecting a time-frequency representation. No single time-frequency representation is always optimal, and no standard method exists for selecting the appropriate time-frequency representation, so selecting the time-frequency representation requires expert knowledge and is susceptible to human bias. To address this, this work introduces a methodology to automate the selection of a time-frequency representation for a dataset using only a subset of the healthy, or normal, class of data. To select the parameters for each type of time-frequency representation, Bayesian optimization is used. With a candidate from each type of time-frequency representation, the average similarity is used to select the final candidate. Additionally, the use of multiple time-frequency representations within a single model is explored. Because there is currently no objective method to compare the selected time frequency representations against, the proposed methodology is evaluated in two case studies. In the case studies, the time frequency representations are used as inputs to a simple convolutional neural network that achieved 100% accuracy in classifying bearing faults and 94% accuracy in classifying the contact tip to workpiece distance in wire arc additive manufacturing. Additionally, the proposed methodology presents a 75% and 94% reduction in the data size for the two case studies. This offers further benefits for reducing costs of data transmission and storage in modern digital manufacturing architectures.}
}
@article{LASH2024114304,
title = {HEX: Human-in-the-loop explainability via deep reinforcement learning},
journal = {Decision Support Systems},
volume = {187},
pages = {114304},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114304},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624001374},
author = {Michael T. Lash},
keywords = {Explainability, Interpretability, Human-in-the-loop, Deep reinforcement learning, Machine learning, Behavioral machine learning, Decision support},
abstract = {The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider’s preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider’s trust and tendency to rely on trusted features.}
}
@article{DIAZSECADES2023114747,
title = {Waste heat recovery system for marine engines optimized through a preference learning rank function embedded into a Bayesian optimizer},
journal = {Ocean Engineering},
volume = {281},
pages = {114747},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114747},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011319},
author = {Luis Alfonso Díaz-Secades and R. González and N. Rivera and Elena Montañés and José Ramón Quevedo},
keywords = {Waste heat recovery, Marine diesel engine, Desalination, Steam rankine cycle, Organic rankine cycle, Thermoelectric generators, Preference learning, Bayesian Optimization},
abstract = {Waste heat recovery is a proven process to improve efficiency on engines and meets current necessities of the maritime industry. Since January 1, 2023, already built vessels must meet the energy efficiency indicators known as EEXI and CII. Aiming to reduce fuel consumption and mitigate pollution emissions, a novel waste heat recovery system composed of steam Rankine cycle, organic Rankine cycle, thermoelectric harvesters and desalination is presented. High, medium and low-grade waste heat from exhaust gas, jacket water, lubricating oil and engine block radiation are targeted for recovery. Performance assessment of each subsystem when implemented on a real case study 6-cylinder medium speed marine engine is analyzed. The equivalent electricity production concept was used for the assessment of the desalination subsystem. The proposed system effectively recovers waste energy, offering economic benefits, reducing pollution and satisfying the daily demand of fresh water. Also, optimal states of the waste heat recovery are provided via Bayesian optimization, which requires an evaluation function for the system to be optimized. However, this function is not available and cannot be straightforwardly established, since the quality of waste heat recovery depends on some indicators with a trade-off among them. Hence, a preference learning procedure that exploits expert knowledge is proposed to induce a function of this kind from those indicators in order to be embedded into the Bayesian optimization procedure afterward. Applied to the case study engine, a fuel consumption reduction of 15.04% is achieved. Fuel savings lead to an improvement in energy efficiency indicators, achieving a reduction of 6.98% on the EEXI and a 13.85% on the CII.}
}
@article{PARK20124240,
title = {Evolutionary attribute ordering in Bayesian networks for predicting the metabolic syndrome},
journal = {Expert Systems with Applications},
volume = {39},
number = {4},
pages = {4240-4249},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411014333},
author = {Han-Saem Park and Sung-Bae Cho},
keywords = {Bayesian network, Prognostic modeling, Attribute ordering optimization, Metabolic syndrome},
abstract = {The metabolic syndrome is a set of risk factors that include abdominal obesity, insulin resistance, dyslipidemia and hypertension. It has affected around 25% of adults in the US and become a serious problem in Asian countries recently due to the change in dietary habit and life style. On the other hand, Bayesian networks that are the models to solve the problems of uncertainty provide a robust and transparent formalism for probabilistic modeling, so they have been used as a method for diagnostic or prognostic model in medical domain. Since the K2 algorithm, a well-known algorithm for Bayesian networks structure learning, is influenced by an input order of the attributes, an optimization of BN attribute ordering has been studied as a research issue. This paper proposes a novel ordering optimization method using a genetic algorithm based on medical expert knowledge in order to solve this problem. For experiments, we use the dataset examined twice in 1993 and 1995 in Yonchon County of Korea. It has 18 attributes of 1193 subjects participated in both surveys. Using this dataset, we make the prognostic model of the metabolic syndrome using Bayesian networks with an optimized ordering by evolutionary approach. Through an ordering optimization, the prognostic model of higher performance is constructed, and the optimized Bayesian network model by the proposed method outperforms the conventional BN model as well as neural networks and k-nearest neighbors. Finally, we present the application program using the prognostic model of the metabolic syndrome in order to show the usefulness of the proposed method.}
}
@article{LI2021107850,
title = {A knowledge-guided and data-driven method for building HVAC systems fault diagnosis},
journal = {Building and Environment},
volume = {198},
pages = {107850},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321002560},
author = {Tingting Li and Yang Zhao and Chaobo Zhang and Jing Luo and Xuejun Zhang},
keywords = {Diagnostic bayesian networks, Genetic algorithm, Fault diagnosis, Fault interpretation, HVAC systems, Energy conservation},
abstract = {Fault diagnosis is crucial for energy conversation of building HVAC systems. Generally, knowledge-driven fault diagnosis methods have good interpretability, whereas data-driven fault diagnosis methods have high diagnosis accuracy. With the aim of integrating the advantages of both types of methods, this paper proposes a knowledge-guided and data-driven fault diagnosis method. The proposed method develops a diagnostic Bayesian network (DBN) based on both expert knowledge and operational data. A probabilistic framework is developed for determining the prior DBN structures based on expert knowledge. An improved genetic algorithm-based approach is raised for further optimizing the DBN structures based on the operational data. Local casual graphs are generated from the DBN for visually interpreting the fault action mechanisms. Experts can evaluate the reliability of the diagnosis results using the local casual graphs, and then make reliable decisions. The proposed method is evaluated using the experimental data from the ASHARE Project 1312-RP. The results show that the performance of the proposed method is promising. Six typical faults are interpreted by the local casual graphs. It is demonstrated that the local casual graphs can effectively reveal the action mechanisms behind the six faults.}
}
@article{CHEN2025107318,
title = {HCPI-HRL: Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning},
journal = {Neural Networks},
volume = {187},
pages = {107318},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107318},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025001972},
author = {Bin Chen and Zehong Cao and Wolfgang Mayer and Markus Stumptner and Ryszard Kowalczyk},
keywords = {Deep reinforcement learning, Hierarchical reinforcement learning, Causal inference, Subgoal discovery},
abstract = {The dependency on extensive expert knowledge for defining subgoals in hierarchical reinforcement learning (HRL) restricts the training efficiency and adaptability of HRL agents in complex, dynamic environments. Inspired by human-guided causal discovery skills, we proposed a novel method, Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning (HCPI-HRL), designed to infer diverse, effective subgoal structures as intrinsic rewards and incorporate critical objects from dynamic environmental states using stable causal relationships. The HCPI-HRL method is supposed to guide an agent’s exploration direction and promote the reuse of learned subgoal structures across different tasks. Our designed HCPI-HRL comprises two levels: the top level operates as a meta controller, assigning subgoals discovered based on human-driven causal critical object perception and causal structure inference; the bottom level employs the Proximal Policy Optimisation (PPO) algorithm to accomplish the assigned subgoals. Experiments conducted across discrete and continuous control environments demonstrated that HCPI-HRL outperforms benchmark methods such as hierarchical and adjacency PPO in terms of training efficiency, exploration capability, and transferability. Our research extends the potential of HRL methods incorporating human-guided causal modelling to infer the effective relationships across subgoals, enhancing the agent’s capability to learn efficient policies in dynamic environments with sparse reward signals.}
}
@article{BLONDET2019289,
title = {A knowledge-based system for numerical design of experiments processes in mechanical engineering},
journal = {Expert Systems with Applications},
volume = {122},
pages = {289-302},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300120},
author = {Gaëtan Blondet and Julien Le Duigou and Nassim Boudaoud},
keywords = {Knowledge based system, Numerical design of experiments, Bayesian network},
abstract = {This paper describes a specific knowledge-based system (KBS) to assist designers in configuring numerical design of experiments (NDoE) processes efficiently. NDoE processes are applied in product design to improve the quality of product, by taking into account variabilities and uncertainties. NDoE processes are defined by various and complex methodologies to achieve several objectives, as optimization, surrogate modeling or sensitivity analysis. On the other hand, NDoE processes may demand huge computing resources to execute hundreds simulations, and also advanced expert knowledge to set the best configuration amongst numerous possibilities. Designers aim to obtain most useful results with a minimal computational cost as soon as possible. Thus, the configuration step must be as fast as possible, and it must lead to an efficient combination of complex methods, algorithms and hyper-parameters, to obtain valuable information on the product. The proposed KBS and its inference engine, a bayesian network, is detailed and applied to a product developed by automotive industry. The KBS propose new efficient configurations to achieve designers' goal. This application shorten the configuration step of the NDoE process, and enables designers to use more complex methods. It also allows designers to capitalize knowledge and learn from each past NDoE process.}
}
@article{JAFRASTEH2021104674,
title = {Objective functions from Bayesian optimization to locate additional drillholes},
journal = {Computers & Geosciences},
volume = {147},
pages = {104674},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104674},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420306464},
author = {Bahram Jafrasteh and Alberto Suárez},
keywords = {Bayesian optimization, Cross-validation, Objective function, Rank comparison, Gaussian process, Infill drillhole},
abstract = {The key available information to choose new locations for drilling are the estimated ore grade values and the corresponding uncertainties at the tentative locations. These pieces of information are combined to generate a single objective function. The mathematical form of the objective function should reflect the effect of these values and their relative importance. Traditional objective function use multiplication of these parameters by different powering values. In this study, we develop two novel objective functions from the Bayesian optimization: the probability of improvement (PI), and the expected improvement (EI). These two objective functions seek new drillholes while considering the effect of the used value and their relative importance. Therefore, they can provide a trade-off between exploration and exploitation. All the objective functions have adjustable parameters. These parameters are typically tuned using expert knowledge or heuristic rules. Here, a statistical method based on cross-validation is proposed to adjust the parameters of the traditional and novel objective functions. The performance of the novel objective functions is validated against other ones using a distance based ranking method, in a phosphate deposit. The obtained results demonstrate the robustness of the EI and PI, the newly introduced objective functions from the Bayesian optimization framework.}
}
@article{LIU2025110279,
title = {Context-aware inverse reinforcement learning for modeling individuals’ daily activity schedules},
journal = {Engineering Applications of Artificial Intelligence},
volume = {146},
pages = {110279},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110279},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625002799},
author = {Dongjie Liu and Dawei Li and Kun Gao and Yuchen Song and Zijie Zhou},
keywords = {Travel demand modeling, Activity generation, Artificial Intelligence, Activity-based models, Inverse reinforcement learning},
abstract = {Understanding individual and crowd dynamics in urban environments is critical for numerous applications, such as urban planning, traffic forecasting, and location-based services. Therefore, accurately modeling individuals' daily activity schedules is essential. Traditional methods, like utility-based and rule-based approaches, rely on expert knowledge and presumed model structures. While machine learning methods offer flexibility, they often ignore explicit behavioral mechanisms, particularly comprehensive discussion and integration of context related to individuals' daily travel. To address these, we propose a framework that integrates travel context with deep Inverse Reinforcement Learning (IRL), learning temporal patterns from sociodemographics, start time and duration of the current activity, travel modes, and land use. Specifically, individuals' activity schedules are initially formulated as a Markov Decision Process to simulate travelers’ sequential decision-making processes, laying the groundwork for the IRL framework; Then, a context-aware IRL method is proposed to model individuals' travel decision-making from observed temporal trajectories. Finally, we validate the proposed model by demonstrating its superior performance over discrete choice model and several well-known imitation learning benchmarks in tasks such as policy performance comparison, reward recovery, model generalizability, and computational efficiency using travel behavior datasets. This approach provides meaningful behavioral insights and paves the way for Artificial Intelligence-driven activity schedulers modeling.}
}
@article{LEE2012469,
title = {Multi-agent knowledge integration mechanism using particle swarm optimization},
journal = {Technological Forecasting and Social Change},
volume = {79},
number = {3},
pages = {469-484},
year = {2012},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2011.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0040162511001715},
author = {Kun Chang Lee and Namho Lee and Habin Lee},
keywords = {Agent-based model (ABM), Particle swarm optimization (PSO), Fuzzy cognitive map (FCM), Expert knowledge, Knowledge integration, IT project risk assessment},
abstract = {Unstructured group decision-making is burdened with several central difficulties: unifying the knowledge of multiple experts in an unbiased manner and computational inefficiencies. In addition, a proper means of storing such unified knowledge for later use has not yet been established. Storage difficulties stem from of the integration of the logic underlying multiple experts' decision-making processes and the structured quantification of the impact of each opinion on the final product. To address these difficulties, this paper proposes a novel approach called the multiple agent-based knowledge integration mechanism (MAKIM), in which a fuzzy cognitive map (FCM) is used as a knowledge representation and storage vehicle. In this approach, we use particle swarm optimization (PSO) to adjust causal relationships and causality coefficients from the perspective of global optimization. Once an optimized FCM is constructed an agent based model (ABM) is applied to the inference of the FCM to solve real world problem. The final aggregate knowledge is stored in FCM form and is used to produce proper inference results for other target problems. To test the validity of our approach, we applied MAKIM to a real-world group decision-making problem, an IT project risk assessment, and found MAKIM to be statistically robust.}
}
@article{FORIO2020101124,
title = {Bayesian Belief Network models as trade-off tools of ecosystem services in the Guayas River Basin in Ecuador},
journal = {Ecosystem Services},
volume = {44},
pages = {101124},
year = {2020},
issn = {2212-0416},
doi = {https://doi.org/10.1016/j.ecoser.2020.101124},
url = {https://www.sciencedirect.com/science/article/pii/S2212041620300668},
author = {Marie Anne Eurie Forio and Gonzalo Villa-Cox and Wout {Van Echelpoel} and Helena Ryckebusch and Koen Lock and Pieter Spanoghe and Arne Deknock and Niels {De Troyer} and Indira Nolivos-Alvarez and Luis Dominguez-Granda and Stijn Speelman and Peter L.M. Goethals},
keywords = {Integrated water management, Agricultural intensification, Land use cover, Trade-off tool, Land use cover optimization, Agricultural crops},
abstract = {Food production often leads to environmental degradation. Consequently, insights into ecosystem functioning in relation to exploitation are needed as a basis for socioeconomically acceptable mitigation of these impacts. A Bayesian Belief Network (BBN) model is developed to link three major ecosystem services (ES), i.e. food production, water provision and ecotourism, and determine the effect of local agricultural practices and management on the ES in the Guayas Basin (Ecuador). Several data sources were integrated into the BBN model, including processed spatial data from primary and secondary sources, sampling and survey data, and expert knowledge. The model suggests that banana and sugar cane generate the highest yield but provide low ecotourism benefits. In contrast, cacao produces the lowest yields but contributes to better water quality. Scenario analyses suggest that environmental gains are possible by optimising the land use (LU) based on the edaphoclimatic requirements of crops. Moreover, the integration of LU optimisation with upscaling and farming intensification can allow for additional advantages in water provision and ecotourism while mitigating productivity losses. The BBN development approach can serve as a reference for other case studies, where data scarcity plays a limiting factor in the assessment of interactions between key ES.}
}
@article{RAMANATHAN2018422,
title = {Smart controller for conical tank system using reinforcement learning algorithm},
journal = {Measurement},
volume = {116},
pages = {422-428},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117307091},
author = {Prabhu Ramanathan and Kamal Kant Mangla and Sitanshu Satpathy},
keywords = {Non-linear process, Machine learning, Reinforcement learning, Data acquisition, Conical tank control},
abstract = {The objective of the paper is to study the implementation of machine learning based controller to control non-linear systems. A smart controller based on reinforcement learning algorithm is proposed, and its performance is demonstrated by using it to control the level of liquid in a non-linear conical tank system. The system is represented in terms of a Markov Decision Process (MDP), and a reinforcement learning technique based on Q-learning algorithm is used to control the process. The advantage is that a standalone controller is designed on its own without prior knowledge of the environment or the system. The hardware implementation of the designed controller showed that the controller controlled the level of fluid in the conical tank efficiently, and rejected random disturbances introduced in the system. This controller provides an edge over PID, fuzzy, and other neural network based controllers, by eliminating the need for linearizing non-linear characteristics, tuning PID parameters, designing transfer functions, and developing fuzzy membership functions.}
}
@article{HOLLOS2022105556,
title = {Conditional interval reduction method: A possible new direction for the optimization of process based models},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105556},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105556},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002560},
author = {R. Hollós and N. Fodor and K. Merganičová and D. Hidy and T. Árendás and T. Grünwald and Z. Barcza},
keywords = {Model optimization, Bayesian calibration, Parameter constraints, Decision tree},
abstract = {Application of process-based models at different spatial scales requires their proper parameterization. This task is typically executed using trial-and-error parameter adjustment or a probabilistic method. Practical application of the probabilistic methods is hampered by methodological complexity and lack of interpretability. Here we present a novel approach for the parameterization of process-based models that we call as conditional interval refinement method (CIRM). The method can be best described as the combination of a probabilistic approach and the advantages of the expert-based parameter adjustment. CIRM was demonstrated by optimizing the Biome-BGCMuSo biogeochemical model using maize yield observations. The proposed approach uses the General Likelihood Uncertainty Estimation (GLUE) method with additional expert knowledge, supplemented by the construction and interpretation of decision trees. It was demonstrated that the iterative, fully automatic method successfully constrained the parameter intervals meanwhile our confidence on the parameters increased. The algorithm can easily be implemented with other process-based models.}
}
@article{LANGEMEYER2020135487,
title = {Creating urban green infrastructure where it is needed – A spatial ecosystem service-based decision analysis of green roofs in Barcelona},
journal = {Science of The Total Environment},
volume = {707},
pages = {135487},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.135487},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719354804},
author = {Johannes Langemeyer and Diego Wedgwood and Timon McPhearson and Francesc Baró and Anders L. Madsen and David N. Barton},
keywords = {Cities, Urban, Nature-based solutions (NBS), Green infrastructure (GI), Bayesian Belief Networks (BBN), Multi-criteria decision analysis (MCDA)},
abstract = {As cities face increasing pressure from densification trends, green roofs represent a valuable source of ecosystem services for residents of compact metropolises where available green space is scarce. However, to date little research has been conducted regarding the holistic benefits of green roofs at a citywide scale, with local policymakers lacking practical guidance to inform expansion of green roofs coverage. The study addresses this issue by developing a spatial multi-criteria screening tool applied in Barcelona, Spain to determine: 1) where green roofs should be prioritized in Barcelona based on expert elicited demand for a wide range of ecosystem services and 2) what type of design of potential green roofs would optimize the ecosystem service provision. As inputs to the model, fifteen spatial indicators were selected as proxies for ecosystem service deficits and demands (thermal regulation, runoff control, habitat and pollination, food production, recreation, and social cohesion) along with five decision alternatives for green roof design (extensive, semi-intensive, intensive, naturalized, and allotment). These indicators and alternatives were analyzed probabilistically and spatially, then weighted according to feedback from local experts. Results of the assessment indicate that there is high demand across Barcelona for the ecosystem services that green roofs potentially might provide, particularly in dense residential neighborhoods and the industrial south. Experts identified habitat, pollination and thermal regulation as the most needed ES with runoff control and food production as the least demanded. Naturalized roofs generated the highest potential ecosystem service provision levels for 87.5% of rooftop area, apart from smaller areas of central Barcelona where intensive rooftops were identified as the preferable green roof design. Overall, the spatial model developed in this study offers a flexible screening based on spatial multi-criteria decision analysis that can be easily adjusted to guide municipal policy in other cities considering the effectiveness of green infrastructure as source of ecosystem services.}
}
@article{XU2023112055,
title = {Bayesian Optimization of photonic curing process for flexible perovskite photovoltaic devices},
journal = {Solar Energy Materials and Solar Cells},
volume = {249},
pages = {112055},
year = {2023},
issn = {0927-0248},
doi = {https://doi.org/10.1016/j.solmat.2022.112055},
url = {https://www.sciencedirect.com/science/article/pii/S092702482200472X},
author = {Weijie Xu and Zhe Liu and Robert T. Piper and Julia W.P. Hsu},
keywords = {Perovskite solar cell, Bayesian optimization, Photonic curing, SHarply additive explanation, Machine learning},
abstract = {Photonic curing is a thin-film processing technique that can enable high-throughput perovskite solar cell (PSC) manufacturing. However, photonic curing has many variables that can affect the processing outcome, making optimization challenging. Here, we introduce Bayesian Optimization (BO), a machine-learning framework, to optimize the power conversion efficiency (PCE) of photonically cured MAPbI3 PSCs on ITO-coated Willow Glass. We apply BO with four input variables—MAPbI3 concentration, additive CH2I2 volume, pulse voltage, and pulse length. These input variables were dynamically adjusted in response to the new data, an example of a human-machine partnership. With the limited experimental budget of 48 conditions, we achieved a champion PCE of 11.42% and predicted 14 new conditions resulting in >10% PCE. Beyond simple optimization, we examined the relationships between pairs of inputs with two-dimensional contour plots and investigated the relative importance of each input to gain insight into photonic curing. We demonstrate that BO is a powerful tool in process optimization and can be adapted to other PSC manufacturing cases.}
}
@article{WEICHERT2023514,
title = {Explainable production planning under partial observability in high-precision manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {514-524},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001590},
author = {Dorina Weichert and Alexander Kister and Peter Volbach and Sebastian Houben and Marcus Trost and Stefan Wrobel},
keywords = {Explainability, Manufacturing, Reinforcement Learning, Monte Carlo Tree Search, Partially Observable Markov Decision Process},
abstract = {Conceptually, high-precision manufacturing is a sequence of production and measurement steps, where both kinds of steps require to use non-deterministic models to represent production and measurement tolerances. This paper demonstrates how to effectively represent these manufacturing processes as Partially Observable Markov Decision Processes (POMDP) and derive an offline strategy with state-of-the-art Monte Carlo Tree Search (MCTS) approaches. In doing so, we face two challenges: a continuous observation space and explainability requirements from the side of the process engineers. As a result, we find that a tradeoff between the quantitative performance of the solution and its explainability is required. In a nutshell, the paper elucidates the entire process of explainable production planning: We design and validate a white-box simulation from expert knowledge, examine state-of-the-art POMDP solvers, and discuss our results from both the perspective of machine learning research and as an illustration for high-precision manufacturing practitioners.}
}
@article{DUNLAP20238061,
title = {Continuous flow synthesis of pyridinium salts accelerated by multi-objective Bayesian optimization with active learning††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3sc01303k},
journal = {Chemical Science},
volume = {14},
number = {30},
pages = {8061-8069},
year = {2023},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc01303k},
url = {https://www.sciencedirect.com/science/article/pii/S204165202300010X},
author = {John H. Dunlap and Jeffrey G. Ethier and Amelia A. Putnam-Neeb and Sanjay Iyer and Shao-Xiong Lennon Luo and Haosheng Feng and Jose Antonio {Garrido Torres} and Abigail G. Doyle and Timothy M. Swager and Richard A. Vaia and Peter Mirau and Christopher A. Crouse and Luke A. Baldwin},
abstract = {ABSTRACT
We report a human-in-the-loop implementation of the multi-objective experimental design via a Bayesian optimization platform (EDBO+) towards the optimization of butylpyridinium bromide synthesis under continuous flow conditions. The algorithm simultaneously optimized reaction yield and production rate (or space-time yield) and generated a well defined Pareto front. The versatility of EDBO+ was demonstrated by expanding the reaction space mid-campaign by increasing the upper temperature limit. Incorporation of continuous flow techniques enabled improved control over reaction parameters compared to common batch chemistry processes, while providing a route towards future automated syntheses and improved scalability. To that end, we applied the open-source Python module, nmrglue, for semi-automated nuclear magnetic resonance (NMR) spectroscopy analysis, and compared the acquired outputs against those obtained through manual processing methods from spectra collected on both low-field (60 MHz) and high-field (400 MHz) NMR spectrometers. The EDBO+ based model was retrained with these four different datasets and the resulting Pareto front predictions provided insight into the effect of data analysis on model predictions. Finally, quaternization of poly(4-vinylpyridine) with bromobutane illustrated the extension of continuous flow chemistry to synthesize functional materials.}
}
@article{GUO201620774,
title = {A new fault diagnosis method based on Bayesian network model in a wastewater treatment plant of northern China},
journal = {Desalination and Water Treatment},
volume = {57},
number = {44},
pages = {20774-20783},
year = {2016},
issn = {1944-3986},
doi = {https://doi.org/10.1080/19443994.2015.1110047},
url = {https://www.sciencedirect.com/science/article/pii/S1944398624164478},
author = {Liang Guo and Ying Zhao and Fu-yi Cui},
keywords = {Bayesian network model, Fault diagnosis, Wastewater treatment plant, A2/O process, Bayesian inference},
abstract = {A novel diagnosing faults method is presented using a Bayesian network (BNT) model to optimize system diagnosis for a wastewater treatment plant (WTP) in northern China. The BNT model is established according to the expert knowledge based on local conditions. The historical data of the WTP are employed to implement the parameter learning of the BNT model. Some practical cases are carried out by the BNT model based on the Bayesian inference. The diagnostic results are compared with the monitoring data of that day to verify accuracy of the BNT model. Meanwhile, several fault diagnosis results and improvement measures are given in this study. The results show that the proposed method is robust enough to diagnose the faults quickly and accurately so as to optimize the operation of the WTP.}
}
@article{WU2024101,
title = {Data and knowledge fusion-driven Bayesian networks for interpretable fault diagnosis of HVAC systems},
journal = {International Journal of Refrigeration},
volume = {161},
pages = {101-112},
year = {2024},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2024.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140700724000732},
author = {Daibiao Wu and Haidong Yang and Kangkang Xu and Xianbing Meng and Sihua Yin and Chengjiu Zhu and Xi Jin},
keywords = {Chillers, Fault diagnosis, Bayesian networks, Broad learning system, Fuzzy logic system, Mult-sources information, Refroidisseurs, Diagnostic des défaillances, Réseaux bayésiens, Système d'apprentissage étendu, Système à logique floue, Information multi-sources},
abstract = {Timely diagnosis and maintenance of faults in chiller units is beneficial for reducing energy consumption in buildings. In order to improve the diagnostic accuracy and provide reasonable explanations for fault occurrences of the heating, ventilation, and air conditioning (HVAC) systems in buildings, a fusion-driven Bayesian network fault diagnosis method is proposed based on broad learning system and fuzzy expert knowledge. Firstly, from the data-driven perspective, an efficient broad learning system is developed as benchmarking model to build the prior Bayesian network structure. Secondly, from the knowledge-driven perspective, the fuzzy logic system is used to fuzz expert knowledge, which then used to optimize the Bayesian network. Finally, the information obtained by experts on-site is incorporated into the optimized Bayesian network as new evidence nodes to determine the Bayesian network. The parameters of the Bayesian network are learned through Noisy-MAX processing of the conditional probability table. The performance of the proposed method is evaluated based on the fault diagnosis of a chiller system. The results demonstrate that the Bayesian network established through this method combines the advantages of data-driven and knowledge-driven approaches. It not only performs well in terms of diagnostic accuracy, with accuracy rates above 98 % for six typical faults, but also provides effective explanations for the underlying mechanisms of the faults through causal relationship diagrams.}
}
@article{MONTEWKA201361,
title = {A probabilistic model estimating oil spill clean-up costs – A case study for the Gulf of Finland},
journal = {Marine Pollution Bulletin},
volume = {76},
number = {1},
pages = {61-71},
year = {2013},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2013.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X13005821},
author = {Jakub Montewka and Mia Weckström and Pentti Kujala},
keywords = {Oil spill, Clean-up costs, The Gulf of Finland, Maritime traffic, Bayesian Belief Networks, Risk analysis},
abstract = {Existing models estimating oil spill costs at sea are based on data from the past, and they usually lack a systematic approach. This make them passive, and limits their ability to forecast the effect of the changes in the oil combating fleet or location of a spill on the oil spill costs. In this paper we make an attempt towards the development of a probabilistic and systematic model estimating the costs of clean-up operations for the Gulf of Finland. For this purpose we utilize expert knowledge along with the available data and information from literature. Then, the obtained information is combined into a framework with the use of a Bayesian Belief Networks. Due to lack of data, we validate the model by comparing its results with existing models, with which we found good agreement. We anticipate that the presented model can contribute to the cost-effective oil-combating fleet optimization for the Gulf of Finland. It can also facilitate the accident consequences estimation in the framework of formal safety assessment (FSA).}
}
@article{KAISER2020352,
title = {Bayesian decomposition of multi-modal dynamical systems for reinforcement learning},
journal = {Neurocomputing},
volume = {416},
pages = {352-359},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.12.132},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305026},
author = {Markus Kaiser and Clemens Otte and Thomas A. Runkler and Carl Henrik Ek},
keywords = {Bayesian machine learning, Gaussian processes, Hierarchical gaussian processes, Reinforcement learning, Model-based reinforcement learning, Stochastic policy search, Data-efficiency},
abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.}
}
@article{JIANG2025720,
title = {Multiple financial analyst opinions aggregation based on uncertainty-aware quality evaluation},
journal = {European Journal of Operational Research},
volume = {320},
number = {3},
pages = {720-738},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724006660},
author = {Shuai Jiang and Wenjun Zhou and Yanhong Guo and Hui Xiong},
keywords = {Analyst opinion, Collective wisdom, Uncertainty quantification, Financial technology},
abstract = {Financial analysts’ opinions are pivotal in investment decision-making, as they provide valuable expert knowledge. Aggregating these opinions offers a promising way to unlock their collective wisdom. However, existing opinion aggregation methods are hindered by their inability to effectively assess differences in opinion quality, resulting in suboptimal outcomes. This Study introduces a novel model called SmartMOA, which addresses this limitation by automatically evaluating the quality of each opinion and integrating this evaluation into the aggregation process. Our model begins with a novel Bayesian neural network that leverages the implicit knowledge embedded in the interactions between analysts and stock characteristics. This methodology produces an assessment of individual opinions that accounts for uncertainties. We then formulate a bi-objective combinatorial optimization problem to determine optimal weights for combining multiple analysts’ opinions, simultaneously minimizing the error and uncertainty of the aggregated outcome. Therefore, SmartMOA systematically highlights high-quality opinions during the aggregation process. Using a real dataset spanning eight years, we present comprehensive empirical evidence that demonstrates the superior performance of SmartMOA in heterogeneous analyst opinion aggregation.}
}
@article{COUTINHO202413,
title = {Human-in-the-loop controller tuning using Preferential Bayesian Optimization},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {14},
pages = {13-18},
year = {2024},
note = {12th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.306},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324010486},
author = {João P.L. Coutinho and Ivan Castillo and Marco S. Reis},
keywords = {Bayesian Optimization, Preference Learning, Multi-objective optimization, PID tuning},
abstract = {The development of human-centric platforms that are able to combine computational resources and advanced analytics with human judgment and qualitative processing ability is a key driver of the Industry 5.0 movement. In this setting, humans are not only active in the loop but also play a key role in the decision-making process. In this work, we propose the use of Preferential Bayesian Optimization (PBO) for human-in-the-loop controller tuning. PBO relies on pairwise comparisons and preference feedback (A is better than B) to search for the optimal trade-of between different performance criteria from the user’s perspective. The advantages of PBO are demonstrated in a simulated Proportional Integral (PI) controller tuning example with real user feedback under a reduced number of experiments. The results show that PBO leads to a greater emphasis on closed-loop responses closer to the user’s desired behavior when compared with multi-objective alternatives, while being straightforward to implement from the user’s perspective.}
}
@article{HOQUETANIA2022,
title = {Thinking Aloud or Screaming Inside: Exploratory Study of Sentiment Around Work},
journal = {JMIR Formative Research},
volume = {6},
number = {9},
year = {2022},
issn = {2561-326X},
doi = {https://doi.org/10.2196/30113},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X22008496},
author = {Marzia {Hoque Tania} and Md Razon Hossain and Nuzhat Jahanara and Ilya Andreev and David A Clifton},
keywords = {work-related mental health, sentiment analysis, natural language processing, occupational health, Bayesian inference, machine learning, artificial intelligence, mobile phone},
abstract = {Background
Millions of workers experience work-related ill health every year. The loss of working days often accounts for poor well-being because of discomfort and stress caused by the workplace. The ongoing pandemic and postpandemic shift in socioeconomic and work culture can continue to contribute to adverse work-related sentiments. Critically investigating state-of-the-art technologies, this study identifies the research gaps in recognizing workers’ need for well-being support, and we aspire to understand how such evidence can be collected to transform the workforce and workplace.
Objective
Building on recent advances in sentiment analysis, this study aims to closely examine the potential of social media as a tool to assess workers’ emotions toward the workplace.
Methods
This study collected a large Twitter data set comprising both pandemic and prepandemic tweets facilitated through a human-in-the-loop approach in combination with unsupervised learning and meta-heuristic optimization algorithms. The raw data preprocessed through natural language processing techniques were assessed using a generative statistical model and a lexicon-assisted rule-based model, mapping lexical features to emotion intensities. This study also assigned human annotations and performed work-related sentiment analysis.
Results
A mixed methods approach, including topic modeling using latent Dirichlet allocation, identified the top topics from the corpus to understand how Twitter users engage with discussions on work-related sentiments. The sorted aspects were portrayed through overlapped clusters and low intertopic distances. However, further analysis comprising the Valence Aware Dictionary for Sentiment Reasoner suggested a smaller number of negative polarities among diverse subjects. By contrast, the human-annotated data set created for this study contained more negative sentiments. In this study, sentimental juxtaposition revealed through the labeled data set was supported by the n-gram analysis as well.
Conclusions
The developed data set demonstrates that work-related sentiments are projected onto social media, which offers an opportunity to better support workers. The infrastructure of the workplace, the nature of the work, the culture within the industry and the particular organization, employers, colleagues, person-specific habits, and upbringing all play a part in the health and well-being of any working adult who contributes to the productivity of the organization. Therefore, understanding the origin and influence of the complex underlying factors both qualitatively and quantitatively can inform the next generation of workplaces to drive positive change by relying on empirically grounded evidence. Therefore, this study outlines a comprehensive approach to capture deeper insights into work-related health.}
}
@article{HAAS2024772,
title = {Improving the weld seam quality in laser welding processes by means of Bayesian optimization},
journal = {Procedia CIRP},
volume = {124},
pages = {772-775},
year = {2024},
note = {13th CIRP Conference on Photonic Technologies [LANE 2024], 15-19 September 2024, Fürth, Germany},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124005766},
author = {Michael Haas and Volkher Onuseit and John Powell and Felix Zaiß and Johannes Wahl and Tobias Menold and Christian Hagenlocher and Andreas Michalowski},
keywords = {laser welding, machine learning, process optimization, pore formation, X-ray imaging},
abstract = {The determination of appropriate process parameters is crucial for the development of laser welding processes. This usually requires extensive and time-consuming experimentation combined with expert knowledge. To reduce the number of experiments required to determine appropriate process parameters, Bayesian optimization was used in this work. Bead on plate laser welding of AA5754 samples was performed while optimizing the laser power, the welding speed, the focus position and the power distribution in the core-ring fiber laser system with the objective of achieving welds with a specific weld depth and low number of defects at high welding speeds. The welds were evaluated using X-ray imaging and height measurements. A cost function was developed to quantify the overall weld quality based on the weld properties. It is demonstrated that the Bayesian optimizer can determine appropriate process parameters for the given objective, based on a cost function, within a comparatively small number of 29 experiments.}
}
@article{PITIOT2010830,
title = {Hybridation of Bayesian networks and evolutionary algorithms for multi-objective optimization in an integrated product design and project management context},
journal = {Engineering Applications of Artificial Intelligence},
volume = {23},
number = {5},
pages = {830-843},
year = {2010},
note = {Advances in metaheuristics for hard optimization: new trends and case studies},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2010.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197610000370},
author = {Paul Pitiot and Thierry Coudert and Laurent Geneste and Claude Baron},
keywords = {Project management, Product preliminary design, Evolutionary algorithm, Experience feedback, Bayesian network, Learning},
abstract = {A better integration of preliminary product design and project management processes at early steps of system design is nowadays a key industrial issue. Therefore, the aim is to make firms evolve from classical sequential approach (first product design the project design and management) to new integrated approaches. In this paper, a model for integrated product/project optimization is first proposed which allows taking into account simultaneously decisions coming from the product and project managers. However, the resulting model has an important underlying complexity, and a multi-objective optimization technique is required to provide managers with appropriate scenarios in a reasonable amount of time. The proposed approach is based on an original evolutionary algorithm called evolutionary algorithm oriented by knowledge (EAOK). This algorithm is based on the interaction between an adapted evolutionary algorithm and a model of knowledge (MoK) used for giving relevant orientations during the search process. The evolutionary operators of the EA are modified in order to take into account these orientations. The MoK is based on the Bayesian Network formalism and is built both from expert knowledge and from individuals generated by the EA. A learning process permits to update probabilities of the BN from a set of selected individuals. At each cycle of the EA, probabilities contained into the MoK are used to give some bias to the new evolutionary operators. This method ensures both a faster and effective optimization, but it also provides the decision maker with a graphic and interactive model of knowledge linked to the studied project. An experimental platform has been developed to experiment the algorithm and a large campaign of tests permits to compare different strategies as well as the benefits of this novel approach in comparison with a classical EA.}
}
@article{PEREZCOLO2023120959,
title = {Intelligent approach for the industrialization of deep learning solutions applied to fault detection},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120959},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120959},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014616},
author = {Ivo {Perez Colo} and Carolina {Saavedra Sueldo} and Mariano {De Paula} and Gerardo G. Acosta},
keywords = {Artificial intelligence, Deep neural networks, Bayesian optimization, Industrialization, Fault detection},
abstract = {Early fault detection, both in equipment and the products in process, is of paramount importance in industrial processes to ensure the quality of the final product, avoid abnormal operating conditions, expensive repairs, and even production process shutdown. The growing complexity of industrial systems and the increase in the amount of available data have encouraged the development of intelligent systems for automatic fault prediction/detection, mainly based on Industry 4.0 technologies and, particularly, those based on deep learning methodologies. However, the vast majority of proposals and research carried out to date define specific solutions for specific cases, which still requires a high level of expert knowledge for scaling the solutions to industrial environments. Actually, one of the major issues towards the industrialization of deep learning solutions is the determination of the optimal, or near-optimal, hyper-parameters. In this paper, we propose a low-level set-up effort intelligent failure detection system that integrates deep neural networks with a Bayesian Optimization algorithm for self-tuning of the system hyper-parameters. In addition, to facilitate the industrialization of the proposal and its incorporation into current industrial systems, we embedded the proposal in our previously formulated and tested Simulai architecture which allows for containing and interaction with multiple and heterogeneous technological components of manufacturing processes. Finally, our proposal is tested in two real cases of a different nature. The obtained results show a successful performance and demonstrate the easy online integration and interaction in a real production system.}
}
@article{SAHIN2007124,
title = {Fault diagnosis for airplane engines using Bayesian networks and distributed particle swarm optimization},
journal = {Parallel Computing},
volume = {33},
number = {2},
pages = {124-143},
year = {2007},
note = {Trends in Parallel Computing},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2006.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106001013},
author = {Ferat Sahin and M. Çetin Yavuz and Ziya Arnavut and Önder Uluyol},
keywords = {Bayesian networks, Fault diagnosis, Particle swarm optimization, Parallel computing},
abstract = {This paper presents a fault diagnosis system for airplane engines using Bayesian networks (BN) and distributed particle swarm optimization (PSO). The PSO is inherently parallel, works for large domains and does not trap into local maxima. We implemented the algorithm on a computer cluster with 48 processors using message passing interface (MPI) in Linux. Our implementation has the advantages of being general, robust, and scalable. Unlike existing BN-based fault diagnosis methods, neither expert knowledge nor node ordering is necessary prior to the Bayesian Network discovery. The raw datasets obtained from airplane engines during actual flights are preprocessed using equal frequency binning histogram and used to generate Bayesian networks fault diagnosis for the engines. We studied the performance of the distributed PSO algorithm and generated a BN that can detect faults in the test data successfully.}
}
@article{WANG2025101343,
title = {A clinical treatment recommender system optimizing adjuvant chemoradiotherapy benefit in Chinese women with breast cancer using interpretable causal Bayesian networks},
journal = {The Lancet Regional Health - Western Pacific},
volume = {55},
pages = {101343},
year = {2025},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2024.101343},
url = {https://www.sciencedirect.com/science/article/pii/S2666606524003377},
author = {Ruoyu Wang and Simiao Tian and Zhe Wang and Yiou Wang and Dianlong Zhang and Jianing Jiang and Yongqiang Yao and Hongshen Chen and Hong Fang and Mingqian Cao},
abstract = {Background
Breast cancer (BC) has surpassed lung cancer in becoming the most common cancer among women worldwide. However, few studies have investigated multivariate BC prognosis outcomes. This study aimed to develop and validate a causal model based on Bayesian networks (BN) that can simultaneously predict long-term risk of all-cause mortality and recurrence among Chinese women with BC, and further to estimate individualised expected benefits of model-based treatment recommendations.
Methods
This study used data from adult women who were diagnosed with primary invasive T1-4 N0-3 M0 BC at the Affiliated Zhongshan Hospital of Dalian University, Dalian, Liaoning, China, between January 2011 and December 2017. After the whole cohort was randomly divided into 70% development and 30% validation cohorts, a causal BN (CBN) model was built by aggregating extensive expert knowledge and data-driven approaches to arrive at a consensus structure for causal inference, and subsequently validated its ability to predict both overall survival (OS) and recurrence free survival (RFS) by using the area under the curve (AUC), Brier score (BS), accuracy and predicted/observation ratio. Variables included sociodemographics; preoperative clinical, histopathological and molecular biomarkers; operative variables; and postoperative treatment variables. OS and RFS in women with BC were the main outcomes of this study and were assessed at the most recent follow-up on December 31, 2022.
Findings
A total of 3512 patients, which consisted of 2458 patients in the development cohort (mean [SD] age, 54.60 [10.68] years) and 1054 patients in the validation cohort (mean [SD] age, 53.77 [11.08] years), were eligible for this study. A total of 175 (5.0%) patients with BC died, and 344 (9.8%) experienced recurrence after at least 5-year follow-up. A CBN model was developed that included the following predictors: age group; preoperative CEA, CA 125 and CA15-3 serum levels; neoadjuvant chemoradiotherapy; surgery type; tumour grade and histology; immunohistochemical expression of oestrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) amplification, and Ki-67; pT and pN stages; and postoperative treatment with chemotherapy, radiotherapy and endocrine therapy. In the validation cohort, the CBN model had excellent performance with desirable AUCs of 0.92 (95% CI 0.87-0.98) and 0.73 (0.62-0.83) and low BSs of 0.025 and 0.062 for OS and RFS, respectively. The model also achieved excellent accuracy and model calibration for OS and RFS, with accuracies of 96.4% and 92.8% and predicted/observed ratios of 1.01 (0.92-1.11) and 1.05 (0.95-1.15), respectively. Furthermore, the chemoradiotherapy treatment according to our CBN model recommendations was associated with notably better prognosis, with a hazard ratio of 0.63 (0.44-0.90) for OS and 0.81 for RFS (0.75-0.87), respectively.
Interpretation
In this prognostic study based on clinical real-world data, a CBN model was developed that accurately predicted two important outcomes for women with BC, and it has the potential to identify patients who could benefit from chemoradiotherapy. The CBN can effectively capture complex interrelationships among risk factors and identify potential causal pathways underlying tumour progression through its graphical representation, and could also provide an adjuvant treatment recommendation system for BC women.}
}
@article{FATEHIKARJOU2025100490,
title = {Human-in-the-loop control strategy for IoT-based smart thermostats with Deep Reinforcement Learning},
journal = {Energy and AI},
volume = {20},
pages = {100490},
year = {2025},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2025.100490},
url = {https://www.sciencedirect.com/science/article/pii/S2666546825000229},
author = {Payam {Fatehi Karjou} and Fabian Stupperich and Phillip Stoffel and Dirk Müller},
keywords = {Human-in-the-loop control, AI, TRV, RLC, IoT},
abstract = {Thermostatic Radiator Valves (TRVs) are a widely used technology for regulating room heating in Europe countries. Smart TRVs can provide significant energy savings, often ranging from 20–40% compared to conventional heating systems. They use sensors and algorithms to learn user behavior and optimize heating schedules accordingly. They can often be easily retrofitted to existing heating systems, making them a practical option for enhancing energy efficiency in present buildings, especially in office buildings due to their highly dynamic operational patterns. This work presents a novel human-in-the-loop control strategy for Internet of Things (IoT)-based TRVs using Deep Reinforcement Learning (DRL). A key focus of this research is enhancing the adaptability of agents’ behavior by implementing a more generic and flexible Markov Decision Process (MDP) to promote policy generalization across diverse scenarios. The study explores the challenges of transferring control behaviors from simulation environments to real-world settings, examining the performance across different thermal zones and evaluating the integration flexibility of the control strategy within building systems. Real-world occupant behavior is incorporated, including dynamic comfort preferences and occupancy predictions, to better align thermostat operation with user preferences. Furthermore, this paper discusses the practical challenges encountered during implementation, including battery consumption of IoT devices, integration of occupancy detection and prediction systems, and maintenance requirements. By addressing these issues, the proposed control strategy seeks to improve the scalability and feasibility of IoT-based TRVs, thereby providing a viable solution for their widespread deployment in buildings.}
}
@article{YANG2020113424,
title = {A physics-informed Run-to-Run control framework for semiconductor manufacturing},
journal = {Expert Systems with Applications},
volume = {155},
pages = {113424},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113424},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420302487},
author = {Wei-Ting Yang and Jakey Blue and Agnès Roussy and Jacques Pinaton and Marco S. Reis},
keywords = {Advanced Process Control (APC), Chemical-Mechanical Polishing (CMP), Dynamic Bayesian Network (DBN), Fault Detection and Classification (FDC), Physics-informed, Run-to-Run (R2R) control},
abstract = {For decades, Run-to-Run (R2R) controllers have been widely implemented in semiconductor manufacturing. They operate over key process parameters on the basis of the metrological measurements acquired from the process and their deviations from the target setpoints. Conventionally, R2R controllers have been implemented independently of the actual equipment condition, which is obviously affecting the process stability and performance. Therefore, both equipment signals and process states shall be considered to make the R2R controllers more robust to the equipment condition drifts. In this paper, we propose a novel physics-informed framework to integrate the real-time equipment condition, based on the Fault Detection and Classification (FDC) data, into the R2R controllers. By utilizing Dynamic Bayesian Networks (DBN), the implicit relationship structure between metrology measurements, FDC indicators, and R2R regulators can be learned and reviewed explicitly. The structure shall be further reviewed to valid with the existing relationships and expert knowledge. Infeasible causalities on the structure will be constrained via setting up the blacklist at the structure learning stage. The proposed framework consists of the offline modeling stage, which incorporates the process, equipment variables, and the expert knowledge in the structure learning, and the online control stage, which constructs the Structured R2R controller (SRC) based on the relationship structure. As a result, the model is consistent by design with empirically known relationships and fundamental physical laws. The proposed SRC not only optimizes the operation with respect to the target control values but also considers the equipment and process states simultaneously. The effectiveness of SRC and the derivative control strategy are validated through a real dataset of a Chemical-Mechanical Polishing (CMP) process, and two simulated studies.}
}
@article{SAVAGE2024108810,
title = {Human-algorithm collaborative Bayesian optimization for engineering systems},
journal = {Computers & Chemical Engineering},
volume = {189},
pages = {108810},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108810},
url = {https://www.sciencedirect.com/science/article/pii/S009813542400228X},
author = {Tom Savage and Ehecatl Antonio {del Rio Chanona}},
keywords = {Bayesian optimization, Experimental design, Human-in-the-loop, Domain knowledge},
abstract = {Bayesian optimization has proven effective for optimizing expensive-to-evaluate functions in Chemical Engineering. However, valuable physical insights from domain experts are often overlooked. This article introduces a collaborative Bayesian optimization approach that re-integrates human input into the data-driven decision-making process. By combining high-throughput Bayesian optimization with discrete decision theory, experts can influence the selection of experiments via a discrete choice. We propose a multi-objective approach togenerate a set of high-utility and distinct solutions, from which the expert selects the desired solution for evaluation at each iteration. Our methodology maintains the advantages of Bayesian optimization while incorporating expert knowledge and improving accountability. The approach is demonstrated across various case studies, including bioprocess optimization and reactor geometry design, demonstrating that even with an uninformed practitioner, the algorithm recovers the regret of standard Bayesian optimization. By including continuous expert opinion, the proposed method enables faster convergence and improved accountability for Bayesian optimization in engineering systems.}
}

@article{KRAUSE2023100738,
title = {A tutorial on data mining for Bayesian networks, with a specific focus on IoT for agriculture},
journal = {Internet of Things},
volume = {22},
pages = {100738},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100738},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523000616},
author = {Paul J. Krause and Vivek Bokinala},
abstract = {We are seeing a steady build up in momentum of two trends that will lead to a significant, and necessary, transformation of agriculture in the 21st Century. Firstly, the move to digital with IoT facilitating the use of sensor networks to support precise decision making. Secondly, the move to “ecological intensification”; working with natural processes to lower the carbon footprint of agricultural processes and increase the biodiversity of agricultural units without sacrificing yield. Clearly data mining and machine learning have an important role to play in supporting this transformation. However, given the range of biotic, abiotic and social contexts that need to inform the development of models for decision support in agriculture, we need data mining techniques that support the use of qualitative and unstructured data as well as hard numerical data. In this tutorial we show how Bayesian Networks can be built from a wide range of sources of expert knowledge and data. Whist we use digital agriculture as a key beneficiary of these techniques, this tutorial will also be of interest to all those with an interest in building IoT systems that require combinations of social, technical and environmental understanding.}
}
@article{XUE2025120291,
title = {Cause analysis and management strategies for ship accidents: A Bayesian decision support model},
journal = {Ocean Engineering},
volume = {320},
pages = {120291},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.120291},
url = {https://www.sciencedirect.com/science/article/pii/S002980182500006X},
author = {Jie Xue and Peijie Yang and Ziheng Wang and Qianbing Li and Hao Hu},
keywords = {Ship accident, Accident causes, Risk analysis, Yangtze river Basin, Decision-making, Bayesian network},
abstract = {Addressing the root causes of ship accidents and enhancing operational decision-making is essential. This study develops an enhanced Bayesian decision support model integrating the Leaky Noisy-MAX mechanism to address uncertainties and complexities in ship accident risk analysis, which simplifies conditional probability calculations while accounting for latent factors. By integrating historical 173 accidents data, expert assessments, and quantitative analysis through questionnaires and a risk matrix, key factors are identified through probabilistic reasoning and reverse inference. Decision nodes are incorporated to assess the utility of various risk mitigation strategies. Key findings indicate that high traffic density, numerous fishing vessels, crew fatigue, aging ships, adverse weather conditions, mechanical or device failures significantly influence the probability of ship accidents. Device and machine are identified as sensitive factors with the greatest impact on accident risks. The model also reveals critical causal chains that outline the most likely pathways leading to accidents. The model demonstrates robustness and adaptability through network sensitivity analysis. Proactive strategies and management suggestions are proposed regarding policy implications, waterway construction and ship risk response in the future. This research provides valuable insights into ship accident risk mitigation, offering a decision-making framework applicable across various maritime contexts to enhance safety and operational efficiency.}
}
@article{BEUZEN201816,
title = {Bayesian Networks in coastal engineering: Distinguishing descriptive and predictive applications},
journal = {Coastal Engineering},
volume = {135},
pages = {16-30},
year = {2018},
issn = {0378-3839},
doi = {https://doi.org/10.1016/j.coastaleng.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0378383917303678},
author = {T. Beuzen and K.D. Splinter and L.A. Marshall and I.L. Turner and M.D. Harley and M.L. Palmsten},
keywords = {Bayesian network, Coastal processes, Predictive modelling, Descriptive modelling, Good modelling practice, Storm erosion},
abstract = {Bayesian networks (BNs) are increasingly being used to model complex coastal processes due to their ability to integrate non-linear systems, their transparent probabilistic framework, and low computational cost. A BN may be suited to descriptive or predictive application. Descriptive BNs are highly calibrated models that are useful for better understanding the physics and causal relationships driving a system. Predictive BNs are generalisations of a system that have skill at predicting outside of the training domain. The predictive and descriptive usefulness of a BN depends on its complexity and the amount of data available to train it, but there is often a trade-off; higher descriptive skill comes at the cost of reduced predictive skill. To demonstrate the differences between predictive and descriptive BNs in a coastal engineering context, a BN to predict shoreline recession caused by coastal storm events is developed and tested using an extensive 10-year dataset incorporating 137 individual storms events monitored at Narrabeen-Collaroy Beach, Australia. A parsimonious approach to BN development is used to separately determine the optimum predictive and descriptive BNs for this dataset. Results show that for this dataset two quite different BNs can be developed: one that is optimized to achieve the highest predictive skill, and a second network that is optimized to maximize descriptive skill. The optimum predictive BN is found to comprise 3 nodes (variables) and can predict the shoreline recession caused by unseen storm events with a skill of 65%. The optimum descriptive BN is composed of 5 nodes and can reproduce 88% of the training dataset, but with more limited predictive capabilities. The uses and limitations of these two different approaches to BN formulation are illustrated with example applications to coastal process modelling. It is anticipated that the insights provided in this paper will help to clarify the further development of Bayesian Networks applied to coastal modelling.}
}
@article{HARUNA2022101613,
title = {Adaptability analysis of design for additive manufacturing by using fuzzy Bayesian network approach},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101613},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101613},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000805},
author = {Auwal Haruna and Pingyu Jiang},
keywords = {Design for additive manufacturing, Bayesian Network, Fuzzy Bayesian Network, Design Adaptability, Decision Making, Fuzzy Numbers},
abstract = {The rapid development of Additive Manufacturing (AM) has been conspicuous and appealing towards manufacturing end-use products and components over the past decade. The continual advancement of AM has brought many advantages such as personalization and customization, reduction of material waste, cutting off the existence of special tooling during fabrication, etc. However, the AM approach has its limitations, such as a lack of knowledge of AM process activities and the progressive industrialization of AM, which makes the design process activities unstable, unpredictable, and have a limited effect. The concept of “design for AM (DFAM)” is increasing, which means we have the opportunity to concentrate almost totally on product functioning. Therefore, the entire design paradigm must be revised to accommodate new production capabilities, geometries, and parameters to avoid molding or machine tooling technology constraints. Few studies have attempted to provide systematic and quantitative knowledge of the relationship between these elements and the feasibility of the design process, making it difficult for designers to assess and control AM industrialization. For this reason, DFAM is needed to reform AM from rapid manufacturing to a mainstream manufacturing method. This paper put forward a framework based on the Fuzzy Bayesian Network (FBN) for DFAM decision-making. Twenty impact factors were encapsulated from experts’ experience and existing literature to investigate the potential adaptability of DFAM. The proposed approach uses expert knowledge and Fuzzy Set Theory (FST) presented with Triangular Fuzzy Numbers (FFN) to perceive the uncertainties. The Bayesian Network (BN) captures the causal relationships and dependencies among the impact components and analyzes the DFAM adaptability for robust probabilistic reasoning. A robot arm claw was used to show the effectiveness of our approach. The results showed that FBN could be used to guide DFAM adaptability in the manufacturing industry.}
}
@article{EKMEKCI2019137,
title = {A context aware model for autonomous agent stochastic planning},
journal = {Robotics and Autonomous Systems},
volume = {112},
pages = {137-153},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2018.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018303294},
author = {Omer Ekmekci and Faruk Polat},
keywords = {Markov decision processes, Stochastic planning},
abstract = {Markov Decision Processes (MDPs) are not able to make use of domain information effectively due to their representational limitations. The lacking of elements which enable the models be aware of context, leads to unstructured representation of that problem such as raw probability matrices or lists. This causes these tools significantly less efficient at determining a useful policy as the state space of a task grows, which is the case for more realistic problems having localized dependencies between states and actions. In this paper, we present a new state machine, called Context-Aware Markov Decision Process (CA-MDP) based on MDP for the purpose of representing Markovian sequential decision making problems in a more structured manner. CA-MDP changes and augments MDP facilities by integrating causal relationships between actions and states thereby enabling structural, hence compact if possible, representation of the tasks. To show the expressive power of CA-MDP, we give the theoretical bounds for complexity of conversion between MDP and CA-MDP to demonstrate the expressive power of CA-MDP. Next, to generate an optimal policy from CA-MDP encoding by exploiting those newly defined facilities, we devised a new solver algorithm based on value iteration (VI), called Context-Aware Value Iteration (CA-VI). Although regular dynamic programming (DP) based algorithms is successful at effectively determining optimal policies, they do not scale well with respect to state–action space, making both the MDP encoding and related solver mechanism practically unusable for real-life problems. Our solver algorithm gets the power of overcoming the scalability problem by integrating the structural information provided in CA-MDP. First, we give theoretical analysis of CA-VI by examining the expected number of Bellman updates being performed on arbitrary tasks. Finally, we present our conducted experiments on numerous problems, with important remarks and discussions on certain aspects of CA-VI and CA-MDP, to justify our theoretical analyses empirically and to assess the real performance of CA-VI with CA-MDP formulation by analysing the execution time by checking how close it gets to the practical minimum runtime bound with respect to VI performance with MDP encoding of the same task.}
}
@article{KYRIMI2021102108,
title = {A comprehensive scoping review of Bayesian networks in healthcare: Past, present and future},
journal = {Artificial Intelligence in Medicine},
volume = {117},
pages = {102108},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102108},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001019},
author = {Evangelia Kyrimi and Scott McLachlan and Kudakwashe Dube and Mariana R. Neves and Ali Fahmi and Norman Fenton},
keywords = {Bayesian networks, Healthcare, Clinical decision support, Scoping review, Survey},
abstract = {No comprehensive review of Bayesian networks (BNs) in healthcare has been published in the past, making it difficult to organize the research contributions in the present and identify challenges and neglected areas that need to be addressed in the future. This unique and novel scoping review of BNs in healthcare provides an analytical framework for comprehensively characterizing the domain and its current state. A literature search of health and health informatics literature databases using relevant keywords found 3810 articles that were reduced to 123. This was after screening out those presenting Bayesian statistics, meta-analysis or neural networks, as opposed to BNs and those describing the predictive performance of multiple machine learning algorithms, of which BNs were simply one type. Using the novel analytical framework, we show that: (1) BNs in healthcare are not used to their full potential; (2) a generic BN development process is lacking; (3) limitations exist in the way BNs in healthcare are presented in the literature, which impacts understanding, consensus towards systematic methodologies, practice and adoption; and (4) a gap exists between having an accurate BN and a useful BN that impacts clinical practice. This review highlights several neglected issues, such as restricted aims of BNs, ad hoc BN development methods, and the lack of BN adoption in practice and reveals to researchers and clinicians the need to address these problems. To map the way forward, the paper proposes future research directions and makes recommendations regarding BN development methods and adoption in practice.}
}
@article{AHLUWALIA2021105108,
title = {Policy-based branch-and-bound for infinite-horizon Multi-model Markov decision processes},
journal = {Computers & Operations Research},
volume = {126},
pages = {105108},
year = {2021},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2020.105108},
url = {https://www.sciencedirect.com/science/article/pii/S0305054820302252},
author = {Vinayak S. Ahluwalia and Lauren N. Steimle and Brian T. Denton},
keywords = {Markov decision processes, Parameter uncertainty, Branch-and-bound},
abstract = {Markov decision processes (MDPs) are models for sequential decision-making that inform decision making in many fields, including healthcare, manufacturing, and others. However, the optimal policy for an MDP may be sensitive to the reward and transition parameters which are often uncertain because parameters are typically estimated from data or rely on expert opinion. To address parameter uncertainty in MDPs, it has been proposed that multiple models of the parameters be incorporated into the solution process, but solving these problems can be computationally challenging. In this article, we propose a policy-based branch-and-bound approach that leverages the structure of these problems and numerically compare several important algorithmic designs. We demonstrate that our approach outperforms existing methods on test cases from the literature including randomly generated MDPs, a machine maintenance MDP, and an MDP for medical decision making.}
}
@article{VANGERVEN2008515,
title = {Dynamic Bayesian networks as prognostic models for clinical patient management},
journal = {Journal of Biomedical Informatics},
volume = {41},
number = {4},
pages = {515-529},
year = {2008},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2008.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046408000154},
author = {Marcel A.J. {van Gerven} and Babs G. Taal and Peter J.F. Lucas},
keywords = {Prognosis, Dynamic Bayesian network, Proportional hazards model, Carcinoid tumor},
abstract = {Prognostic models in medicine are usually been built using simple decision rules, proportional hazards models, or Markov models. Dynamic Bayesian networks (DBNs) offer an approach that allows for the incorporation of the causal and temporal nature of medical domain knowledge as elicited from domain experts, thereby allowing for detailed prognostic predictions. The aim of this paper is to describe the considerations that must be taken into account when constructing a DBN for complex medical domains and to demonstrate their usefulness in practice. To this end, we focus on the construction of a DBN for prognosis of carcinoid patients, compare performance with that of a proportional hazards model, and describe predictions for three individual patients. We show that the DBN can make detailed predictions, about not only patient survival, but also other variables of interest, such as disease progression, the effect of treatment, and the development of complications. Strengths and limitations of our approach are discussed and compared with those offered by traditional methods.}
}
@article{KAHN199719,
title = {Construction of a Bayesian network for mammographic diagnosis of breast cancer},
journal = {Computers in Biology and Medicine},
volume = {27},
number = {1},
pages = {19-29},
year = {1997},
issn = {0010-4825},
doi = {https://doi.org/10.1016/S0010-4825(96)00039-X},
url = {https://www.sciencedirect.com/science/article/pii/S001048259600039X},
author = {Charles E. Kahn and Linda M. Roberts and Katherine A. Shaffer and Peter Haddawy},
keywords = {Bayesian networks, Artificial intelligence, Breast cancer, Mammography, Computer-aided diagnosis, Expert systems},
abstract = {Bayesian networks use the techniques of probability theory to reason under uncertainty, and have become an important formalism for medical decision support systems. We describe the development and validation of a Bayesian network (MammoNet) to assist in mammographic diagnosis of breast cancer. MammoNet integrates five patient-history features, two physical findings, and 15 mammographic features extracted by experienced radiologists to determine the probability of malignancy. We outline the methods and issues in the system's design, implementation, and evaluation. Bayesian networks provide a potentially useful tool for mammographic decision support.}
}
@article{BOUTILIER200049,
title = {Stochastic dynamic programming with factored representations},
journal = {Artificial Intelligence},
volume = {121},
number = {1},
pages = {49-107},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00033-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000333},
author = {Craig Boutilier and Richard Dearden and Moisés Goldszmidt},
keywords = {Decision-theoretic planning, Markov decision processes, Bayesian networks, Regression, Decision trees, Abstraction},
abstract = {Markov decision processes (MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specifications and computations. To alleviate the combinatorial problems associated with such methods, we propose new representational and computational techniques for MDPs that exploit certain types of problem structure. We use dynamic Bayesian networks (with decision trees representing the local families of conditional probability distributions) to represent stochastic actions in an MDP, together with a decision-tree representation of rewards. Based on this representation, we develop versions of standard dynamic programming algorithms that directly manipulate decision-tree representations of policies and value functions. This generally obviates the need for state-by-state computation, aggregating states at the leaves of these trees and requiring computations only for each aggregate state. The key to these algorithms is a decision-theoretic generalization of classic regression analysis, in which we determine the features relevant to predicting expected value. We demonstrate the method empirically on several planning problems, showing significant savings for certain types of domains. We also identify certain classes of problems for which this technique fails to perform well and suggest extensions and related ideas that may prove useful in such circumstances. We also briefly describe an approximation scheme based on this approach.}
}
@article{ORDOVAS2023107405,
title = {A Bayesian network model for predicting cardiovascular risk},
journal = {Computer Methods and Programs in Biomedicine},
volume = {231},
pages = {107405},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107405},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300072X},
author = {J.M. Ordovas and D. Rios-Insua and A. Santos-Lozano and A. Lucia and A. Torres and A. Kosgodagan and J.M. Camacho},
keywords = {Bayesian network, Cardiovascular diseases, Healthcare, Disease treatment, Health policy},
abstract = {Background and Objective
Cardiovascular diseases are the leading death cause in Europe and entail large treatment costs. Cardiovascular risk prediction is crucial for the management and control of cardiovascular diseases. Based on a Bayesian network built from a large population database and expert judgment, this work studies interrelations between cardiovascular risk factors, emphasizing the predictive assessment of medical conditions, and providing a computational tool to explore and hypothesize such interrelations.
Methods
We implement a Bayesian network model that considers modifiable and non-modifiable cardiovascular risk factors as well as related medical conditions. Both the structure and the probability tables in the underlying model are built using a large dataset collected from annual work health assessments as well as expert information, with uncertainty characterized through posterior distributions.
Results
The implemented model allows for making inferences and predictions about cardiovascular risk factors. The model can be utilized as a decision- support tool to suggest diagnosis, treatment, policy, and research hypothesis. The work is complemented with a free software implementing the model for practitioners’ use.
Conclusions
Our implementation of the Bayesian network model facilitates answering public health, policy, diagnosis, and research questions concerning cardiovascular risk factors.}
}
@article{MOLINA2010383,
title = {Integrated water resources management of overexploited hydrogeological systems using Object-Oriented Bayesian Networks},
journal = {Environmental Modelling & Software},
volume = {25},
number = {4},
pages = {383-397},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209002679},
author = {J.L. Molina and J. Bromley and J.L. García-Aróstegui and C. Sullivan and J. Benavente},
keywords = {Object-Oriented Bayesian Networks, Decision support systems, Integrated water management, Aquifer overexploitation, Stakeholders' engagement},
abstract = {Object-Oriented Bayesian Networks (OOBNs) have been used increasingly over the past few decades in fields as diverse as medicine, transport and aeronautics. In this paper, OOBNs are applied to the domain of integrated water management and used as a Decision Support System (DSS). This pioneering study, set in the Altiplano region of Murcia in Southern Spain, describes a method for the integrated analysis of a complex water system supplied by groundwater from four aquifers. This method is based on the development of a multivariable integrated technique based on Bayes' theorem. After identifying all relevant factors related to water management in the area these were then translated to variables within a Bayesian Network (BN) and the relationships between them investigated. Each network represented one of the four aquifer units. These individual BNs were then linked to form an OOBN which was used to represent the complex real-world situation. In this way a DSS to simulate the entire water system was constructed using a group of conventional Bns, linked to produce an OOBN. The main stakeholders of the region contributed to network design and construction throughout the entire process. The paper shows how this type of DSS can be used to evaluate the impacts of a range of management strategies that are available to local planners.}
}
@article{DELGADO20111498,
title = {Efficient solutions to factored MDPs with imprecise transition probabilities},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1498-1527},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000026},
author = {Karina Valdivia Delgado and Scott Sanner and Leliane Nunes {de Barros}},
keywords = {Probabilistic planning, Markov Decision Process, Robust planning},
abstract = {When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MDP transition models from an expert or estimation from data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs, they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP and propose efficient dynamic programming methods to exploit its structure. Noting that the key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional “flat” dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-IPs while producing the lowest error of any approximation algorithm evaluated.}
}
@article{XIE2025100214,
title = {Reinforcement learning for vehicle-to-grid: A review},
journal = {Advances in Applied Energy},
volume = {17},
pages = {100214},
year = {2025},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2025.100214},
url = {https://www.sciencedirect.com/science/article/pii/S2666792425000083},
author = {Hongbin Xie and Ge Song and Zhuoran Shi and Jingyuan Zhang and Zhenjia Lin and Qing Yu and Hongdi Fu and Xuan Song and Haoran Zhang},
keywords = {Vehicle-to-grid, Reinforcement learning, Electric vehicle charging, Scheduling optimization, Markov decision process},
abstract = {The rapid development of Vehicle-to-Grid technology has played a crucial role in peak shaving and power scheduling within the power grid. However, with the random integration of a large number of electric vehicles into the grid, the uncertainty and complexity of the system have significantly increased, posing substantial challenges to traditional algorithms. Reinforcement learning has shown great potential in addressing these high-dimensional dynamic scheduling optimization problems. However, there is currently a lack of comprehensive analysis and systematic understanding of reinforcement learning applications in Vehicle-to-Grid, which limits the further development of this technology in the Vehicle-to-Grid domain. To this end, this review systematically analyzes the application of reinforcement learning in Vehicle-to-Grid from the perspective of different stakeholders, including the power grid, aggregators, and electric vehicle users, and clarifies the effectiveness and mechanisms of reinforcement learning in addressing the uncertainty in power scheduling. Based on a comprehensive review of the development trajectory of reinforcement learning in Vehicle-to-Grid applications, this paper proposes a structured framework for method classification and application analysis. It also highlights the major challenges currently faced by reinforcement learning in the Vehicle-to-Grid domain and provides targeted directions for future research. Through this systematic review of reinforcement learning applications in Vehicle-to-Grid, the paper aims to provide relevant references for subsequent studies.}
}
@article{WANG2019106529,
title = {Knowledge representation using non-parametric Bayesian networks for tunneling risk analysis},
journal = {Reliability Engineering & System Safety},
volume = {191},
pages = {106529},
year = {2019},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106529},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018315710},
author = {Fan Wang and Heng Li and Chao Dong and Lieyun Ding},
keywords = {Non-parametric Bayesian networks, Structured expert judgment, Expert system, Risk analysis, Tunneling},
abstract = {Knowledge capture and reuse are critical in the risk management of tunneling works. Bayesian networks (BNs) are promising for knowledge representation due to their ability to integrate domain knowledge, encode causal relationships, and update models when evidence is available. However, the model development based on classic BNs is challenging when expert opinions are solicited due to the discretization of variables and quantification of large conditional probability tables. This study applies non-parametric BNs, which only require the elicitation of the marginal distribution corresponding to each node and correlation coefficient associated with each edge, to develop a knowledge-based expert system for tunneling risk analysis. In particular, we propose to use the pair-wise Pearson's linear correlations to parameterize the model because the assessment is intuitive and experts in the engineering domain are more familiar and comfortable with this notion. However, when Spearman's rank correlation is given, the method can also be used by modification of the marginals. The method is illustrated with a tunnel case in the Wuhan metro project. The expert knowledge of risk assessment for common failures in shield tunneling is integrated and visualized. The developed model is validated by real documented accidents. Potential applications of the model are also explored, such as decision support for risk-based design.}
}
@article{OGUNSINA2022104846,
title = {Relational dynamic Bayesian network modeling for uncertainty quantification and propagation in airline disruption management},
journal = {Engineering Applications of Artificial Intelligence},
volume = {112},
pages = {104846},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104846},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622001026},
author = {Kolawole Ogunsina and Marios Papamichalis and Daniel DeLaurentis},
keywords = {Airline disruption management, Probabilistic graphical models, Hidden Markov models, Intelligent systems, Explainable artificial intelligence, Expert systems},
abstract = {Disruption management during the airline scheduling process can be compartmentalized into proactive and reactive processes depending upon the time of schedule execution. The state of the art for decision-making in airline disruption management involves a heuristic human-centric approach that does not categorically study uncertainty in proactive and reactive processes for managing airline schedule disruptions. Hence, this paper introduces an uncertainty transfer function model (UTFM) framework that characterizes uncertainty for proactive airline disruption management before schedule execution, reactive airline disruption management during schedule execution, and proactive airline disruption management after schedule execution to enable the construction of quantitative tools that can allow an intelligent agent to rationalize complex interactions and procedures for robust airline disruption management. Specifically, we use historical scheduling and operations data from a major U.S. airline to facilitate the development and assessment of the UTFM, defined by hidden Markov models (a special class of probabilistic graphical models) that can efficiently perform pattern learning and inference on portions of large data sets. We employ the UTFM to assess two independent and separately disrupted flight legs from the airline route network. Assessment of a flight leg from Dallas to Houston, disrupted by air traffic control hold for bad weather at Dallas, revealed that proactive disruption management for turnaround in Dallas before schedule execution is impractical because of zero transition probability between turnaround and taxi-out. Assessment of another flight leg from Chicago to Boston, disrupted by air traffic control hold for bad weather at Boston, showed that proactive disruption management before schedule execution is possible because of non-zero state transition probabilities at all phases of flight operation.}
}
@article{CONSTANTINOU201675,
title = {From complex questionnaire and interviewing data to intelligent Bayesian network models for medical decision support},
journal = {Artificial Intelligence in Medicine},
volume = {67},
pages = {75-93},
year = {2016},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2016.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S093336571600004X},
author = {Anthony Costa Constantinou and Norman Fenton and William Marsh and Lukasz Radlinski},
keywords = {Decision support, Expert knowledge, Bayesian networks, Belief networks, Causal intervention, Questionnaire data, Survey data, Mental health, Criminology, Forensic psychiatry},
abstract = {Objectives
(1) To develop a rigorous and repeatable method for building effective Bayesian network (BN) models for medical decision support from complex, unstructured and incomplete patient questionnaires and interviews that inevitably contain examples of repetitive, redundant and contradictory responses; (2) To exploit expert knowledge in the BN development since further data acquisition is usually not possible; (3) To ensure the BN model can be used for interventional analysis; (4) To demonstrate why using data alone to learn the model structure and parameters is often unsatisfactory even when extensive data is available.
Method
The method is based on applying a range of recent BN developments targeted at helping experts build BNs given limited data. While most of the components of the method are based on established work, its novelty is that it provides a rigorous consolidated and generalised framework that addresses the whole life-cycle of BN model development. The method is based on two original and recent validated BN models in forensic psychiatry, known as DSVM-MSS and DSVM-P.
Results
When employed with the same datasets, the DSVM-MSS demonstrated competitive to superior predictive performance (AUC scores 0.708 and 0.797) against the state-of-the-art (AUC scores ranging from 0.527 to 0.705), and the DSVM-P demonstrated superior predictive performance (cross-validated AUC score of 0.78) against the state-of-the-art (AUC scores ranging from 0.665 to 0.717). More importantly, the resulting models go beyond improving predictive accuracy and into usefulness for risk management purposes through intervention, and enhanced decision support in terms of answering complex clinical questions that are based on unobserved evidence.
Conclusions
This development process is applicable to any application domain which involves large-scale decision analysis based on such complex information, rather than based on data with hard facts, and in conjunction with the incorporation of expert knowledge for decision support via intervention. The novelty extends to challenging the decision scientists to reason about building models based on what information is really required for inference, rather than based on what data is available and hence, forces decision scientists to use available data in a much smarter way.}
}
@article{MEDINA20137034,
title = {An investigation of critical factors in medical device development through Bayesian networks},
journal = {Expert Systems with Applications},
volume = {40},
number = {17},
pages = {7034-7045},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413003941},
author = {Lourdes A. Medina and Marija Jankovic and Gül E. {Okudan Kremer} and Bernard Yannou},
keywords = {Medical device development, Bayesian networks},
abstract = {In this paper, we investigate the impact of product, company context and regulatory environment factors for their potential impact on medical device development (MDD). The presented work investigates the impact of these factors on the Food and Drug Administration’s (FDA) decision time for submissions that request clearance, or approval to launch a medical device in the market. Our overall goal is to identify critical factors using historical data and rigorous techniques so that an expert system can be built to guide product developers to improve the efficiency of the MDD process, and thereby reduce associated costs. We employ a Bayesian network (BN) approach, a well-known machine learning method, to examine what the critical factors in the MDD context are. This analysis is performed using the data from 2400 FDA approved orthopedic devices that represent products from 474 different companies. Presented inferences are to be used as the backbone of an expert system specific to MDD.}
}
@article{YAO2024102701,
title = {A novel completion status prediction for the aircraft mixed-model assembly lines: A study in dynamic Bayesian networks},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102701},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102701},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003495},
author = {Ya Yao and Jie Zhang and Shoushan Jiang and Yixuan Li and Tengfei Long},
keywords = {Aircraft, Mixed-model assembly line, Dynamic Bayesian network, Completion status prediction},
abstract = {In the context of Industry 4.0, amidst the normalization of multi-variety and low-volume custom flexible production models, aircraft mixed-model assembly lines (AMMALs) have become widely adopted in various aerospace companies. Within AMMALs, each aircraft has its own manufacturing cycles and delivery dates, making accurate prediction of the completion status of each model critical. However, owing to the complexity of production operation data, the dynamic time-varying nature of the process, its weak regularity, and other factors, current methods for predicting completion states struggle to ensure prediction accuracy. Moreover, owing to the product characteristics of aircraft and economic considerations, it is difficult to accumulate a substantial amount of data for different models, especially for newly improved models. To solve the problem of completion state prediction in the context of dynamic time-varying conditions and limited data accumulation, we propose a Dynamic Bayesian Networks (DBNs)-based method for predicting AMMAL completion states. First, we construct a hierarchical agent model for cross-products based on DBNs to effectively generalize the dynamic time-varying features across other multi-products. Then, we utilize expert knowledge and DBN parameter learning to solve small-sample problems in AMMALs. Simultaneously, we propose a model synchronization update mechanism based on improved particle filtering to enhance reasoning speed and prediction accuracy. Experiments conducted at AMMALs have demonstrated the approach’s feasibility, yielding results showing improved predictive ability of the job model. This method enables aerospace companies to perceive development trends and make scientific decisions in advance.}
}
@article{JIAO2025124343,
title = {The secrets to high-level green technology innovation of China's waste power battery recycling enterprises},
journal = {Journal of Environmental Management},
volume = {375},
pages = {124343},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.124343},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725003196},
author = {Jianling Jiao and Yuqin Chen and Jingjing Li and Shanlin Yang},
keywords = {Green technology innovation, Waste power battery recycling, PSR, Bayesian network, GPT-4},
abstract = {Green technology innovation (GTI) in China's waste power battery recycling (WPBR) sector is a key driver for sustainable resource management, environmental protection, and economic prosperity. Using the PSR-BN-GPT-4 model and multi-source data, this study explores China's WPBRenterprises' high-level GTI mechanism. The research concludes that (1) Compared to traditional expert knowledge, the Bayesian network model based on GPT-4 exhibits superior causal reasoning capability. (2) The current level of GTI in China's WPBR industry is relatively low, with the probability of high-level GTI being only 19%. (3) Key factors identified include incentives like R&D investment, bottlenecks such as green finance policy tools, and hindrances like government procurement policy tools. (4) “Supporting Infrastructure Policy Tools - Recycling Outlets Number - Market Potential -Green Technology Innovation” and “Green Finance Policy Tools - R&D Investment - Green Technology Innovation” are two critical paths for enhancing the high-level development of GTI in WPBR enterprises. The study offers valuable insights for governmental, industrial, and corporate decision-making regarding GTI in battery recycling.}
}
@article{DINCER2024721,
title = {Assessment of water electrolysis projects for green hydrogen production with a novel hybrid Q-learning algorithm and molecular fuzzy-based modelling},
journal = {International Journal of Hydrogen Energy},
volume = {95},
pages = {721-733},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.11.262},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924049528},
author = {Hasan Dinçer and Serkan Eti and Merve Acar and Serhat Yüksel},
keywords = {Water electrolysis, Green hydrogen, Molecular fuzzy, Q-learning, Energy investment},
abstract = {Determining the most important criteria for increasing the efficiency of water electrolysis investments provides businesses with a competitive advantage. Although there are many studies in the literature on this importance, there are very few studies determining the most important of these performance indicators. To satisfy this gap, the purpose of this study is to make assessment of water electrolysis projects for green hydrogen production via a novel model. First, the balanced expert evaluation matrices are obtained by q-learning algorithm. Secondly, the criteria for water electrolysis investments are prioritized using molecular fuzzy Bayesian network (BANEW). Thirdly, green hydrogen strategies for water electrolysis investments are ranked with molecular fuzzy multi-objective particle swarm optimization (MOPSO). The most important contribution of this study to the literature is the determination of the criteria that should be applied primarily for the performance increase of water electrolysis investments by creating a new model. The use of molecular fuzzy numbers is a very important contribution of the model to the literature. In this process, the use of three-dimensional geometric figures allows the reduction of uncertainties in decision-making processes. The findings indicate that lifespan of electrolysers and production capacity are the most essential criteria. Additionally, proton exchange membrane electrolysers and alkaline water electrolysis are found as the most critical green hydrogen strategies. Extending the life of electrolysers is crucial to increase sustainability in hydrogen production and reduce long-term costs. In this context, research incentives should be provided for the development of materials and technologies to increase the durability of electrolysers. Similarly, establishing quality standards to extend the life of electrolysers also contributes to achieving this goal.}
}
@article{KUMAR2022103632,
title = {Bayesian network modeling for economic-socio-cultural sustainability of neighborhood-level urban communities: Reflections from Kolkata, an Indian megacity},
journal = {Sustainable Cities and Society},
volume = {78},
pages = {103632},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103632},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008957},
author = {Sudeshna Kumar and Haimanti Banerji},
abstract = {The study has adopted a knowledge-based, decision support system of Bayesian network (BN) modeling approach to address the lack of incorporation of the interlinkage of dimensions and uncertainty associated with neighborhood sustainability assessment (NSA). The study aims in developing a three-tier top-down BN model incorporating expert elicitation with three sub-models constituting 30 nodes concerning the Economic, Social, and Cultural Dimensions for the assessment of economic-sociocultural (ESC) sustainability of neighborhood-level urban communities (NLUCs) in Kolkata, an Indian megacity. The conceptual BN investigates the causal relationships and effects of different indicators and sub-models by quantifying their influence in achieving ESC sustainability. The study has proposed a robust algorithm for model parametrization and a detailed methodology for its structure development and validation. It has been tested using 550 sets of real-world survey data, with a prediction error rate of 2% for the query node. The model suggests the social sub-model is most sensitive to ESC sustainability, followed by the Economic and Cultural sub-models. It concludes with a discourse concerning spatial planning of NLUCs on dealing with the COVID-19 pandemic in compact cities of the Global South and discusses the relevance of the indicator set for achieving long-term sustainability.}
}
@article{FU2024470,
title = {Risk assessment of fire casualty in underground commercial building based on FFTA-BN model},
journal = {Journal of Safety Science and Resilience},
volume = {5},
number = {4},
pages = {470-485},
year = {2024},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2666449624000495},
author = {Wenjun Fu and Jintao Li and Jinghong Wang and Jialin Wu},
keywords = {Underground commercial building fire incidents, Risk of fire casualty, Fuzzy fault tree analysis, Bayesian network, Expert evaluation},
abstract = {With the development of urbanization, underground commercial buildings (UCB) are facing severe challenges in fire safety management due to their unique structure and environmental characteristics. This study constructed a fire casualty risk assessment model that combines fuzzy fault tree analysis (FFTA) and Bayesian network (BN), aiming to quantitatively analyze the dynamic risk of casualties caused by fires in UCB. Fault tree analysis (FTA) is used to comprehensively identify the key risk factors leading to fire casualties in UCB, involving 55 basic events, and the occurrence probability of basic events was calculated via a fuzzy set. The FTA model was transformed into a BN structure via conversion rules and was optimized. The optimized BN model can dynamically analyze the specific fire evolution process and quantify the impacts of different emergency response measures on fire control, evacuation, and casualties. Innovatively, from the post-incident (a historical case study) and pre-incident (two potentially different fire scenarios) perspectives, various emergency plans were scientifically evaluated, providing reasonable suggestions and decision support for emergency management. The results indicate that the model can effectively guide the formulation of fire prevention and control strategies and emergency response work of UCB and provide an innovative tool for improving the safety of UCB and reducing fire accidents and casualties.}
}
@article{BONZANIGO2016665,
title = {Conditions for the Adoption of Conservation Agriculture in Central Morocco: An Approach Based on Bayesian Network Modelling},
journal = {Italian Journal of Agronomy},
volume = {11},
number = {1},
pages = {665},
year = {2016},
issn = {1125-4718},
doi = {https://doi.org/10.4081/ija.2016.665},
url = {https://www.sciencedirect.com/science/article/pii/S1125471824005681},
author = {Laura Bonzanigo and Carlo Giupponi and Rachid Moussadek},
keywords = {Adoption, Bayesian decision networks, Central Morocco, conservation agriculture},
abstract = {ABSTRACT
Research in Central Morocco, proves that conservation agriculture increases yields, reduces labour requirements, and erosion, and improves soil fertility. However, after nearly two decades of demonstration and advocacy, adoption is still limited. This paper investigates the critical constraints and potential opportunities for the adoption of conservation agriculture for different typologies of farms. We measured the possible pathways of adoption via a Bayesian decision network (BDN). BDNs allow the inclusion of stakeholders’ knowledge where data is scant, whilst at the same time they are supported by a robust mathematical background. We first developed a conceptual map of the elements affecting the decision about tillage, which we refined in a workshop with farmers and researchers from the Settat area. We then involved experts in the elicitation of conditional probabilities tables, to quantify the cascade of causal links that determine (or not) the adoption. Via BDNs, we could categorise under which specific technical and socio-economic conditions no tillage agriculture is best suited to which farmers. We, by identifying the main constraints and running sensitivity analyses, were able to convey clear messages on how policy-makers may facilitate the conversion. As new evidence is collected, the BDN can be updated to obtain evidence more targeted and fine tuned to the adoption contexts.}
}
@article{CHARITOS20091249,
title = {A dynamic Bayesian network for diagnosing ventilator-associated pneumonia in ICU patients},
journal = {Expert Systems with Applications},
volume = {36},
number = {2, Part 1},
pages = {1249-1258},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2007.11.065},
url = {https://www.sciencedirect.com/science/article/pii/S0957417407005805},
author = {Theodore Charitos and Linda C. {van der Gaag} and Stefan Visscher and Karin A.M. Schurink and Peter J.F. Lucas},
keywords = {Ventilator-associated pneumonia, Diagnosis, Dynamic Bayesian networks, Stochastic processes, Inference},
abstract = {Diagnosing ventilator-associated pneumonia in mechanically ventilated patients in intensive care units is seen as a clinical challenge. The difficulty in diagnosing ventilator-associated pneumonia stems from the lack of a simple yet accurate diagnostic test. To assist clinicians in diagnosing and treating patients with pneumonia, a decision-theoretic network had been designed with the help of domain experts. A major limitation of this network is that it does not represent pneumonia as a dynamic process that evolves over time. In this paper, we construct a dynamic Bayesian network that explicitly captures the development of the disease over time. We discuss how probability elicitation from domain experts served to quantify the dynamics involved and how the nature of the patient data helps reduce the computational burden of inference. We evaluate the diagnostic performance of our dynamic model for a number of real patients and report promising results.}
}
@article{RETTIG2023108158,
title = {A bayesian network to inform the management of key species in Kosterhavet National Park under contrasting storylines of environmental change},
journal = {Estuarine, Coastal and Shelf Science},
volume = {280},
pages = {108158},
year = {2023},
issn = {0272-7714},
doi = {https://doi.org/10.1016/j.ecss.2022.108158},
url = {https://www.sciencedirect.com/science/article/pii/S0272771422004164},
author = {Katharina Rettig and Andreas Skriver Hansen and Matthias Obst and Daniel Hering and Christian K. Feld},
keywords = {Coastal ecosystem, Water quality, Eelgrass, Northern shrimp, Human activities},
abstract = {Global climate change and related land use changes are expected to impose unprecedented pressures on coastal biodiversity and ecosystem processes. To sustainably manage coastal ecosystems, it is crucial to predict the consequences of human activities for coastal ecosystems and identify areas for directed abatement measures. Empirical data together with expert knowledge and evidence from the literature were integrated into a Bayesian Belief Network (BBN) for a marine protected area, the Kosterhavet National Park off the Swedish west coast. The variability and interactions of anthropogenic pressures and two key ecosystem components, eelgrass meadows and northern shrimp stock, were tested under four storylines of environmental change. The results show that of the influential drivers of environmental change, only three variables (bottom trawling, leisure boating and aquaculture) are manageable within the national park itself. Scenario analysis suggested that notable gains of both ecosystem components were most likely under a storyline of sustainable development, assuming a radiative forcing of 4.5 W/m2 by 2100 in concert with a preventive cooperation among neighboring countries and a tighter restriction of commercial and recreational uses in the park area. The findings suggest that the sustainable management of eelgrass meadows and northern shrimp stock in Kosterhavet National Park requires both local measures at the scale of the park's water bodies and, to a greater part, also regional measures, e.g., to reduce nutrient influx from adjacent water bodies. In conclusion, this approach can help practitioners to make more informed management decisions and foresee the effects of routes of socio-economic development.}
}
@article{LI2025100039,
title = {Optimizing post-hurricane recovery of interdependent infrastructure systems via knowledge-enhanced deep reinforcement learning},
journal = {Advances in Wind Engineering},
volume = {2},
number = {1},
pages = {100039},
year = {2025},
issn = {2950-6018},
doi = {https://doi.org/10.1016/j.awe.2025.100039},
url = {https://www.sciencedirect.com/science/article/pii/S2950601825000107},
author = {Shaopeng Li and Teng Wu},
keywords = {Traffic network, Electric power network, Interdependent infrastructure, Hurricane, Reinforcement learning, Deep neural network},
abstract = {The swift restoration of infrastructure systems damaged by hurricane hazards (e.g., strong winds, heavy rainfall, and high surges/waves) hinges on efficiently managing the limited resources for different repair tasks during the recovery process, while prioritizing repair tasks can be significantly influenced by the interdependencies among these infrastructure systems. The restoration for interdependent infrastructure systems following hurricane damage can be formulated as a stochastic sequential decision problem using Markov decision process (MDP). This study employs knowledge-enhanced deep reinforcement learning (RL) to tackle the dynamic optimization of the MDP. Specifically, a deep neural network (DNN) is used to represent the recovery policy, which maps observations of the system's status to the allocation decision of repair resources. The optimal DNN weights are determined through RL techniques. Domain knowledge is integrated into the learning process by the mechanisms of knowledge-guided exploration and knowledge-based reward to enhance the training efficiency of the deep RL approach. To illustrate the effectiveness of the proposed framework, a case study is conducted to optimize the post-hurricane recovery of an interconnected traffic-electric power network. The simulation results show the improved training efficiency achieved by incorporating domain knowledge into the deep RL algorithm and highlight the advantages of collaborative decision-making over independent scheduling.}
}
@article{VALVERDE2023106657,
title = {Causal reinforcement learning based on Bayesian networks applied to industrial settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106657},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106657},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008412},
author = {Gabriel Valverde and David Quesada and Pedro Larrañaga and Concha Bielza},
keywords = {Reinforcement learning, Bayesian networks, Causality, Parameter learning, Dynamic simulators, Ordinary differential equations},
abstract = {The increasing amount of real-time data collected from sensors in industrial environments has accelerated the application of machine learning in decision-making. Reinforcement learning (RL) is a powerful tool to find optimal policies for achieving a given goal. However, RL’s typical application is risky and insufficient in environments where actions can have irreversible consequences and require interpretability and fairness. While new trends in RL may provide guidance based on expert knowledge, they do not often consider uncertainty or include prior knowledge in the learning process. We propose a causal reinforcement learning alternative based on Bayesian networks (RLBNs) to address this challenge. The RLBN simultaneously models a policy and takes advantage of the joint distribution of the state and action space, reducing uncertainty in unknown situations. We propose a training algorithm for the network’s parameters and structure based on the reward function and likelihood of the effects and measurements taken. Our experiment with the CartPole benchmark and industrial fouling using ordinary differential equations (ODEs) demonstrates that RLBNs are interpretable, secure, flexible, and more robust than their competitors. Our contributions include a novel method that incorporates expert knowledge into the decision-making engine. It uses Bayesian networks with a predefined structure as a causal graph and a hybrid learning strategy that considers both likelihood and reward. This would avoid losing the virtues of the Bayesian network.}
}
@article{MROWCZYNSKA2022116035,
title = {A new fuzzy model of multi-criteria decision support based on Bayesian networks for the urban areas' decarbonization planning},
journal = {Energy Conversion and Management},
volume = {268},
pages = {116035},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116035},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422008251},
author = {M. Mrówczyńska and M. Skiba and A. Leśniak and A. Bazan-Krzywoszańska and F. Janowiec and M. Sztubecka and R. Grech and J.K. Kazak},
keywords = {Cities' sustainable development, Renewable energy sources, Energy policy scenarios, Bayesian network, Fuzzy Analytical Hierarchy Process, Geographic Information System},
abstract = {The study introduces a framework for forecasting and decision-making in multi-criteria processes and proposes their application in the decarbonization of urban areas. Optimizing the multi-criteria decision-making process is an integrated set of information-processing-decision activities in which actual data, expert knowledge using fuzzy inference rules, Geographic Information System, and Bayesian networks are combined. Using proposed tools leads to designing a new approach to improving the energy efficiency of cities and reducing CO2 emissions using renewable energy. The integration of modern computational methods leads to rational planning of environmentally friendly and energy-conscious smart cities by the provisions of the Fit for 55 packages. The effectiveness of the proposed approach has been demonstrated in the example of three scenarios considering different types of renewable energy sources that can be implemented in urban areas. The success probability of decarbonizing these areas was calculated for defined quarters of the city of Zielona Góra with different parameters. Thereby the usefulness of the method was confirmed. Significantly, the likelihood of a successful deployment of photovoltaics (PV) in urban areas was estimated at 55.25% and for heat pumps at 28.79%. The proposed method enables a clear interpretation of the results, which may be the basis for urban energy efficiency planning.}
}
@article{NGUYEN202310101,
title = {A Spectral Method of Moments for Hierarchical Imitation Learning},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {10101-10106},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.881},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323012612},
author = {Nguyen Nguyen and Timothy L. Molloy and Girish N. Nair and Ioannis {Ch. Paschalidis}},
keywords = {Learning for control, Machine learning, Markov Decision Processes, Options, Imitation Learning, Method of Moments},
abstract = {Recent empirical success has led to a rise in popularity of the options framework for Hierarchical Reinforcement Learning (HRL). This framework tackles the scalability problem in Reinforcement Learning (RL) by introducing a layer of abstraction (i.e. high-level options) over the (low-level) decision process. Hierarchical Imitation Learning (HIL) is the problem of learning low-level and high-level policies within HRL from expert demonstrations consisting only of the low-level actions and states, with the high-level options being hidden (or latent). Due to the latent options, recent work on HIL has focused on the development of Expectation-Maximization (EM) algorithms inspired by approaches such as the celebrated Baum-Welch algorithm for hidden Markov models (HMMs). In this work, we take a different approach and derive a new HIL framework inspired by the spectral method of moments for HMMs. The method of moments offers global and consistent convergence under mild regulatory conditions, whilst only requiring one sweep through the data set of state and action pairs, giving it a competitive run time.}
}
@article{DOBE2022104978,
title = {Model checking hyperproperties for Markov decision processes},
journal = {Information and Computation},
volume = {289},
pages = {104978},
year = {2022},
note = {Special Issue on 11th Int. Symp. on Games, Automata, Logics and Formal Verification},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2022.104978},
url = {https://www.sciencedirect.com/science/article/pii/S089054012200133X},
author = {Oyendrila Dobe and Erika Ábrahám and Ezio Bartocci and Borzoo Bonakdarpour},
keywords = {Markov models, Hyperproperties, Model checking, Policy synthesis, Information leakage},
abstract = {We study the problem of formalizing and checking probabilistic hyperproperties for Markov decision processes (MDPs). We introduce the temporal logic HyperPCTL that allows explicit and simultaneous quantification over schedulers as well as probabilistic computation trees. We show that the logic can express important quantitative requirements in security and privacy such as probabilistic noninterference, differential privacy, timing side-channel countermeasures, and probabilistic conformance testing. We show that HyperPCTL model checking over MDPs is in general undecidable, but restricting the domain of scheduler quantification to memoryless non-probabilistic schedulers turns the model checking problem decidable. Subsequently, we propose an SMT-based encoding for model checking this language. Finally, we demonstrate the applicability of our method by providing experimental results for verification, and we show how it can be used to solve even certain synthesis problems.}
}
@article{KHONJI2023103968,
title = {Approximability and efficient algorithms for constrained fixed-horizon POMDPs with durative actions},
journal = {Artificial Intelligence},
volume = {323},
pages = {103968},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103968},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001145},
author = {Majid Khonji},
keywords = {Constrained partially observable Markov decision process, Durative actions, Risk-bounded planning, Heuristic search},
abstract = {Partially Observable Markov Decision Process (POMDP) is a fundamental model for probabilistic planning in stochastic domains. More recently, constrained POMDP and chance-constrained POMDP extend the model allowing constraints to be specified on some aspects of the policy in addition to the objective function. Despite their expressive power, these models assume all actions take a fixed duration, which poses a limitation in modeling real-world planning problems. In this work, we propose a unified model for durative POMDP and its constrained extensions. First, we convert these extensions into an Integer Linear Programming (ILP) formulation, which can be solved using existing solvers in the ILP literature. Second, a heuristic search approach is provided that can efficiently prune the search space, guided by solving successive partial ILP programs. Third, we give a theoretical analysis of the problem: unlike short-horizon POMDPs, with policies of a constant depth, which can be solved in polynomial time, the constrained extensions are NP-Hard even with a planning horizon of two and non-negative rewards. To alleviate that, we propose a Fully Polynomial Time Approximation Scheme (FPTAS) that computes (near) optimal deterministic policies in polynomial time. The FPTAS is among the best achievable in theory in terms of approximation ratio. Finally, evaluation results show that our approach is empirically superior to the state-of-the-art fixed-horizon chance-constrained POMDP solver.}
}
@article{PEREZPIQUERAS2023106555,
title = {FEDA-NRP: A fixed-structure multivariate estimation of distribution algorithm to solve the multi-objective Next Release Problem with requirements interactions},
journal = {Engineering Applications of Artificial Intelligence},
volume = {124},
pages = {106555},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106555},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300739X},
author = {Víctor Pérez-Piqueras and Pablo Bermejo and José A. Gámez},
keywords = {Next Release problem, Estimation of distribution algorithms, Evolutionary multi-objective search, Search-based software engineering, Bayesian networks, Agile},
abstract = {In the development of a software product, the Next Release Problem is the selection of the most appropriate subset of requirements (tasks) to include in the next release of the product, such that the selected subset maximises the overall satisfaction of the stakeholders and minimises the total cost. Furthermore, in most cases, requirements or tasks cannot be developed independently, as there are dependencies between them, which must be respected in the selection for the next release. In this paper, we approach the Next Release Problem as a constrained bi-objective optimisation problem. The main contribution is the design of an Estimation of Distribution Algorithm that exploits domain knowledge, i.e. the dependencies between the requirements, to define the structure of a Bayesian network that models the relationships between the binary variables (requirements) to be optimised. The use of a Bayesian network with a fixed structure reduces the complexity of the search process, since it is unnecessary to learn the structure at each iteration of the algorithm. Moreover, it ensures that the sampled individuals are always valid with respect to the required dependencies. The second main contribution is the generation of a corpus of synthetic datasets with cost estimations derived from agile and classic management methodologies. Standard multi-objective metrics are computed in order to assess our proposal and compare it with other evolutionary multi-criterion optimisation algorithms, determining that it is the optimal choice when dealing with complex datasets.}
}
@article{NADKARNI2001479,
title = {A Bayesian network approach to making inferences in causal maps},
journal = {European Journal of Operational Research},
volume = {128},
number = {3},
pages = {479-498},
year = {2001},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(99)00368-9},
url = {https://www.sciencedirect.com/science/article/pii/S0377221799003689},
author = {Sucheta Nadkarni and Prakash P Shenoy},
keywords = {Causal maps, Cognitive maps, Bayesian networks, Bayesian causal maps},
abstract = {The main goal of this paper is to describe a new graphical structure called ‘Bayesian causal maps’ to represent and analyze domain knowledge of experts. A Bayesian causal map is a causal map, i.e., a network-based representation of an expert’s cognition. It is also a Bayesian network, i.e., a graphical representation of an expert’s knowledge based on probability theory. Bayesian causal maps enhance the capabilities of causal maps in many ways. We describe how the textual analysis procedure for constructing causal maps can be modified to construct Bayesian causal maps, and we illustrate it using a causal map of a marketing expert in the context of a product development decision.}
}
@article{FRAYER2014248,
title = {Analyzing the drivers of tree planting in Yunnan, China, with Bayesian networks},
journal = {Land Use Policy},
volume = {36},
pages = {248-258},
year = {2014},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0264837713001555},
author = {Jens Frayer and Zhanli Sun and Daniel Müller and Darla K. Munroe and Jianchu Xu},
keywords = {Forest transition, SLCP, Afforestation, Land use change, Bayesian belief network, China},
abstract = {Strict enforcement of forest protection and massive afforestation campaigns have contributed to a significant increase in China's forest cover during the last 20 years. At the same time, demographic changes in rural areas due to changes in reproduction patterns and the emigration of younger population segments have affected land-use strategies. We identified proximate causes and underlying drivers that influence the decisions of farm households to plant trees on former cropland with Bayesian networks (BNs). BNs allow the incorporation of causal relationships in data analysis and can combine qualitative stakeholder knowledge with quantitative data. We defined the structure of the network with expert knowledge and in-depth discussions with land users. The network was calibrated and validated with data from a survey of 509 rural households in two upland areas of Yunnan Province in Southwest China. The results substantiate the influence of land endowments, labor availability and forest policies for switching from cropland to tree planting. State forest policies have constituted the main underlying driver to the forest transition in the past, but private afforestation activities increasingly dominate the expansion of tree cover. Farmers plant trees on private incentives mainly to cash in on the improved economic opportunities provided by tree crops, but tree planting also constitutes an important strategy to adjust to growing labor scarcities.}
}
@article{MEYER201442,
title = {Development of a stakeholder-driven spatial modeling framework for strategic landscape planning using Bayesian networks across two urban-rural gradients in Maine, USA},
journal = {Ecological Modelling},
volume = {291},
pages = {42-57},
year = {2014},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2014.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0304380014003056},
author = {Spencer R. Meyer and Michelle L. Johnson and Robert J. Lilieholm and Christopher S. Cronan},
keywords = {Bayesian networks, Conservation, Land use planning, Stakeholder engagement, Natural resource management, Land use suitability},
abstract = {Land use change results from frequent, independent actions by decision-makers working in isolation, often with a focus on a single land use. In order to develop integrated land use policies that encourage sustainable outcomes, scientists and practitioners must understand the specific drivers of land use change across mixed land use types and ownerships, and must consider the combined influences of biophysical, economic, and social factors that affect land use decisions. In this analysis of two large watersheds covering a total of 1.9 million hectares in Maine, USA, we co-developed with groups of stakeholders land use suitability models that integrated four land uses: economic development, ecosystem protection, forestry, and agriculture. We elicited stakeholder knowledge to: (1) identify generalized drivers of land use change; (2) construct Bayesian network models of suitability for each of the four land uses based on site-level factors that affect land use decisions; and (3) identify thresholds of suitability for each factor and give relative weights to each factor. We then applied 12 distinct Bayesian models using 99 spatially explicit, empirical socio-economic and biophysical datasets to predict spatially the suitability for each of our four land uses on a 30m×30m pixel basis across 1.9 million hectares. We evaluated both the stakeholder engagement process and the land use suitability maps. Results demonstrated the potential efficacy of these models for strategic land use planning, but also revealed that trade-offs occur when stakeholder knowledge is used to augment limited empirical data. First, stakeholder-derived Bayesian land use models can provide decision-makers with relevant insights about the factors affecting land use change. Unfortunately, these models are not easily validated for predictive purposes. Second, integrating stakeholders throughout different phases of the modeling process provides a flexible framework for developing localized or generalizable land use models depending on the scope of stakeholder knowledge and available empirical data. The potential downside is that this can lead to more complex models than anticipated. The trade-offs between model rigor and relevance suggest an adaptive management approach to modeling is needed to improve the integration of stakeholder knowledge into robust land use models.}
}
@article{VANGERVEN2007171,
title = {Selecting treatment strategies with dynamic limited-memory influence diagrams},
journal = {Artificial Intelligence in Medicine},
volume = {40},
number = {3},
pages = {171-186},
year = {2007},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2007.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S093336570700053X},
author = {Marcel A.J. {van Gerven} and Francisco J. Díez and Babs G. Taal and Peter J.F. Lucas},
keywords = {Planning, Partially observable Markov decision processes, Limited-memory influence diagrams, Simulated annealing, Carcinoid tumors},
abstract = {Summary
Objective
The development of dynamic limited-memory influence diagrams as a framework for representing factorized infinite-horizon partially observable Markov decision processes (POMDPs), the introduction of algorithms for their (approximate) solution, and the application to a dynamic decision problem in clinical oncology.
Materials and methods
A dynamic limited-memory influence diagram for high-grade carcinoid tumor pathophysiology was developed in collaboration with an expert physician. Three algorithms, known as single policy updating, single rule updating, and simulated annealing have been examined for approximating the optimal treatment strategy from a space of 1019 possible strategies.
Results
Single policy updating proved intractable for finding a treatment strategy for carcinoid tumors. Single rule updating and simulated annealing both found the treatment strategy that is applied by physicians in practice.
Conclusions
Dynamic limited-memory influence diagrams are a suitable framework for the representation of factorized infinite-horizon POMDPs, and the developed algorithms find acceptable solutions under the assumption of limited memory about past observations. The framework allows for finding reasonable treatment strategies for complex dynamic decision problems in medicine.}
}
@article{BEALL2022101601,
title = {PermaBN: A Bayesian Network framework to help predict permafrost thaw in the Arctic},
journal = {Ecological Informatics},
volume = {69},
pages = {101601},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101601},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000504},
author = {Katherine Beall and Julie Loisel and Zenon Medina-Cetina},
keywords = {Bayesian modeling, Probabilistic modeling, Environmental modeling, Ecological modeling, Uncertainty, Expert assessment},
abstract = {Rising global temperatures are a threat to the current state of the Arctic. In particular, permafrost degradation has been impacting the terrestrial cryosphere in many ways, including effects on carbon cycling and the global climate, regional hydrological connectivity and ecosystem dynamics, as well as human health and infrastructure. However, the ability to simulate permafrost dynamics under future climate projections is limited, and model outputs are often associated with large uncertainties. A model structured on a Bayesian Network is presented to address existing limitations in the representation of physically complex processes and the limited availability of observational data. A strength of Bayesian methods over more traditional modeling methods is the ability to integrate various types of evidence (i.e., observations, model outputs, expert assessments) into a single model by mapping the evidence into probability distributions. Here, we outline PermaBN, a new modeling framework, to simulate permafrost thaw in the continuous permafrost region of the Arctic. Pre-validation and expert assessment validation results show that the model produces estimations of permafrost thaw depth that are consistent with current research, i.e., thaw depth increases during the snow-free season under initial conditions favoring warming temperatures, lowered soil moisture conditions, and low active layer ice content. Using a case study from northwestern Canada to evaluate PermaBN, we show that model performance is enhanced when certainty about the system components increases for known scenarios described by observations directly integrated into the model; in this case, insulation properties from vegetation were integrated to the model. Overall, PermaBN could provide informative predictions about permafrost dynamics without high computational cost and with the ability to integrate multiple types of evidence that traditional physics-based models sometimes do not account for, allowing PermaBN to be applied to carbon modeling studies, infrastructure hazard assessments, and policy decisions aimed at mitigation of, and adaptation to, permafrost degradation.}
}
@article{MAGNAN201763,
title = {Efficient incremental planning and learning with multi-valued decision diagrams},
journal = {Journal of Applied Logic},
volume = {22},
pages = {63-90},
year = {2017},
note = {SI:Uncertain Reasoning},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2016.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S157086831630091X},
author = {Jean-Christophe Magnan and Pierre-Henri Wuillemin},
abstract = {In the domain of decision theoretic planning, the factored framework (Factored Markov Decision Process, fmdp) has produced optimized algorithms using structured representations such as Decision Trees (Structured Value Iteration (svi), Structured Policy Iteration (spi)) or Algebraic Decision Diagrams (Stochastic Planning Using Decision Diagrams (spudd)). Since it may be difficult to elaborate the factored models used by these algorithms, the architecture sdyna, which combines learning and planning algorithms using structured representations, was introduced. However, the state-of-the-art algorithms for incremental learning, for structured decision theoretic planning or for reinforcement learning require the problem to be specified only with binary variables and/or use data structures that can be improved in term of compactness. In this paper, we propose to use Multi-Valued Decision Diagrams (mdds) as a more efficient data structure for the sdyna architecture and describe a planning algorithm and an incremental learning algorithm dedicated to this new structured representation. For both planning and learning algorithms, we experimentally show that they allow significant improvements in time, in compactness of the computed policy and of the learned model. We then analyzed the combination of these two algorithms in an efficient sdyna instance for simultaneous learning and planning using mdds.}
}
@article{BENJAMINFINK201727,
title = {A road map for developing and applying object-oriented bayesian networks to “WICKED” problems},
journal = {Ecological Modelling},
volume = {360},
pages = {27-44},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2017.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016307955},
author = {Nicole Benjamin-Fink and Brian K. Reilly},
keywords = {Applied wildlife management, Bayesian modelling, Ecological uncertainty, Ecosystem services, Object-Oriented bayesian network, “Wicked”, problems},
abstract = {Wildlife managers, conservation-based authorities and NGOS are often tasked to provide management guidelines concerning “wicked” problems and Ecosystem Services (ESS) with scarce data and limited resources. More often than not, this dilemma is further intensified by competing objectives and the need for economic profitability. A range of quantitative and qualitative models are often applied to model ecosystem services. However, lack of quantifiable data remains an obstacle. We provide a brief overview of Bayesian Networks (BNs), Object Oriented Bayesian Networks (OOBNs), Dynamic Bayesian Networks (DBNs), and Dynamic Object Oriented Bayesian Networks (DOOBNs) construction and a flow chart highlights their utility. OOBNs are a semi-quantitative modelling approach that uses Bayes probability theorem to represent ecological complexities and conservation concerns. Its transparency allows it to act as a conceptual system and a decision support tool. And yet, Bayesian networks are not commonly applied to the ecological discipline because they are computationally expensive. We promote the notion that sustainable management is advanced by providing biology-based decision-making networks to address wicked problems. First, we provide a brief reasoning for the applicability of Bayes theorem to wicked challenges. Then, we demonstrate the efficiency of Bayesian modelling by applying it to the case study of the black wildebeest (Connochaetes gnou) and blue wildebeest (Connochaetes taurinus) hybridization concern in South Africa. Empirical data, in addition to expert explicit understanding of uncertainties was utilized to infer the probability that specific ecological, biological, and economic parameters, and their relationships, facilitate hybridization. We found the following management variables to be key to impact the probability of hybridization: (i) blue wildebeest male to black wildebeest male ratio, (ii) land cover (tree savanna vs. grassland), and (iii) spatial connectivity (i.e., fences). By quantifying ecological uncertainty, OOBN illuminates decision-making tradeoffs and serves to promote informed decision making either during the risk assessment stage or the management impact analysis stage. We offer a SWOT analysis (Strength, weaknesses, opportunities, and threats) and put forth a step-by-step user friendly roadmap by which OOBNs may be applied to solve similar “wicked” problems. We identify three primary phases: phases: (1) a priori data generation, (2) model development, and (3) model validation and verification. Our suggested step-by-step framework of OOBN construction and validation is of global relevance; it is designed to model similar “wicked” problems worldwide.}
}
@article{CHAI2024128071,
title = {A substructure transfer reinforcement learning method based on metric learning},
journal = {Neurocomputing},
volume = {598},
pages = {128071},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128071},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008427},
author = {Peihua Chai and Bilian Chen and Yifeng Zeng and Shenbao Yu},
keywords = {Transfer learning, Reinforcement learning, Distance measure, Markov decision process},
abstract = {Transfer reinforcement learning has gained significant traction in recent years as a critical research area, focusing on bolstering agents’ decision-making prowess by harnessing insights from analogous tasks. The primary transfer learning method involves identifying the appropriate source domains, sharing specific knowledge structures and subsequently transferring the shared knowledge to novel tasks. However, existing transfer methods exhibit a pronounced dependency on high task similarity and an abundance of source data. Consequently, we attempt to formulate a more efficacious approach that optimally exploits the previous learning experiences to direct an agent’s exploration as it learns new tasks. Specifically, we introduce a novel transfer learning paradigm rooted within the distance measure in the Markov chain, denoted as Distance Measure Substructure Transfer Reinforcement Learning (DMS-TRL). The core idea involves partitioning the Markov chain into the most basic small Markov units, which contain basic information about the agent’s transfer between two states, and then followed by employing a new distance measure technique to find the most similar structure, which is also the most suitable for transfer. Finally, we propose a policy transfer method to transfer knowledge through the Q table from the selected Markov unit to the target task. Through a series of experiments conducted on discrete Gridworld scenarios, we compare our approach with state-of-the-art learning methods. The results clearly illustrate that DMS-TRL can adeptly identify optimal policy in target tasks, exhibiting swifter convergence.}
}
@article{CORSO2024102983,
title = {Sequentially optimized data acquisition for a geothermal reservoir},
journal = {Geothermics},
volume = {120},
pages = {102983},
year = {2024},
issn = {0375-6505},
doi = {https://doi.org/10.1016/j.geothermics.2024.102983},
url = {https://www.sciencedirect.com/science/article/pii/S0375650524000725},
author = {Anthony Corso and Maria Chiotoroiu and Torsten Clemens and Markus Zechner and Mykel J. Kochenderfer},
keywords = {Geothermal reservoirs, Data acquisition, POMDPs, Sequential decision making under uncertainty, Artificial intelligence},
abstract = {Given the high energy demands for heating and cooling, and the currently limited use of renewable sources, there is a pressing need for efficient and economically viable geothermal development. However, the success of new geothermal reservoir projects hinges on numerous uncertain geological and economic factors. Prior to development, the project uncertainty can be reduced by performing costly data acquisition campaigns. The central question in these campaigns is: which data should be acquired in which order to determine the viability of the proposed project? Traditional methods based on value of information and other heuristics are insufficient for assessing large-scale multi-well geothermal projects. Our approach reformulates geothermal assessment as a partially observable Markov decision process (POMDP), employing algorithmic decision-making techniques to devise an approximately optimal data acquisition strategy. Applied to a real-world low-enthalpy geothermal project in Austria, our methodology increased the expected net present value of the project by nearly 30% over baseline methods (including human experts) by minimizing the risk of developing a non-profitable project. This advancement not only promises a more efficient path towards large-scale geothermal energy production but also sets a precedent for applying sophisticated decision-making frameworks in renewable energy projects.}
}
@article{CORRALES2024108407,
title = {Colorectal cancer risk mapping through Bayesian networks},
journal = {Computer Methods and Programs in Biomedicine},
volume = {257},
pages = {108407},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108407},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724004000},
author = {D. Corrales and A. Santos-Lozano and S. López-Ortiz and A. Lucia and D. Ríos Insua},
keywords = {Colorectal cancer, Bayesian network, Risk mapping, Modifiable risk factors, Health policy},
abstract = {Background and Objective:
Only about 14% of eligible EU citizens finally participate in colorectal cancer (CRC) screening programs despite it being the third most common type of cancer worldwide. The development of CRC risk models can enable predictions to be embedded in decision-support tools facilitating CRC screening and treatment recommendations. This paper develops a predictive model that aids in characterizing CRC risk groups and assessing the influence of a variety of risk factors on the population.
Methods:
A CRC Bayesian Network is learnt by aggregating extensive expert knowledge and data from an observational study and making use of structure learning algorithms to model the relations between variables. The network is then parametrised to characterize these relations in terms of local probability distributions at each of the nodes. It is finally used to predict the risks of developing CRC together with the uncertainty around such predictions.
Results:
A graphical CRC risk mapping tool is developed from the model and used to segment the population into risk subgroups according to variables of interest. Furthermore, the network provides insights on the predictive influence of modifiable risk factors such as alcohol consumption and smoking, and medical conditions such as diabetes or hypertension linked to lifestyles that potentially have an impact on an increased risk of developing CRC.
Conclusion:
CRC is most commonly developed in older individuals. However, some modifiable behavioral factors seem to have a strong predictive influence on its potential risk of development. Modeling these effects facilitates identifying risk groups and targeting influential variables which are subsequently helpful in the design of screening and treatment programs.}
}
@article{DUTREIX2022101204,
title = {Abstraction-based synthesis for stochastic systems with omega-regular objectives},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {45},
pages = {101204},
year = {2022},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2022.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X22000322},
author = {Maxence Dutreix and Jeongmin Huh and Samuel Coogan},
keywords = {Finite-state abstractions, Formal methods, Interval-valued Markov chains, Bounded-parameter Markov decision processes, Stochastic systems},
abstract = {This paper studies the synthesis of controllers for discrete-time, continuous state stochastic systems subject to omega-regular specifications using finite-state abstractions. Omega-regular properties allow specifying complex behaviors and encompass, for example, linear temporal logic. First, we present a synthesis algorithm for minimizing or maximizing the probability that a discrete-time switched stochastic system with a finite number of modes satisfies an omega-regular property. Our approach relies on a finite-state abstraction of the underlying dynamics in the form of a Bounded-parameter Markov Decision Process arising from a finite partition of the system’s domain. Such Markovian abstractions allow for a range of probabilities of transition between states for each selected action representing a mode of the original system. Our method is built upon an analysis of the Cartesian product between the abstraction and a Deterministic Rabin Automaton encoding the specification of interest or its complement. Specifically, we show that synthesis can be decomposed into a qualitative problem, where the so-called greatest permanent winning components of the product automaton are created, and a quantitative problem, which requires maximizing the probability of reaching this component in the worst-case instantiation of the transition intervals. Additionally, we propose a quantitative metric for measuring the quality of the designed controller with respect to the continuous abstracted states and devise a specification-guided domain partition refinement heuristic with the objective of reaching a user-defined optimality target. Next, we present a method for computing control policies for stochastic systems with a continuous set of available inputs. In this case, the system is assumed to be affine in input and disturbance, and we derive a technique for solving the qualitative and quantitative problems in the resulting finite-state abstractions of such systems. For this, we introduce a new type of abstractions called Controlled Interval-valued Markov Chains. Specifically, we show that the greatest permanent winning component of such abstractions are found by appropriately partitioning the continuous input space in order to generate a bounded-parameter Markov decision process that accounts for all possible qualitative transitions between the finite set of states. Then, the problem of maximizing the probability of reaching these components is cast as a (possibly non-convex) optimization problem over the continuous set of available inputs. A metric of quality for the synthesized controller and a partition refinement scheme are described for this framework as well. Finally, we present a detailed case study.}
}
@article{HASANBEIG2023103949,
title = {Certified reinforcement learning with logic guidance},
journal = {Artificial Intelligence},
volume = {322},
pages = {103949},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103949},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000954},
author = {Hosein Hasanbeig and Daniel Kroening and Alessandro Abate},
keywords = {Reinforcement learning, Control synthesis, Policy synthesis, Formal methods, Temporal logics, Automata, Markov decision processes},
abstract = {Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Büchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.}
}
@article{GERSTENBERGER2015150,
title = {Bi-directional risk assessment in carbon capture and storage with Bayesian Networks},
journal = {International Journal of Greenhouse Gas Control},
volume = {35},
pages = {150-159},
year = {2015},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2015.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S1750583615000262},
author = {M.C. Gerstenberger and A. Christophersen and R. Buxton and A. Nicol},
keywords = {CCS, Bayesian Networks, Risk assessment},
abstract = {The complex system required for carbon capture and storage (CCS) encompasses numerous sub-systems with inter-dependencies and large parameter uncertainties that propagate throughout the system. Exploring and understanding these inter-dependencies and uncertainties is invaluable for developing robust risk information. Bayesian Networks (BN), a decision support tool, are being increasingly used in the broader risk assessment community and show promise for use in CCS. BNs explore the dependencies and uncertainties within a system and have the potential to provide a better understanding of risk than more traditional tools such as logic trees or other less integrated approaches. Working with experts from within the Cooperative Research Centre for Greenhouse Gas Technologies (CO2CRC), we have developed a generic BN structure for the storage sub-system of CCS which can be used to guide the development of BNs for other CCS applications and for use in both diagnostic and predictive analysis. This bi-directionality provides one of the more important benefits of BNs; it allows for (1) traditional bottom-up risk assessment where the likely consequences based on the expected state of the system can be calculated and also (2) top-down, or outcome oriented risk, where the state of the system leading to a particular outcome, such as the likelihood of 2% leakage in 1000 years, is determined. This allows for a comprehensive sensitivity analysis which highlights important contributors to the risk and also where additional knowledge may benefit the project and reduce uncertainty. A robust expert elicitation procedure, for both the development of the network structure and the determination of event probabilities, is an integral part of the use of any such BN tool in CCS. Finally, we show the direct application of a smaller CCS BN by the CO2CRC.}
}
@article{YET2014373,
title = {Combining data and meta-analysis to build Bayesian networks for clinical decision support},
journal = {Journal of Biomedical Informatics},
volume = {52},
pages = {373-385},
year = {2014},
note = {Special Section: Methods in Clinical Research Informatics},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2014.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S1532046414001816},
author = {Barbaros Yet and Zane B. Perkins and Todd E. Rasmussen and Nigel R.M. Tai and D. William R. Marsh},
keywords = {Clinical decision support, Bayesian networks, Evidence-based medicine, Evidence synthesis, Meta-analysis},
abstract = {Complex clinical decisions require the decision maker to evaluate multiple factors that may interact with each other. Many clinical studies, however, report ‘univariate’ relations between a single factor and outcome. Such univariate statistics are often insufficient to provide useful support for complex clinical decisions even when they are pooled using meta-analysis. More useful decision support could be provided by evidence-based models that take the interaction between factors into account. In this paper, we propose a method of integrating the univariate results of a meta-analysis with a clinical dataset and expert knowledge to construct multivariate Bayesian network (BN) models. The technique reduces the size of the dataset needed to learn the parameters of a model of a given complexity. Supplementing the data with the meta-analysis results avoids the need to either simplify the model – ignoring some complexities of the problem – or to gather more data. The method is illustrated by a clinical case study into the prediction of the viability of severely injured lower extremities. The case study illustrates the advantages of integrating combined evidence into BN development: the BN developed using our method outperformed four different data-driven structure learning methods, and a well-known scoring model (MESS) in this domain.}
}
@article{ANTONUCCI2008345,
title = {Decision-theoretic specification of credal networks: A unified language for uncertain modeling with sets of Bayesian networks},
journal = {International Journal of Approximate Reasoning},
volume = {49},
number = {2},
pages = {345-361},
year = {2008},
note = {Special Section on Probabilistic Rough Sets and Special Section on PGM’06},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2008.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X08000200},
author = {Alessandro Antonucci and Marco Zaffalon},
keywords = {Probabilistic graphical models, Bayesian networks, Credal networks, Credal sets, Imprecise probabilities, Conservative updating, Conservative inference rule},
abstract = {Credal networks are models that extend Bayesian nets to deal with imprecision in probability, and can actually be regarded as sets of Bayesian nets. Credal nets appear to be powerful means to represent and deal with many important and challenging problems in uncertain reasoning. We give examples to show that some of these problems can only be modeled by credal nets called non-separately specified. These, however, are still missing a graphical representation language and updating algorithms. The situation is quite the opposite with separately specified credal nets, which have been the subject of much study and algorithmic development. This paper gives two major contributions. First, it delivers a new graphical language to formulate any type of credal network, both separately and non-separately specified. Second, it shows that any non-separately specified net represented with the new language can be easily transformed into an equivalent separately specified net, defined over a larger domain. This result opens up a number of new outlooks and concrete outcomes: first of all, it immediately enables the existing algorithms for separately specified credal nets to be applied to non-separately specified ones. We explore this possibility for the 2U algorithm: an algorithm for exact updating of singly connected credal nets, which is extended by our results to a class of non-separately specified models. We also consider the problem of inference on Bayesian networks, when the reason that prevents some of the variables from being observed is unknown. The problem is first reformulated in the new graphical language, and then mapped into an equivalent problem on a separately specified net. This provides a first algorithmic approach to this kind of inference, which is also proved to be NP-hard by similar transformations based on our formalism.}
}
@article{REIS2019287,
title = {Robust topological policy iteration for infinite horizon bounded Markov Decision Processes},
journal = {International Journal of Approximate Reasoning},
volume = {105},
pages = {287-304},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304420},
author = {Willy Arthur Silva Reis and Leliane Nunes {de Barros} and Karina Valdivia Delgado},
keywords = {Probabilistic planning, Bounded-parameter Markov Decision Processes, Asynchronous policy iteration},
abstract = {Markov Decision Processes (mdps) are commonly used to solve sequential decision problems. A less restrictive model is the Bounded-parameter mdp (bmdp) that allows: (i) the transition function to be expressed in terms of probability intervals and (ii) reasoning about a robust solution, i.e., the best solution under the worst model. In this paper, we propose the Robust Topological Policy Iteration (rtpi) algorithm which is a new policy iteration algorithm for infinite horizon bmdps based on a partition of the state space. The empirical results show that the more structured the domain, the better is the performance of rtpi.}
}
@article{HU2013439,
title = {Software project risk analysis using Bayesian networks with causality constraints},
journal = {Decision Support Systems},
volume = {56},
pages = {439-449},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2012.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167923612003338},
author = {Yong Hu and Xiangzhou Zhang and E.W.T. Ngai and Ruichu Cai and Mei Liu},
keywords = {Software project risk analysis, Bayesian networks, Causality analysis, Knowledge discovery, Expert knowledge constraint},
abstract = {Many risks are involved in software development and risk management has become one of the key activities in software development. Bayesian networks (BNs) have been explored as a tool for various risk management practices, including the risk management of software development projects. However, much of the present research on software risk analysis focuses on finding the correlation between risk factors and project outcome. Software project failures are often a result of insufficient and ineffective risk management. To obtain proper and effective risk control, risk planning should be performed based on risk causality which can provide more risk information for decision making. In this study, we propose a model using BNs with causality constraints (BNCC) for risk analysis of software development projects. Through unrestricted automatic causality learning from 302 collected software project data, we demonstrated that the proposed model can not only discover causalities in accordance with the expert knowledge but also perform better in prediction than other algorithms, such as logistic regression, C4.5, Naïve Bayes, and general BNs. This research presents the first causal discovery framework for risk causality analysis of software projects and develops a model using BNCC for application in software project risk management.}
}
@article{STANIASZEK2025104926,
title = {Time-bounded planning with uncertain task duration distributions},
journal = {Robotics and Autonomous Systems},
volume = {186},
pages = {104926},
year = {2025},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2025.104926},
url = {https://www.sciencedirect.com/science/article/pii/S0921889025000120},
author = {Michal Staniaszek and Lara Brudermüller and Yang You and Raunak Bhattacharyya and Bruno Lacerda and Nick Hawes},
keywords = {Markov decision process, Mission planning, Temporal methods, Travelling salesperson problem},
abstract = {We consider planning problems where a robot must gather reward by completing tasks at each of a large set of locations while constrained by a time bound. Our focus is problems where the context under which each task will be executed can be predicted, but is not known in advance. Here, the term context refers to the conditions under which the task is executed, and can be related to the robot’s internal state (e.g., how well it is localised?), or the environment itself (e.g., how dirty is the floor the robot must clean?). This context has an impact on the time required to execute the task, which we model probabilistically. We model the problem of time-bounded planning for tasks executed under uncertain contexts as a Markov decision process with discrete time in the state, and propose variants on this model which allow adaptation to different robotics domains. Due to the intractability of the general model, we propose simplifications to allow planning in large domains. The key idea behind these simplifications is constraining navigation using a solution to the travelling salesperson problem. We evaluate our models on maps generated from real-world environments and consider two domains with different characteristics: UV disinfection, and cleaning. We evaluate the effect of model variants and simplifications on performance, and show that policies obtained for our models outperform a rule-based baseline, as well as a model which does not consider context. We also evaluate our models in a real robot experiment where a quadruped performs simulated inspection tasks in an industrial environment.}
}
@article{KAUSHIK2023113411,
title = {An integrated approach of intuitionistic fuzzy fault tree and Bayesian network analysis applicable to risk analysis of ship mooring operations},
journal = {Ocean Engineering},
volume = {269},
pages = {113411},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.113411},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822026944},
author = {Manvi Kaushik and Mohit Kumar},
keywords = {Ship mooring operation, Improved similarity aggregation method, Intuitionistic fuzzy fault tree analysis, Bayesian network, Importance ranking},
abstract = {Mooring is a technique to anchor the ship to a fixed or drifting component and make it associated while loading and unloading operations are in process. In the risk assessment of ship mooring operation, it is hard to get the precise failure data of system components. In this study, an integrated intuitionistic fuzzy fault tree and Bayesian network-based method is proposed for system failure probability evaluation in case of imprecise and insufficient failure data. Root causes of the ‘parted rope injury during ship mooring operation’ are obtained using fault tree analysis. The improved similarity aggregation method based intuitionistic fuzzy fault tree analysis is proposed to better deal with uncertainty. To update the failure probabilities for new evidences, the Bayesian network model is constructed. Importance ranking to the basic events of the system fault tree is given using the ‘Fussell–Vesely importance’ measure to identify the contribution of each basic event in system failure. The results demonstrate that the proposed approach is an alternative for probabilistic reliability evaluation when the components’ statistical failure data is not precisely available. This will help decision-makers and operators in taking decisions for preventive and corrective actions in the risk management process.}
}
@article{BELZNER2021102620,
title = {Synthesizing safe policies under probabilistic constraints with reinforcement learning and Bayesian model checking},
journal = {Science of Computer Programming},
volume = {206},
pages = {102620},
year = {2021},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102620},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321000137},
author = {Lenz Belzner and Martin Wirsing},
keywords = {Policy synthesis, Constrained Markov decision process, Safe reinforcement learning, Bayesian model checking, Probabilistic constraints},
abstract = {We propose to leverage epistemic uncertainty about constraint satisfaction of a reinforcement learner in safety critical domains. We introduce a framework for specification of requirements for reinforcement learners in constrained settings, including confidence about results. We show that an agent's confidence in constraint satisfaction provides a useful signal for balancing optimization and safety in the learning process.}
}
@article{LIU2025110675,
title = {Multiscenario deduction analysis for railway emergencies using knowledge metatheory and dynamic Bayesian networks},
journal = {Reliability Engineering & System Safety},
volume = {255},
pages = {110675},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2024.110675},
url = {https://www.sciencedirect.com/science/article/pii/S0951832024007464},
author = {Guanyi Liu and Shifeng Liu and Xuewei Li and Xueyan Li and Daqing Gong},
keywords = {Railway emergency, Scenario deduction, Knowledge metatheory, Dynamic Bayesian network},
abstract = {Railway emergencies exhibit uncertainty and complex evolutionary processes during their development. Scenario deduction analysis plays a critical role in identifying the progression of railway emergencies, which is essential for effective response. This paper adopts a “scenario‒response” decision-making approach and proposes a multiscenario deduction model based on knowledge metatheory and dynamic Bayesian networks. First, through an in-depth analysis of railway accident cases, a scenario knowledge metarepresentation model is constructed on the basis of knowledge metatheory. On this basis, a scenario deduction model based on dynamic Bayesian networks is constructed, which is capable of analyzing the evolutionary trajectories of various scenarios. Additionally, an evidence conflict calculation method based on the Tanimoto measure is proposed to reduce the subjectivity of expert evaluations. Finally, the empirical part of this study focuses on a case analysis of a train derailment accident, with the experimental results demonstrating the effectiveness of the proposed model. Furthermore, this study validates the feasibility and utility of the proposed methods, providing valuable insights and guidance for enhancing railway operational safety.}
}
@article{DESAI2023100617,
title = {Toward a better expert system for auditor going concern opinions using Bayesian network inflation factors},
journal = {International Journal of Accounting Information Systems},
volume = {49},
pages = {100617},
year = {2023},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2023.100617},
url = {https://www.sciencedirect.com/science/article/pii/S146708952300009X},
author = {Vikram Desai and Anthony C. Bucaro and Joung W. Kim and Rajendra Srivastava and Renu Desai},
keywords = {Going Concern Opinions, Bayesian network, Revised decisions, Design science, Expert systems, Model},
abstract = {We develop an analytical model intended as the first stage in the development of expert systems to improve auditor knowledge in, and assist in the decision process of, Going Concern Opinions (“GCOs”). Our approach is consistent with a design science approach to developing information systems, resulting in an initial artifact, the mathematical model, which can, through iterative design science and behavioral research, inform a technology-based expert system. Based on Bayesian networks, our model provides insights about auditors’ revision, or inflation, of the probability to issue a GCO based on the interrelationship that forms with the incremental existence of one, two, or three publicly observable financial statement risk factors – net operating loss, negative cash flows from operations, and negative working capital. We calculate the revised probabilities using empirical data of GCOs from 2004 to 2015. Results reveal that the incremental relationship (one, two, or three factors present) effectively models expert auditors’ decisions to issue a GCO, and suggests the existence of these measurable inflation factors that represent situational and auditor-specific factors. We also find that Non-Big Four auditors inflate these factors differently than Big Four auditors to arrive at a decision to issue a GCO.}
}
@article{LIN2024105193,
title = {Multi-status Bayesian network for analyzing collapse risk of excavation construction},
journal = {Automation in Construction},
volume = {158},
pages = {105193},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105193},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004533},
author = {Song-Shun Lin and Annan Zhou and Shui-Long Shen},
keywords = {Excavation engineering, Multi-status Bayesian network, Fuzzy set theory, Risk analysis, Decision- making},
abstract = {In response to the need for effective collapse risk analysis in engineering projects, a decision support approach is presented. It is rooted in multi-status Bayesian network (MSBN) and fuzzy set theory, encompassing MSBN construction, risk analysis, and management. This research addresses the causative correlation between influential factors and excavation collapse, considering the inherent uncertainty and fuzziness of the project. To enhance the reliability of evaluation results, an expert confidence index, integrating judgment ability, subjective reliability, and risk preference, is introduced. The approach is applied to an excavation case study, demonstrating its viability and capacity. Furthermore, it offers actionable insights for decision-makers to proactively mitigate accident probabilities. Notably, sensitivity analysis identifies critical risk factors. This research contributes to the field of engineering project risk assessment and management. It serves as a foundation for future research and development, guiding the way for improved strategies and decision support systems.}
}
@article{WILLIAMS201326,
title = {Mining monitored data for decision-making with a Bayesian network model},
journal = {Ecological Modelling},
volume = {249},
pages = {26-36},
year = {2013},
note = {Modelling for decision making in ecological systems},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S030438001200333X},
author = {B.J. Williams and B. Cole},
keywords = {Bayesian networks, Elicitation, Data mining, Water quality, Reservoir management, Cyanobacteria},
abstract = {A Bayesian network model of Anabaena blooms in Grahamstown Dam, near Newcastle, Australia is described. This model meets the criteria of being decision-focused, data driven, transparent, and capable of being used by non-expert modellers. Monitored data were arranged in a consistently formatted database from which the model could ‘learn’ probabilistic relationships between model elements such as pumped nutrient load, lake water column nutrient concentrations, and Anabaena concentrations. This ‘minimal model’ produced useful insights into ecosystem relationships and provided a basic model for later development. Subsequent modelling and elicitation of conditional probabilities from experts strengthened components of the model for which there is little data available. The approach to incorporating elicited data is described and some simple scenario testing is also presented. Management outcomes resulting from application of the model are presented.}
}
@article{PAPAKONSTANTINOU201493,
title = {Optimum inspection and maintenance policies for corroded structures using partially observable Markov decision processes and stochastic, physically based models},
journal = {Probabilistic Engineering Mechanics},
volume = {37},
pages = {93-108},
year = {2014},
issn = {0266-8920},
doi = {https://doi.org/10.1016/j.probengmech.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0266892014000423},
author = {K.G. Papakonstantinou and M. Shinozuka},
keywords = {Partially observable Markov decision processes, Stochastic control, Uncertain observations, Structural life-cycle cost, Inspection and maintenance policies, Spatial stochastic corrosion model},
abstract = {Stochastic control methods have a history of implementation in risk management and life-cycle cost procedures for civil engineering structures. The synergy of stochastic control methods and Bayesian principles can result in Partially Observable Markov Decision Processes (POMDPs) that allow consideration of uncertainty within the entire domain of the model and expand available policy options compared to other state-of-the art methods. The superior attributes of POMDPs enable optimum decisions which are based on the belief space or otherwise only on the best knowledge that a decision-maker can have at each time. In this work the effort is mostly based in modeling and solving the problem of finding optimal policies for the maintenance and management of aging structures through a POMDP framework with large state spaces that can adequately and sufficiently describe real-life problems. In order to form the POMDP framework, stochastic, physically based models can be used and their connection to the control process is explained in detail. Specific examples of a corroded existing structure are presented, based on non-stationary POMDPs, for both infinite and finite horizon cases with 332 and 14,009 states respectively. Results from both cases are compared and discussed and the capabilities of the method become apparent.}
}
@article{RIZZO2016408,
title = {Use of Bayesian Networks for Qualification Planning: Early Results of Factor Analysis},
journal = {Procedia Computer Science},
volume = {95},
pages = {408-417},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.354},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325273},
author = {Davinia B. Rizzo and Mark R. Blackburn},
keywords = {Bayesian network, qualification, vibration, systems engineering, 6DOF, multi-axis, decision model, factor analysis},
abstract = {This paper discusses the factor analysis that provides the basis for development and use of Bayesian Network (BN) models to support qualification planning in order to predict the suitability of Six Degrees of Freedom (6DOF) vibration testing for qualification. Qualification includes environmental testing such as temperature, vibration and shock to support a stochastic argument about the suitability of a design. Qualification is becoming more complex because it involves significant human expert judgment and relies on new technologies that have often never been fully utilized to support design assessment. Technology has advanced to the state where 6DOF vibration tests are possible, but these tests are far more complex than traditional single degree of freedom tests. This challenges systems engineers as they strive to plan qualification in an environment where technical and environmental constraints are coupled with the traditional costs, risk and schedule constraints. BN models may provide a framework to aid Systems Engineers in planning qualification efforts with complex constraints. Previous work identified a method for building a BN model for the predictive framework. This paper discusses validation efforts of models derived from the factor analysis and summarizes some recommendations on the factor analyses from industry subject matter experts.}
}
@article{BORSUK2004219,
title = {A Bayesian network of eutrophication models for synthesis, prediction, and uncertainty analysis},
journal = {Ecological Modelling},
volume = {173},
number = {2},
pages = {219-239},
year = {2004},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2003.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0304380003003958},
author = {Mark E Borsuk and Craig A Stow and Kenneth H Reckhow},
keywords = {Cross-scale modelling, Risk analysis, Decision support, Graphical models, Coastal eutrophication, Water quality, Bayesian statistics},
abstract = {A Bayesian network consists of a graphical structure and a probabilistic description of the relationships among variables in a system. The graphical structure explicitly represents cause-and-effect assumptions that allow a complex causal chain linking actions to outcomes to be factored into an articulated series of conditional relationships. Each of these relationships can then be independently quantified using a submodel suitable for the type and scale of information available. This approach is particularly useful for ecological modelling because predictable patterns may emerge at a variety of scales, necessitating a multiplicity of model forms. As an example, we describe a Bayesian network integrating models of the various processes involved in eutrophication in the Neuse River estuary, North Carolina. These models were developed using a range of methods, including: process-based models statistically fit to long-term monitoring data, Bayesian hierarchical modelling of cross-system data gathered from the literature, multivariate regression modelling of mesocosm experiments, and judgements elicited from scientific experts. The ability of the network to accommodate such a diversity of methods allowed for the prediction of policy-relevant ecosystem attributes not normally included in models of eutrophication. All of the submodels in the network include estimates of predictive uncertainty in the form of probability distributions which are propagated to model endpoints. Predictions expressed as probabilities give stakeholders and decision-makers a realistic appraisal of the chances of achieving desired outcomes under alternative nutrient management strategies. In general, the further down the causal chain a variable was, the greater the predictive uncertainty. This suggests that a compromise is necessary between policy relevance and predictive precision, and that, to select defensible environmental management strategies, public officials must adopt decision-making methods that deal explicitly with scientific uncertainty.}
}
@article{CHANG2016182,
title = {Sleeping experts and bandits approach to constrained Markov decision processes},
journal = {Automatica},
volume = {63},
pages = {182-186},
year = {2016},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2015.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S000510981500415X},
author = {Hyeong Soo Chang},
keywords = {Markov decision processes, Sleeping experts and bandits, Learning algorithm, Constrained optimization},
abstract = {This communique presents simple simulation-based algorithms for obtaining an approximately optimal policy in a given finite set in large finite constrained Markov decision processes. The algorithms are adapted from playing strategies for “sleeping experts and bandits” problem and their computational complexities are independent of state and action space sizes if the given policy set is relatively small. We establish convergence of their expected performances to the value of an optimal policy and convergence rates, and also almost-sure convergence to an optimal policy with an exponential rate for the algorithm adapted within the context of sleeping experts.}
}
@article{ARRUDA2015697,
title = {Solving average cost Markov decision processes by means of a two-phase time aggregation algorithm},
journal = {European Journal of Operational Research},
volume = {240},
number = {3},
pages = {697-705},
year = {2015},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714006584},
author = {E.F. Arruda and M.D. Fragoso},
keywords = {Dynamic programming, Markov decision processes, Embedding, Time aggregation, Stochastic optimal control},
abstract = {This paper introduces a two-phase approach to solve average cost Markov decision processes, which is based on state space embedding or time aggregation. In the first phase, time aggregation is applied for policy optimization in a prescribed subset of the state space, and a novel result is applied to expand the evaluation to the whole state space. This evaluation is then used in the second phase in a policy improvement step, and the two phases are then alternated until convergence is attained. Some numerical experiments illustrate the results.}
}
@article{MORI20161,
title = {Inference in hybrid Bayesian networks with large discrete and continuous domains},
journal = {Expert Systems with Applications},
volume = {49},
pages = {1-19},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S095741741500785X},
author = {Junichi Mori and Vladimir Mahalec},
keywords = {Large domain of discrete and continuous variables, Decision tree, Hybrid Bayesian network, Belief propagation algorithm, Context-specific independence},
abstract = {Inference in Bayesian networks with large domain of discrete variables requires significant computational effort. In order to reduce the computational effort, current approaches often assume that discrete variables have some bounded number of values or are represented at an appropriate size of clusters. In this paper, we introduce decision-tree structured conditional probability representations that can efficiently handle a large domain of discrete and continuous variables. These representations can partition the large number of values into some reasonable number of clusters and lead to more robust parameter estimation. Very rapid computation and ability to treat both discrete and continuous variables are accomplished via modified belief propagation algorithm. Being able to compute various types of reasoning from a single Bayesian network eliminates development and maintenance issues associated with the use of distinct models for different types of reasoning. Application to real-world steel production process data is presented.}
}
@article{MAYER2023126387,
title = {Mapping human- and bear-centered perspectives on coexistence using a participatory Bayesian framework},
journal = {Journal for Nature Conservation},
volume = {73},
pages = {126387},
year = {2023},
issn = {1617-1381},
doi = {https://doi.org/10.1016/j.jnc.2023.126387},
url = {https://www.sciencedirect.com/science/article/pii/S1617138123000584},
author = {Paula Mayer and Adrienne Grêt-Regamey and Paolo Ciucci and Nicolas Salliou and Ana Stritih},
keywords = {Large carnivores, Human-wildlife coexistence, Social-ecological modeling, Participation, Bayesian network, Apennine brown bear},
abstract = {Coexistence with wildlife is becoming a key challenge in Europe as populations of large carnivores recover in human-dominated landscapes. Modeling the spatial distribution of conditions for human-bear coexistence can help support conservation by identifying priority areas and measures to support coexistence, but existing models often only address risks either to humans or to large carnivores. In this study, we developed a participatory modeling process that incorporates both human-centered and large carnivore-centered perspectives on coexistence and applied it to a case study of coexistence between humans and the endangered Apennine brown bears (Ursus arctos marsicanus) in Italy. Local and expert knowledge, as well as available data on bear habitats and land use, were integrated into a spatially explicit Bayesian network. This model is used to predict and map the tolerance to bears from the human perspective and the risk of fitness loss from the bear perspective. We found that conditions for human-bear coexistence vary between human communities and are spatially heterogeneous at the local scale, depending on ecological factors, social factors influencing the level of tolerance in community, such as people’s emotions and knowledge, economic factors, such as livelihoods, and policies such as damage compensation. The participatory modeling approach allowed us to integrate perceptions of local people, expert assessments, and spatial data, and can help bridge the gap between science and conservation practice. The resulting coexistence maps can inform conservation decisions, and can be updated as new information becomes available. Our modeling approach could help to efficiently target measures for improving human-large carnivore coexistence in different settings in a site-specific manner.}
}
@article{DICINTIO2024102879,
title = {Socio-economic factors boosting the effectiveness of marine protected areas: A Bayesian network analysis},
journal = {Ecological Informatics},
volume = {84},
pages = {102879},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102879},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124004217},
author = {Antonio {Di Cintio} and Jose Antonio Fernandes-Salvador and Riikka Puntila-Dodd and Igor Granado and Federico Niccolini and Fabio Bulleri},
keywords = {EU biodiversity strategy, Nature-based solutions, Stakeholder engagement, Scenarios},
abstract = {Marine protected areas (MPAs) represent an example of nature-based solutions for the conservation and sustainable management of marine biodiversity. Despite the number of MPAs growing worldwide, many of them fail to achieve their goals, sometimes up to the point of becoming the so-called “paper parks”: protected areas without real protection or enforcement that are virtually non-existent in terms of their effectiveness in achieving the ecological and socioeconomic goals for which they have been set up. Following the Kunming–Montreal Biodiversity Agreement (COP 15), the EU Biodiversity Strategy for 2030, and the Biodiversity Beyond National Jurisdiction treaty, global MPA coverage should increase substantially in the coming years. Hence, identifying the factors that contribute to raising the effectiveness of MPAs is pivotal to informing their planning and management. Our study introduces a model based on the Bayesian network that allows testing how different socioeconomic factors (e.g., stakeholder involvement, increased communication and enforcement) can impact the effectiveness of MPAs. The system is a user-friendly decision-support tool to quantify the contribution of each factor in the creation of a successful MPA, thus narrowing the gap between science and decision-making. We modelled the evolution of the effectiveness of MPAs under three contrasting policy-relevant scenarios based on the Intergovernmental Panel on Climate Change frameworks. Our results indicate that the highest and lowest the effectiveness of MPAs is achieved under the “global sustainability” and “national enterprise” scenarios, respectively. Our work sheds light on the complexity of the interactions among the different factors underpinning the effectiveness of MPAs and supports the growth process of MPAs at the global level on the pathway towards the sustainable exploitation of marine living resources.}
}
@article{DELGADO2016192,
title = {Real-time dynamic programming for Markov decision processes with imprecise probabilities},
journal = {Artificial Intelligence},
volume = {230},
pages = {192-223},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001411},
author = {Karina V. Delgado and Leliane N. {de Barros} and Daniel B. Dias and Scott Sanner},
keywords = {Probabilistic planning, Markov decision process, Robust planning},
abstract = {Markov Decision Processes have become the standard model for probabilistic planning. However, when applied to many practical problems, the estimates of transition probabilities are inaccurate. This may be due to conflicting elicitations from experts or insufficient state transition information. The Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) was introduced to obtain a robust policy where there is uncertainty in the transition. Although it has been proposed a symbolic dynamic programming algorithm for MDP-IPs (called SPUDD-IP) that can solve problems up to 22 state variables, in practice, solving MDP-IP problems is time-consuming. In this paper we propose efficient algorithms for a more general class of MDP-IPs, called Stochastic Shortest Path MDP-IPs (SSP MDP-IPs) that use initial state information to solve complex problems by focusing on reachable states. The (L)RTDP-IP algorithm, a (Labeled) Real Time Dynamic Programming algorithm for SSP MDP-IPs, is proposed together with three different methods for sampling the next state. It is shown here that the convergence of (L)RTDP-IP can be obtained by using any of these three methods, although the Bellman backups for this class of problems prescribe a minimax optimization. As far as we are aware, this is the first asynchronous algorithm for SSP MDP-IPs given in terms of a general set of probability constraints that requires non-linear optimization over imprecise probabilities in the Bellman backup. Our results show up to three orders of magnitude speedup for (L)RTDP-IP when compared with the SPUDD-IP algorithm.}
}
@article{ASSAF2023104756,
title = {Predicting Urban Heat Island severity on the census-tract level using Bayesian networks},
journal = {Sustainable Cities and Society},
volume = {97},
pages = {104756},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104756},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723003670},
author = {Ghiwa Assaf and Xi Hu and Rayan H. Assaad},
keywords = {UHI severity, Urban climate, Bayesian networks, White-box machine learning, Forecasting models, Census-tract predictions},
abstract = {Urban development and population growth have resulted in several critical impacts on the society, environment, and economy. One of the main impacts is the increase in temperature observed in urban areas, also known as the Urban Heat Island (UHI) effect. UHI has become the focus of several research studies due to the associated negative implications it causes. Despite that some research efforts examined different characteristics of the UHI phenomenon, there is a gap in the literature in terms of developing interpretable machine learning models that can accurately estimate or predict the severity of UHI (rather than air temperature or other UHI characteristics) as well as that can provide predictions on the census-tract level in the US. To that extent, this paper fills this gap by developing a knowledge-based white-box Bayesian network model that predicts UHI severity based on demographic, meteorological, and land use/land cover factors. First, a dataset for all census tracts in the state of New Jersey, USA was developed, which is comprised of 13 independent variables or factors affecting UHI severity. Second, expert knowledge was obtained from 10 UHI experts using the systematic three-round Delphi method to develop four different Bayesian network models. Third, the performance of the four Bayesian models was assessed and compared to choose the optimal model with the highest accuracy. Finally, sensitivity analysis was conducted to assess the influence of each key factor on the UHI severity. The results showed that the optimal model is a tree-augmented Bayesian network that can predict or estimate the UHI severity with an accuracy of 87.88%. The outcomes of this paper also reflected that the following 8 variables are the key factors that impact UHI severity: NDVI during winter season, NDVI during summer season, imperviousness, tree canopy, building area, population density, water body areas, and annual rainfall. The findings also identified the land use/land cover category as the major category affecting UHI severity compared to demographic- and meteorological-related factors. The proposed white-box Bayesian network model in this paper adds to the body of knowledge by allowing practitioners and researchers to perform micro-level UHI predictions and inferences about future or unknown UHI severity pertaining to individual communities or small/localized geographical areas. This enables more focused and targeted UHI mitigation actions to be planned, designed, and implemented in the most affected communities, which ultimately results in better decisions, actions, and outcomes to reduce or address the UHI effect.}
}
@article{MAYFIELD202311,
title = {Designing an expert-led Bayesian network to understand interactions between policy instruments for adoption of eco-friendly farming practices},
journal = {Environmental Science & Policy},
volume = {141},
pages = {11-22},
year = {2023},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122003872},
author = {Helen J. Mayfield and Rachel Eberhard and Christopher Baker and Umberto Baresi and Michael Bode and Anthea Coggan and Angela J. Dean and Felicity Deane and Evan Hamman and Diane Jarvis and Barton Loechel and Bruce M. Taylor and Lillian Stevens and Karen Vella and Kate J. Helmstedt},
keywords = {Environmental policy, Bayesian networks, Great Barrier Reef, Agricultural practices, Socio-ecological systems, Participatory modelling, Policy instruments, Adoption},
abstract = {Governments employ a range of policy instruments to encourage landholders to adopt land management practices that reduce the environmental impacts of agriculture. While the impact of policy instruments may be well-theorised, their implementation in diverse contexts and landholders’ complex behavioural responses, makes measurement and prediction of the resulting adoption rates difficult. This constrains the ability of governments to select the optimal combination of policy instruments. We used a participatory modelling approach to incorporate expert knowledge into a Bayesian network model exploring the effect of different policy combinations on the adoption of sustainable farming practices in the Great Barrier Reef catchment, Australia. The model integrates policy instruments including regulating farming practices, offering financial incentives, and facilitating extension programs to educate and assist farmers. Results showed that the effectiveness of a policy instrument on practice adoption was expected to vary depending on which other instruments are implemented, the characteristics of the land managers, the surrounding social context, and the practice itself. This approach demonstrates the utility of Bayesian networks in integrating high-level multi-disciplinary knowledge to address complex environmental policy decisions such as water quality management in the Great Barrier Reef.}
}
@article{GAMEZ2002261,
title = {Searching for the best elimination sequence in Bayesian networks by using ant colony optimization},
journal = {Pattern Recognition Letters},
volume = {23},
number = {1},
pages = {261-277},
year = {2002},
issn = {0167-8655},
doi = {https://doi.org/10.1016/S0167-8655(01)00123-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167865501001234},
author = {José A. Gámez and José M. Puerta},
keywords = {Ant colony optimization, Bayesian (belief) networks, Probabilistic expert systems, Graph triangulation, NP-hard problems},
abstract = {The knowledge base of a probabilistic expert system is usually represented as a Bayesian network. Most of the knowledge engineering tools used in the development of probabilistic expert systems do not carry out the inference process directly over the network, but in a secondary graphical structure called a junction tree. The efficiency of inference (propagation) algorithms depends on the size of the junction tree obtained, and this size depends on the elimination sequence used during the compilation/transformation of the Bayesian network into a junction tree. The problem of searching for the best elimination sequence is an NP-hard problem [W. Wen, in: P. Bonissone, M. Henrion, L. Kanal and Z. Lemmer (Eds.), Uncertainty in Artificial Intelligence, vol. 6, North-Holland, Amsterdam, 1991, pp. 209–224], and this has motivated the proliferation of approximate methods to approach it (based variously on greedy heuristics, genetic algorithms, simulated annealing, etc.). In this paper we investigate the applicability to this problem of a new combinatorial optimization technique, inspired by a natural model, which has appeared recently: ant colony optimization [M. Dorigo, Optimization, learning and natural algorithms, Ph.D. thesis, Politecnico di Milano, Italy, 1992; M. Dorigo and L. Gambardella, IEEE Trans. Evol. Comput. 1 (1997) 53; M. Dorigo and G.Di. Caru, in: New Ideas in Optimization, McGraw-Hill, New York, 1999]. Our approach is validated by using a set of complex networks obtained from a repository.}
}
@article{WU2020101895,
title = {Predicting the causative pathogen among children with osteomyelitis using Bayesian networks – improving antibiotic selection in clinical practice},
journal = {Artificial Intelligence in Medicine},
volume = {107},
pages = {101895},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101895},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719307213},
author = {Yue Wu and Charlie McLeod and Christopher Blyth and Asha Bowen and Andrew Martin and Ann Nicholson and Steven Mascaro and Tom Snelling},
keywords = {Bone infection, Infectious disease, Bayesian belief network, Clinical decision support, Causal diagram, Probabilistic graph model},
abstract = {Infection of bone, osteomyelitis (OM), is a serious bacterial infection in children requiring urgent antibiotic therapy. While biological specimens are often obtained and cultured to guide antibiotic selection, culture results may take several days, are often falsely negative, and may be falsely positive because of contamination by non-causative bacteria. This poses a dilemma for clinicians when choosing the most suitable antibiotic. Selecting an antibiotic which is too narrow in spectrum risks treatment failure; selecting an antibiotic which is too broad risks toxicity and promotes antibiotic resistance. We have developed a Bayesian Network (BN) model that can be used to guide individually targeted antibiotic therapy at point-of-care, by predicting the most likely causative pathogen in children with OM and the antibiotic with optimal expected utility. The BN explicitly models the complex relationship between the unobserved infecting pathogen, observed culture results, and clinical and demographic variables, and integrates data with critical expert knowledge under a causal inference framework. Development of this tool resulted from a multidisciplinary approach, involving experts in infectious diseases, modelling, paediatrics, microbiology, computer science and statistics. The model-predicted prevalence of causative pathogens among children with osteomyelitis were 56 % for Staphylococcus aureus, 17 % for ‘other’ culturable bacteria (like Streptococcus pyogenes), and 27 % for bacterial pathogens that are not culturable using routine methods (like Kingella kingae). Log loss cross-validation suggests that the model performance is robust, with the best fit to culture results achieved when data and expert knowledge were combined during parameterisation. AUC values of 0.68 – 0.77 were achieved for predicting culture results of different types of specimens. BN-recommended antibiotics were rated optimal or adequate by experts in 82–98% of 81 cases sampled from the cohort. We have demonstrated the potential use of BNs in improving antibiotic selection for children with OM, which we believe to be generalisable in the development of a broader range of decision support tools. With appropriate validation, such tools might be effectively deployed for real-time clinical decision support, to promote a shift in clinical practice from generic to individually-targeted antibiotic therapy, and ultimately improve the management and outcomes for a range of serious bacterial infections.}
}
@article{KAMIL2021472,
title = {Data-driven operational failure likelihood model for microbiologically influenced corrosion},
journal = {Process Safety and Environmental Protection},
volume = {153},
pages = {472-485},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.07.040},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021004110},
author = {Mohammad Zaid Kamil and Mohammed Taleb-Berrouane and Faisal Khan and Paul Amyotte},
keywords = {Corrosion, Microbiologically influenced corrosion (MIC), Learning-based bayesian network (LBN), Bayesian learning, Floating, Production, Storage and offloading (FPSO)},
abstract = {Corrosion is a threat to asset integrity, with engineering challenges and economic burdens. Since the last decade, microbiologically influenced corrosion (MIC) began to be recognized among corrosion professionals as a severe corrosion form. It is challenging to detect and predict MIC due to the complex behaviour of microorganisms. The current MIC risk assessment models define the dependencies of parameters with their synergic interactions. A data-driven approach is needed to utilize available operational and microbiological data and learn as the data changes. The model proposed in this study is used to strengthen the variables' correlation and their features to assess MIC likelihood. It can integrate available field and laboratory data into a Learning-based Bayesian network (LBN) model. The model minimizes current research gap and has the advantage of adapting to changes in process operation. It is based on an advanced Bayesian learning algorithm, which develops topology of the Bayesian network (BN) from the input data and its parameters. This paper focuses on the development of the LBN model that utilizes available MIC data for likelihood estimation. The model is benchmarked and validated using data reported in the public domain. The application of the model is demonstrated on the processing facility on a Floating, Production, Storage and Offloading (FPSO). The topology and parameter estimation will update as data changes/improve to capture the system behaviour to assess MIC likelihood, which helps in decision-making to control and mitigate MIC threats.}
}
@article{KATIC201644,
title = {Targeting investments in small-scale groundwater irrigation using Bayesian networks for a data-scarce river basin in Sub-Saharan Africa},
journal = {Environmental Modelling & Software},
volume = {82},
pages = {44-72},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216300937},
author = {Pamela Katic and Joanne Morris},
keywords = {Smallholders, Irrigation technologies, Outscaling, Decision-support},
abstract = {Irrigation for smallholder farming systems is an important approach for sustainable intensification and increased productivity in Sub-Saharan Africa, provided investments in irrigation are properly targeted and accompanied by complementary improvements. Many GIS-based tools have been developed to identify suitable areas for investments in different types of small scale irrigation (SSI), but they do not explicitly address uncertainty on the data input and on the determination of factors that affect success of an investment in a given context. This paper addresses this problem by presenting an application of a decision-support targeting tool based on Bayesian networks (BNs) that can be used by non-expert policy-makers and investors to assess the potential success of specific technologies used for groundwater-based SSI. A case study application for the White Volta Basin in West Africa is presented to illustrate the BN approach.}
}
@article{GUO2023103674,
title = {Fuzzy dynamic Bayesian network based on a discrete aggregation method for risk assessment of marine nuclear power platform hinge joints accidents},
journal = {Applied Ocean Research},
volume = {138},
pages = {103674},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103674},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002158},
author = {Chongchong Guo and Wenhua Wu},
keywords = {Marine nuclear power platforms (MNPP), Risk assessment, Hinge joints failure, Discrete aggregation method (DAM), Fuzzy set theory (FST), Bow-tie model (BT), Dynamic Bayesian networks (DBNs)},
abstract = {Hinge joints play a critical role in the positioning system of Marine Nuclear Power Platforms (MNPP) by facilitating the transmission of mooring forces and accommodating rotational motions of the hull. Failure of hinge joints during operation can result in severe consequences for both the environment and human safety. Therefore, this study aims to address the limitations associated with the dynamic characteristics and uncertain information pertaining to the risk systems of hinge joint structures in MNPP. To tackle the challenge of conflicting expert opinions in group decision-making, a novel method called the discrete aggregation method (DAM) is proposed. DAM utilizes fuzzy opinion aggregation, taking into account the degree of consensus among experts. This approach effectively overcomes the problem of conflicting opinions. Furthermore, building a hinge joint accidents model using Bow-tie (BT) for hazard identification, and then mapping the BT model to a dynamic Bayesian networks (DBN) model. Fuzzy set theory (FST) and DBN are employed to handle cognitive uncertainty caused by data limitations and incomplete knowledge. These techniques enable causal inference for time-varying dynamic risk systems. To validate the proposed method, a case study of the positioning system of the first MNPP in China is conducted. The dynamic fatigue failure probability of hinge joints is calculated using predictive technology based on Bayesian theory. Additionally, diagnostic analysis techniques are employed to accurately identify key events and their underlying causes. The analysis results provide valuable data support for on-site operations and management personnel, aiding in the development of effective risk mitigation measures.}
}
@article{MIHALJEVIC2021648,
title = {Bayesian networks for interpretable machine learning and optimization},
journal = {Neurocomputing},
volume = {456},
pages = {648-665},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.138},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221009644},
author = {Bojan Mihaljević and Concha Bielza and Pedro Larrañaga},
keywords = {Interpretability, Explainable machine learning, Probabilistic graphical models},
abstract = {As artificial intelligence is being increasingly used for high-stakes applications, it is becoming more and more important that the models used be interpretable. Bayesian networks offer a paradigm for interpretable artificial intelligence that is based on probability theory. They provide a semantics that enables a compact, declarative representation of a joint probability distribution over the variables of a domain by leveraging the conditional independencies among them. The representation consists of a directed acyclic graph that encodes the conditional independencies among the variables and a set of parameters that encodes conditional distributions. This representation has provided a basis for the development of algorithms for probabilistic reasoning (inference) and for learning probability distributions from data. Bayesian networks are used for a wide range of tasks in machine learning, including clustering, supervised classification, multi-dimensional supervised classification, anomaly detection, and temporal modeling. They also provide a basis for estimation of distribution algorithms, a class of evolutionary algorithms for heuristic optimization. We illustrate the use of Bayesian networks for interpretable machine learning and optimization by presenting applications in neuroscience, the industry, and bioinformatics, covering a wide range of machine learning and optimization tasks.}
}
@article{HANNINEN20147837,
title = {Bayesian network model of maritime safety management},
journal = {Expert Systems with Applications},
volume = {41},
number = {17},
pages = {7837-7846},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0957417414003704},
author = {Maria Hänninen and Osiris A. {Valdez Banda} and Pentti Kujala},
keywords = {Safety management, The ISM Code, Bayesian networks, Safety indicators, Maritime traffic safety, Expert elicitation},
abstract = {This paper presents a model of maritime safety management and its subareas. Furthermore, the paper links the safety management to the maritime traffic safety indicated by accident involvement, incidents reported by Vessel Traffic Service and the results from Port State Control inspections. Bayesian belief networks are applied as the modeling technique and the model parameters are based on expert elicitation and learning from historical data. The results from this new application domain of a Bayesian network based expert system suggest that, although several its subareas are functioning properly, the current status of the safety management on vessels navigating in the Finnish waters has room for improvement; the probability of zero poor safety management subareas is only 0.13. Furthermore, according to the model a good IT system for the safety management is the strongest safety-management related signal of an adequate overall safety management level. If no deficiencies have been discovered during a Port State Control inspection, the adequacy of the safety management is almost twice as probable as without knowledge on the inspection history. The resulted model could be applied to performing several safety management related queries and it thus provides support for maritime safety related decision making.}
}
@article{WANG2020105638,
title = {FMDBN: A first-order Markov dynamic Bayesian network classifier with continuous attributes},
journal = {Knowledge-Based Systems},
volume = {195},
pages = {105638},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105638},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120300940},
author = {Shuangcheng Wang and Siwen Zhang and Tao Wu and Yongrui Duan and Liang Zhou and Hao Lei},
keywords = {Time-series data, Dynamic Bayesian network classifier, Time-delayed transformation, Dislocated transformation, Evolutionary learning},
abstract = {With the development of data driven decision making and prediction, time-series data are ubiquitous and the demand for its classification is vast. Although a large body of research has been reported in the literature, it is mainly oriented to situations in which class and attributes are changing simultaneously. In practice however, those class and attributes changes are not always synchronous. This means that further studies for asynchronous classifier problems are necessary. In this paper, a first-order Markov dynamic Bayesian network classifier is proposed to address the asynchronous issue, by combing time-series data preprocessing, time-delayed and dislocated transformation of variables, initial and evolutionary learning. The attribute density in this classifier is estimated based on Gaussian function, and the classification accuracy criterion for time-series progressiveness is also considered. This classifier has a relatively simple structure, which can avoid the problem of overfitting. In addition, data can effectively be classified by utilizing three kinds of classification information, namely time-delayed, non-time-delayed and mixed information in multivariate time-series datasets. The proposed method is also able to accumulate classification information via iterative evolution and thus improve the generalization of classifiers. Experiments were carried out by using standard time-series datasets from UCI, financial and macroeconomic domains. The experimental results show that the proposed first-order Markov dynamic Bayesian network classifier is more accurate in dealing with these dynamic classification problems.}
}
@article{HUANG2022,
title = {A Bayesian Network to Predict the Risk of Post Influenza Vaccination Guillain-Barré Syndrome: Development and Validation Study},
journal = {JMIR Public Health and Surveillance},
volume = {8},
number = {3},
year = {2022},
issn = {2369-2960},
doi = {https://doi.org/10.2196/25658},
url = {https://www.sciencedirect.com/science/article/pii/S2369296022001090},
author = {Yun Huang and Chongliang Luo and Ying Jiang and Jingcheng Du and Cui Tao and Yong Chen and Yuantao Hao},
keywords = {adverse events, Bayesian network, Guillain-Barré syndrome, risk prediction, trivalent influenza vaccine},
abstract = {Background
Identifying the key factors of Guillain-Barré syndrome (GBS) and predicting its occurrence are vital for improving the prognosis of patients with GBS. However, there are scarcely any publications on a forewarning model of GBS. A Bayesian network (BN) model, which is known to be an accurate, interpretable, and interaction-sensitive graph model in many similar domains, is worth trying in GBS risk prediction.
Objective
The aim of this study is to determine the most significant factors of GBS and further develop and validate a BN model for predicting GBS risk.
Methods
Large-scale influenza vaccine postmarketing surveillance data, including 79,165 US (obtained from the Vaccine Adverse Event Reporting System between 1990 and 2017) and 12,495 European (obtained from the EudraVigilance system between 2003 and 2016) adverse events (AEs) reports, were extracted for model development and validation. GBS, age, gender, and the top 50 prevalent AEs were included for initial BN construction using the R package bnlearn.
Results
Age, gender, and 10 AEs were identified as the most significant factors of GBS. The posttest probability of GBS suggested that male vaccinees aged 50-64 years and without erythema should be on the alert or be warned by clinicians about an increased risk of GBS, especially when they also experience symptoms of asthenia, hypesthesia, muscular weakness, or paresthesia. The established BN model achieved an area under the receiver operating characteristic curve of 0.866 (95% CI 0.865-0.867), sensitivity of 0.752 (95% CI 0.749-0.756), specificity of 0.882 (95% CI 0.879-0.885), and accuracy of 0.882 (95% CI 0.879-0.884) for predicting GBS risk during the internal validation and obtained values of 0.829, 0.673, 0.854, and 0.843 for area under the receiver operating characteristic curve, sensitivity, specificity, and accuracy, respectively, during the external validation.
Conclusions
The findings of this study illustrated that a BN model can effectively identify the most significant factors of GBS, improve understanding of the complex interactions among different postvaccination symptoms through its graphical representation, and accurately predict the risk of GBS. The established BN model could further assist clinical decision-making by providing an estimated risk of GBS for a specific vaccinee or be developed into an open-access platform for vaccinees’ self-monitoring.}
}
@article{KWAG2018380,
title = {Probabilistic risk assessment based model validation method using Bayesian network},
journal = {Reliability Engineering & System Safety},
volume = {169},
pages = {380-393},
year = {2018},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2017.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0951832016305907},
author = {Shinyoung Kwag and Abhinav Gupta and Nam Dinh},
abstract = {Past few decades have seen a rapid growth in the availability of computational power and that induces continually reducing cost of simulation. This rapidly changing scenario together with availability of high precision and large-scale experimental data has enabled development of high fidelity simulation tools capable of simulating multi-physics multi-scale phenomena. At the same time, there has been an increased emphasis on developing strategies for verification and validation of such high fidelity simulation tools. The problem is more pronounced in cases where it is not possible to collect experimental data or field measurements on a large-scale or full scale system performance. This is particularly true in case of systems such as nuclear power plants subjected to external hazards such as earthquakes or flooding. In such cases, engineers rely solely on simulation tools but struggle to establish the credibility of the system level simulations. In practice, validation approaches rely heavily on expert elicitation. There is an increasing need of a quantitative approach for validation of high fidelity simulations that is comprehensive, consistent, and effective. A validation approach should be able to consider uncertainties due to incomplete knowledge and randomness in the system's performance as well as in the characterization of external hazard. A new approach to validation is presented in this paper that uses a probabilistic index as a degree of validation and propagates it through the system using the performance-based probabilistic risk assessment (PRA) framework. Unlike traditional PRA approaches, it utilizes the power of Bayesian statistic to account for non-Boolean relationships and correlations among events at various levels of a network representation of the system. Bayesian updating facilitates evaluation of updated validation information as additional data from experimental observations or improved simulations is incorporated. PRA based framework assists in identifying risk-consistent events and critical path for appropriate allocation of resources to improve the validation.}
}
@article{YUAN2021104570,
title = {Evaluation on consequences prediction of fire accident in emergency processes for oil-gas storage and transportation by scenario deduction},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {72},
pages = {104570},
year = {2021},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104570},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021001807},
author = {Changfeng Yuan and Yichao Hu and Yulong Zhang and Tao Zuo and Jiahui Wang and Shuangjiao Fan},
keywords = {Oil-gas storage and transportation, Fire accident, Scenario deduction, Consequence prediction, Dynamic bayesian network},
abstract = {This paper takes the safety in emergency processes as the starting point, from the perspective of scenario deduction, to study the consequences of fire accidents for oil-gas storage and transportation. Through the statistical analysis of actual accident cases, 19 frequently occurring basic scenarios in emergency processes are summarized. The scenario evolution paths of fire accidents for oil-gas storage and transportation are given by analyzing the evolution law of the accident development. Fuzzy numbers are introduced to express experts' qualitative judgment on accident scenarios. The empirical probabilities of scenario nodes are obtained by defuzzification calculation, and the state probability of each scenario node is calculated by using the dynamic Bayesian network joint probability formula. Under the comprehensive consideration about the probability statistics of actual accident cases, the critical scenario nodes on the evolution path and their final scenario probabilities are jointly determined to realize the optimization of the scenario evolution path. By constructing the correlation between the optimized scenario evolution path and the accident consequences, an accident consequence prediction model is established. The occurrence probability of accident consequences is calculated by the defuzzification method and dynamic Bayesian network. The accuracy of the consequence prediction model is verified by the July 16 Dalian's Xingang Harbor oil pipeline explosion accident. The research results provide scientific basis for helping decision makers to make the effective emergency measures that are most conducive to the rapid elimination of accidents and reducing the severity of accident consequences.}
}
@article{CHOJNACKI201928,
title = {An expert system based on a Bayesian network for fire safety analysis in nuclear area},
journal = {Fire Safety Journal},
volume = {105},
pages = {28-40},
year = {2019},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2019.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0379711218304697},
author = {E. Chojnacki and W. Plumecocq and L. Audouin},
keywords = {Bayesian network, Expert system, Pressure effects},
abstract = {For fire safety studies in nuclear installations, IRSN uses the SYLVIA software. The SYLVIA two-zone model was developed by IRSN to simulate a full ventilation network, fire scenarios in a highly confined and mechanically ventilated facility, and airborne contamination transfers inside nuclear installations. In order to take into account the different sources of uncertainty coming from initial and boundary conditions as well as from model parameters, the SYLVIA software is associated with the SUNSET statistical software. However, such a use of SYLVIA software has a major drawback: it requires a large number of runs and a significant statistical analysis what is not always compatible with the requirements of safety assessments in terms of deadlines. To overcome this difficulty, IRSN is currently developing an expert system based on a SYLVIA database. This approach allows deriving the most likely diagnosis or prognosis in a very short time, but also deriving a more complex form of reasoning intertwining prognostic and diagnostic inferences. The proposed expert system is based on the Bayesian Belief Network (BBN) methodology and consists in two steps: First, a large database obtained from SYLVIA runs allows the estimation of Conditional Probability Tables. Then, a message passing algorithm is used to exploit dynamically this data base. The illustrating example is based on the study of pressure effects due to fire scenarios in nuclear facilities and the database is made up of 1,600,000 runs of the SYLVIA software. The goal of this paper is to detail the methodology and process to carry out an expert system for fire safety studies, and is supported by one example showing how it can be used as a decision support tool for fire safety analysis in nuclear area. To our opinion, the development of expert systems represents a new generation of computational tools in the field of probabilistic fire simulation.}
}
@article{CHEN2025107318,
title = {HCPI-HRL: Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning},
journal = {Neural Networks},
volume = {187},
pages = {107318},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107318},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025001972},
author = {Bin Chen and Zehong Cao and Wolfgang Mayer and Markus Stumptner and Ryszard Kowalczyk},
keywords = {Deep reinforcement learning, Hierarchical reinforcement learning, Causal inference, Subgoal discovery},
abstract = {The dependency on extensive expert knowledge for defining subgoals in hierarchical reinforcement learning (HRL) restricts the training efficiency and adaptability of HRL agents in complex, dynamic environments. Inspired by human-guided causal discovery skills, we proposed a novel method, Human Causal Perception and Inference-driven Hierarchical Reinforcement Learning (HCPI-HRL), designed to infer diverse, effective subgoal structures as intrinsic rewards and incorporate critical objects from dynamic environmental states using stable causal relationships. The HCPI-HRL method is supposed to guide an agent’s exploration direction and promote the reuse of learned subgoal structures across different tasks. Our designed HCPI-HRL comprises two levels: the top level operates as a meta controller, assigning subgoals discovered based on human-driven causal critical object perception and causal structure inference; the bottom level employs the Proximal Policy Optimisation (PPO) algorithm to accomplish the assigned subgoals. Experiments conducted across discrete and continuous control environments demonstrated that HCPI-HRL outperforms benchmark methods such as hierarchical and adjacency PPO in terms of training efficiency, exploration capability, and transferability. Our research extends the potential of HRL methods incorporating human-guided causal modelling to infer the effective relationships across subgoals, enhancing the agent’s capability to learn efficient policies in dynamic environments with sparse reward signals.}
}
@article{HILL2023,
title = {Assessing Serious Spinal Pathology Using Bayesian Network Decision Support: Development and Validation Study},
journal = {JMIR Formative Research},
volume = {7},
year = {2023},
issn = {2561-326X},
doi = {https://doi.org/10.2196/44187},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X23004456},
author = {Adele Hill and Christopher H Joyner and Chloe Keith-Jopp and Barbaros Yet and Ceren {Tuncer Sakar} and William Marsh and Dylan Morrissey},
keywords = {artificial intelligence, back pain, Bayesian network, expert consensus},
abstract = {Background
Identifying and managing serious spinal pathology (SSP) such as cauda equina syndrome or spinal infection in patients presenting with low back pain is challenging. Traditional red flag questioning is increasingly criticized, and previous studies show that many clinicians lack confidence in managing patients presenting with red flags. Improving decision-making and reducing the variability of care for these patients is a key priority for clinicians and researchers.
Objective
We aimed to improve SSP identification by constructing and validating a decision support tool using a Bayesian network (BN), which is an artificial intelligence technique that combines current evidence and expert knowledge.
Methods
A modified RAND appropriateness procedure was undertaken with 16 experts over 3 rounds, designed to elicit the variables, structure, and conditional probabilities necessary to build a causal BN. The BN predicts the likelihood of a patient with a particular presentation having an SSP. The second part of this study used an established framework to direct a 4-part validation that included comparison of the BN with consensus statements, practice guidelines, and recent research. Clinical cases were entered into the model and the results were compared with clinical judgment from spinal experts who were not involved in the elicitation. Receiver operating characteristic curves were plotted and area under the curve were calculated for accuracy statistics.
Results
The RAND appropriateness procedure elicited a model including 38 variables in 3 domains: risk factors (10 variables), signs and symptoms (17 variables), and judgment factors (11 variables). Clear consensus was found in the risk factors and signs and symptoms for SSP conditions. The 4-part BN validation demonstrated good performance overall and identified areas for further development. Comparison with available clinical literature showed good overall agreement but suggested certain improvements required to, for example, 2 of the 11 judgment factors. Case analysis showed that cauda equina syndrome, space-occupying lesion/cancer, and inflammatory condition identification performed well across the validation domains. Fracture identification performed less well, but the reasons for the erroneous results are well understood. A review of the content by independent spinal experts backed up the issues with the fracture node, but the BN was otherwise deemed acceptable.
Conclusions
The RAND appropriateness procedure and validation framework were successfully implemented to develop the BN for SSP. In comparison with other expert-elicited BN studies, this work goes a step further in validating the output before attempting implementation. Using a framework for model validation, the BN showed encouraging validity and has provided avenues for further developing the outputs that demonstrated poor accuracy. This study provides the vital first step of improving our ability to predict outcomes in low back pain by first considering the problem of SSP.
International Registered Report Identifier (IRRID)
RR2-10.2196/21804}
}
@article{LIU2025110279,
title = {Context-aware inverse reinforcement learning for modeling individuals’ daily activity schedules},
journal = {Engineering Applications of Artificial Intelligence},
volume = {146},
pages = {110279},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110279},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625002799},
author = {Dongjie Liu and Dawei Li and Kun Gao and Yuchen Song and Zijie Zhou},
keywords = {Travel demand modeling, Activity generation, Artificial Intelligence, Activity-based models, Inverse reinforcement learning},
abstract = {Understanding individual and crowd dynamics in urban environments is critical for numerous applications, such as urban planning, traffic forecasting, and location-based services. Therefore, accurately modeling individuals' daily activity schedules is essential. Traditional methods, like utility-based and rule-based approaches, rely on expert knowledge and presumed model structures. While machine learning methods offer flexibility, they often ignore explicit behavioral mechanisms, particularly comprehensive discussion and integration of context related to individuals' daily travel. To address these, we propose a framework that integrates travel context with deep Inverse Reinforcement Learning (IRL), learning temporal patterns from sociodemographics, start time and duration of the current activity, travel modes, and land use. Specifically, individuals' activity schedules are initially formulated as a Markov Decision Process to simulate travelers’ sequential decision-making processes, laying the groundwork for the IRL framework; Then, a context-aware IRL method is proposed to model individuals' travel decision-making from observed temporal trajectories. Finally, we validate the proposed model by demonstrating its superior performance over discrete choice model and several well-known imitation learning benchmarks in tasks such as policy performance comparison, reward recovery, model generalizability, and computational efficiency using travel behavior datasets. This approach provides meaningful behavioral insights and paves the way for Artificial Intelligence-driven activity schedulers modeling.}
}
@article{TICEHURST201152,
title = {Using Bayesian Networks to complement conventional analyses to explore landholder management of native vegetation},
journal = {Environmental Modelling & Software},
volume = {26},
number = {1},
pages = {52-65},
year = {2011},
note = {Thematic Issue on Science to Improve Regional Environmental Investment Decisions},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2010.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S1364815210000915},
author = {J.L Ticehurst and A. Curtis and W.S. Merritt},
keywords = {Natural resource management, Bayesian Networks, Adoption, Policy development},
abstract = {Influencing the management of private landholders is a key element of improving the condition of Australia’s natural resources. Despite substantial investment by governments, effecting behavioural change on a scale likely to stem biodiversity losses has proven difficult. Understanding landholder decision-making is now acknowledged as fundamental to achieving better policy outcomes. There is a large body of research examining landholder adoption of conservation practices. Social researchers are able to employ a suite of conventional techniques to analyse their survey data and assist in identifying significant and causal relationships between variables. However, these techniques can be limited by the type of data available, the breadth of issues that can be represented and the extent that causality can be explored. In this paper we discuss the findings of a unique study exploring the benefits of combining Bayesian Networks (BNs) with conventional statistical analysis to examine landholder adoption. Our research examined the landholder fencing of native bushland in the Wimmera region in south east Australia. Findings from this study suggest that BNs provided enhanced understanding of the presence and strength of causal relationships. There was also the additional benefit that a BN could be quickly developed and that this process helped the research team clarify and understand relationships between variables. However, a key finding was that the interpretation of the results of the BNs was complemented by the conventional data analysis and expert review. An additional benefit of the BNs is their capacity to present key findings in a format that is more easily interpreted by researchers and enables researchers to more easily communicate their findings to natural resource practitioners and policy makers.}
}
@article{HU2024121677,
title = {Discovering key factors and causalities impacting bridge pile resistance using Ensemble Bayesian networks: A bridge infrastructure asset management system},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121677},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121677},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423021796},
author = {Xi Hu and Rayan H. Assaad and Mohab Hussein},
keywords = {Bayesian network, Bridge expert system, Bridge infrastructure},
abstract = {Bridges are one of the critical infrastructure systems and play a critical role in supporting the economic development of nations. During the planning, design, and construction phases of bridges, it is important to ensure that the different bridge components satisfy the desired performance for avoiding catastrophic failure. One example is the foundation system with deep driven piles. A crucial consideration when planning, designing, and constructing bridge foundation system is to accurately estimate the pile resistance (i.e., capacity). However, this process constitutes a considerable challenge for bridge managers and engineers due to the uncertainties, subjectivities, and biases involved in providing accurate estimates. In fact, the current design process of bridge pile foundations is still performed in a relatively simple and deterministic manner. Thus, there is still a lack of studies that were conducted to effectively represent the causalities between different variables impacting the bridge pile resistance using objective, data-driven methods rather than subjective, expert-based methods. Therefore, this paper addresses this knowledge gap by proposing a data-driven, knowledge-discovery Bayesian network model as an expert system to (1) identify the key factors impacting the performance of bridge piles through data-driven knowledge discovery; (2) capture the causal (direct and indirect) relationships between the different variables that influence bridge pile resistance, and (3) predict bridge pile resistance in support of decision making in bridge design and construction. Multiple Bayesian network models were developed using two distinct learning paradigms based on a pile dataset with 18 variables and 2,375 data samples from a large bridge project. One paradigm is to run a regular process for learning graphical models using Bayesian structure learning, while the other one is to couple Bayesian networks with bagging ensemble learning. Model validation results showed that the best model learned by bagging ensemble learning paradigm achieves the highest prediction accuracy of 89.78% for predicting bridge pile resistance. Additionally, the developed Bayesian network models identify 8 critical factors affecting bridge pile resistance. These factors are categorized into four groups, including the external applied force on pile head, pile mechanical property (i.e., compressive stresses in bridge piles), hammer property and operation characteristics (e.g., hammer blow count), and pile cap elevation. The observations from this study would assist bridge managers and practitioners in decreasing the uncertainties that impact the estimation of pile capacity, and thus help in balancing cost-effectiveness with good performance. The paper adds to the body of knowledge by providing a great deal of insights related to the factors and causalities influencing the performance of deep bridge foundations to ultimately help in the plan, design, and construction phases of bridges.}
}
@article{RUIZTAGLE2022108507,
title = {BaNTERA: A Bayesian Network for Third-Party Excavation Risk Assessment},
journal = {Reliability Engineering & System Safety},
volume = {223},
pages = {108507},
year = {2022},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2022.108507},
url = {https://www.sciencedirect.com/science/article/pii/S0951832022001661},
author = {Andres Ruiz-Tagle and Austin D. Lewis and Colin A. Schell and Ernest Lever and Katrina M. Groth},
keywords = {Third-party damage, Pipeline damage, Pipeline safety, Bayesian networks, Risk assessment},
abstract = {Third-party damage constitutes a major threat to underground natural gas pipeline safety; in the U.S., between 2016 and 2020, it caused eleven fatalities, twenty-nine injuries, and $124M USD in property damage losses. Several research studies have been carried out to identify the causes and contextual factors leading to third-party damage. However, there is a lack of models that are not only causally-based, but also comprehensive and suitable for modeling the probabilities of a pipe hit and subsequent damage. This paper presents the development process and results of building BaNTERA, a probabilistic Bayesian network model for third-party excavation risk assessment in the U.S. BaNTERA’s capabilities for risk-informed decision support are presented in three ways: verification of the model’s performance, validation of its damage rate predictions with historical industry data, and application in multiple case study scenarios. Preliminary results indicate that BaNTERA offers valuable insight including and beyond a probability estimation of third-party damage. Using the best available industry data and previous models derived from multiple sources, different inference methods can assist in pipeline damage prevention and risk mitigation. As such, BaNTERA represents a promising holistic and rigorous tool for addressing third-party excavation damage in natural gas pipelines.}
}
@article{WEICHERT2023514,
title = {Explainable production planning under partial observability in high-precision manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {514-524},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001590},
author = {Dorina Weichert and Alexander Kister and Peter Volbach and Sebastian Houben and Marcus Trost and Stefan Wrobel},
keywords = {Explainability, Manufacturing, Reinforcement Learning, Monte Carlo Tree Search, Partially Observable Markov Decision Process},
abstract = {Conceptually, high-precision manufacturing is a sequence of production and measurement steps, where both kinds of steps require to use non-deterministic models to represent production and measurement tolerances. This paper demonstrates how to effectively represent these manufacturing processes as Partially Observable Markov Decision Processes (POMDP) and derive an offline strategy with state-of-the-art Monte Carlo Tree Search (MCTS) approaches. In doing so, we face two challenges: a continuous observation space and explainability requirements from the side of the process engineers. As a result, we find that a tradeoff between the quantitative performance of the solution and its explainability is required. In a nutshell, the paper elucidates the entire process of explainable production planning: We design and validate a white-box simulation from expert knowledge, examine state-of-the-art POMDP solvers, and discuss our results from both the perspective of machine learning research and as an illustration for high-precision manufacturing practitioners.}
}
@article{DOUGLAS2014235,
title = {Evaluation of Bayesian networks for modelling habitat suitability and management of a protected area},
journal = {Journal for Nature Conservation},
volume = {22},
number = {3},
pages = {235-246},
year = {2014},
issn = {1617-1381},
doi = {https://doi.org/10.1016/j.jnc.2014.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1617138114000053},
author = {Sarah J. Douglas and Adrian C. Newton},
keywords = {Expert knowledge, Conservation, Biodiversity, Species distribution modelling, Habitat condition, Decision support},
abstract = {To be effective, management of protected areas should be based on the best available evidence, including the scientific literature and expert knowledge. However, lack of such evidence in a suitable form to support decision-making may hinder effective management. Here we examine the use of Bayesian networks to support the management of protected areas, through the development of habitat suitability models for eight species of conservation concern. Bayesian networks were constructed on the basis of the scientific literature and expert knowledge, and were then tested using results from a field survey. Models of all species demonstrated very high discrimination between presence and absence sites, as indicated by AUC values >0.8, with values >0.9 obtained for four species, and Kappa values in the range of 0.4–0.9. The Bayesian networks were then used to examine the impact of different management interventions on habitat suitability of each species, including tree cutting, grazing and burning. Species differed in terms of their sensitivity to different management interventions, and model output provided evidence of both negative and positive interactions between types of intervention. These results highlight the trade-offs that must often be made when undertaking conservation management, and demonstrate the value of Bayesian networks in helping to make such trade-offs explicit. The identification of management impacts through analysis of available evidence also demonstrates the value of Bayesian networks for supporting evidence-based approaches to protected area management.}
}
@article{JIANG2025102873,
title = {A Bayesian network approach for dynamic behavior analysis: Real-time intention recognition},
journal = {Information Fusion},
volume = {118},
pages = {102873},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102873},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524006511},
author = {Jiaxuan Jiang and Jiapeng Liu and Miłosz Kadziński and Xiuwu Liao},
keywords = {Decision analysis, Intention recognition, Real-time intention, Dynamic behavior, Data fusion, Noise filtering},
abstract = {Dynamic intention recognition is widely applied across diverse domains, including autonomous driving, e-commerce, and human–computer interaction, to understand and identify individuals’ evolving behavioral intentions. While observable behaviors often serve as proxies for underlying intentions, accurately establishing the relationships between dynamic behaviors and evolving intentions becomes a challenging task. Moreover, external factors introduce dynamism and noise into behavioral data, complicating the process of inferring intentions. To address these challenges, we propose a novel Bayesian network approach that comprehensively models the real-time inference of behavioral intentions from dynamic behaviors. Our model incorporates a filtering mechanism designed to process evolving, noisy, and time-stamped behavioral data, enhancing data quality and ensuring reliable intention recognition. By mapping latent states to intentions through conditional dependencies and visualizing the generative process using directed acyclic graphs, we provide a transparent representation of the model’s structure and reasoning. Experimental evaluations conducted on both real and synthetic datasets demonstrate the superior performance of our model compared to existing benchmarks, particularly in handling imbalanced data and minority classes. Furthermore, we extend our analysis to multi-target intention recognition scenarios, validating the model’s adaptability in inferring the intentions of multiple individuals concurrently. Our approach offers a practical tool for decision analysis, empowering managers and practitioners to understand, predict, and proactively respond to individual behavioral intentions, thereby facilitating the development of targeted strategies and personalized services.}
}
@article{KIM201359,
title = {NEST: A quantitative model for detecting emerging trends using a global monitoring expert network and Bayesian network},
journal = {Futures},
volume = {52},
pages = {59-73},
year = {2013},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2013.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S001632871300102X},
author = {Seonho Kim and You-Eil Kim and Kuk-Jin Bae and Sung-Bae Choi and Jong-Kyu Park and Young-Duk Koo and Young-Wook Park and Hyun-Kyoo Choi and Hyun-Moo Kang and Sung-Wha Hong},
keywords = {Weak signal, Emerging trend, Qualitative analysis, Quantitative analysis, Bayesian network, Delphi study},
abstract = {The analysis of changes in the research and development (R&D) environment and developing foresight of future technologies are increasingly recognized as important to support policy decision making and efficient resource distribution. Many futurists are developing foresight of future technologies based on Delphi studies, unfolding history, brainstorming, expert surveys, trend analysis, data mining, and so on. However, formalizing these processes is still a necessary task. In this paper, we introduce the NEST (New and Emerging Signals of Trends) model developed by the Korea Institute of Science and Technology Information (KISTI). The NEST collects information from worldwide expert networks and detects the weak signals of emerging future trends systematically, based on massive data analysis, inference techniques, and Delphi studies, to support the development of foresight of future research and technology. The NEST model combines quantitative and qualitative approaches. In the quantitative approach stages, NEST uses clustering, pattern recognition, and cross-impact analysis using a Bayesian network. In the stages of qualitative approaches, NEST conducts environmental scanning, brainstorming, and a Delphi study.}
}
@article{PEREZ2024111440,
title = {Generation of probabilistic synthetic data for serious games: A case study on cyberbullying},
journal = {Knowledge-Based Systems},
volume = {286},
pages = {111440},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124000753},
author = {Jaime Pérez and Mario Castro and Edmond Awad and Gregorio López},
keywords = {Synthetic data, Serious games, Cyberbullying, Item response theory, Bayesian network, Hierarchical Bayesian model, Computational social science},
abstract = {Synthetic data generation has been a growing area of research in recent years. However, its potential applications in serious games have yet to be thoroughly explored. Advances in this field could anticipate data modeling and analysis, as well as speed up the development process. To fill this gap in the literature, we propose a simulator architecture for generating probabilistic synthetic data for decision-based serious games. This architecture is designed to be versatile and modular so that it can be used by other researchers on similar problems (e.g., multiple choice exams, political surveys, any type of questionnaire). To simulate the interaction of synthetic players with the game, we use a cognitive testing model based on the Item Response Theory framework. We also show how probabilistic graphical models (in particular, Bayesian networks) can introduce expert knowledge and external data into the simulation. Finally, we apply the proposed architecture and methods in the case of a serious game focused on cyberbullying. We perform Bayesian inference experiments using a hierarchical model to demonstrate the identifiability and robustness of the generated data.}
}
@article{CELIO2014222,
title = {Modeling land use decisions with Bayesian networks: Spatially explicit analysis of driving forces on land use change},
journal = {Environmental Modelling & Software},
volume = {52},
pages = {222-233},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213002570},
author = {Enrico Celio and Thomas Koellner and Adrienne Grêt-Regamey},
keywords = {Land use decisions, Land use modeling, Bayesian networks, Participatory modeling},
abstract = {Land use decisions result from complex deliberative processes and fundamentally influence the livelihoods of many. These decisions are made based on quantitatively measurable information like topography and on qualitative criteria such as personal preferences. Bayesian networks (BN) are able to integrate both quantitative and qualitative data and are thus suitable to approach such processes. We model land use decisions in a pre-Alpine area in Switzerland, integrating biophysical data and local actors' knowledge into a spatially explicit BN. A structured experts' process to elaborate three different BN including agriculture, forestry, and settlement provides the base for the modeling. A spatially explicit updating of the BN via questionnaires enables us to take local actors' characteristics into account. Results show which drivers are most important for land use decision-making in our case study region, and how an alteration of these drivers could change future land use. Furthermore, focusing on the probability of occurrence of various land uses in a spatially explicit manner gives insights into path-dependency of land use change. This knowledge can serve as information for planners and policy makers to design more effective policy instruments.}
}
@article{HOSSEINI2019110,
title = {Development of a Bayesian network model for optimal site selection of electric vehicle charging station},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {105},
pages = {110-122},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2018.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0142061517309936},
author = {Seyedmohsen Hosseini and MD Sarder},
keywords = {Site selection, Electric vehicle, Sustainability, Bayesian network},
abstract = {Fast charging sites play a crucial role for public acceptance of electric vehicle (EV). Selection of the most sustainable site positively contributes to the life cycle of electric vehicle charging station (EVCS), which requires considering some conflicting criteria. Previous researches mainly focused on utilizing optimization models to deal with EVCS site selection that only accounts for quantitative factors, while this paper proposes a Bayesian Network (BN) model that considers not only quantitative factors but also qualitative (subjective) ones. Based on academic literature and expert judgments, the assessment index for EVCS site selection was mainly made from sustainability point of view, which contains of economic, environmental, and social criteria with a total of eleven sub-criteria. BNs are powerful tools for handling risk assessment and decision making under uncertainty. The developed BN model is validated through sensitivity analysis approach. Finally, different propagation analyses have been performed to make special types of reasoning. This paper provides a new research perspective by considering uncertainty, qualitative and quantitative factors into the site selection assessment, and presents the mainstream penetration of BN as a powerful decision making tool in the context of electrical energy management.}
}
@article{GAO2025116075,
title = {An innovative deep reinforcement learning-driven cutting parameters adaptive optimization method taking tool wear into account},
journal = {Measurement},
volume = {242},
pages = {116075},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.116075},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124019602},
author = {Zhilie Gao and Ni Chen and Yingfei Yang and Liang Li},
keywords = {The cutting parameters adaptive optimization, Tool wear monitoring, Markov decision process, Deep reinforcement learning, Proximal policy optimization algorithm},
abstract = {Tool wear is critically important for the optimization of cutting parameters. However, the increasing nature of tool wear presents challenges to traditional meta-heuristic cutting parameter optimization methods. To address this issue, we propose an innovative deep reinforcement learning-driven cutting parameters adaptive optimization method taking tool wear into account. More specifically, we use the Markov Decision Process to simulate the optimization process of cutting parameters. Firstly, an innovative deep transfer learning algorithm is used for monitoring tool wear. With the progress of tool wear, the proximal policy optimization method of the transformer with multi-head attention mechanism interacts with the processing environment through a process of trial and error, and accumulates a wealth of experience in selecting cutting parameters through the reward function. The deep reinforcement learning model has quickly discern the best cutting parameters, relying on real-time tool wear value. The experimental results show that the proposed method outperforms other algorithms.}
}
@article{HARRIS2022100410,
title = {A Bayesian network approach for multi-sectoral flood damage assessment and multi-scenario analysis},
journal = {Climate Risk Management},
volume = {35},
pages = {100410},
year = {2022},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2022.100410},
url = {https://www.sciencedirect.com/science/article/pii/S2212096322000171},
author = {Remi Harris and Elisa Furlan and Hung Vuong Pham and Silvia Torresan and Jaroslav Mysiak and Andrea Critto},
keywords = {Machine Learning, Flood risk assessment, Climate adaptation, Sensitivity analysis, Secchia river},
abstract = {Extreme weather and climate related events, from river flooding to droughts and tropical cyclones, are likely to become both more severe and more frequent in the coming decades, and the damages caused by these events will be felt across all sectors of society. In the face of this threat, policy- and decision-makers are increasingly calling for new approaches and tools to support risk management and climate adaptation pathways that can capture the full extent of the impacts. In this frame, a GIS-based Bayesian Network (BN) approach is presented for the capturing and modelling of multi-sectoral flooding damages against future ‘what-if’ scenarios. Building on a risk-based conceptual framework, the BN model was trained and validated by exploiting data collected from the 2014 Secchia River flooding event, as well as other contextual variables. Moreover, a novel approach to defining the structure of the BN was performed, reconfiguring the model according to expert judgment and data-based validation. The model showed a good predictive capacity for damages in the agricultural, industrial and residential sectors, predicting the severity of damages with a classification accuracy of about 60% for each of these assessment endpoints. ‘What-if’ scenario analysis was performed to understand the potential impacts of future changes in i) land use patterns and ii) increasing flood depths resulting from more severe flood events. The output of the model showed a rising probability of experiencing high monetary damages under both scenarios. In spite of constraints within the case study dataset, the results of the appraisal show good promise, and together with the designed BN model itself represent a valuable support for disaster risk management and reduction actions against extreme river flooding events, enabling better informed decision making.}
}
@article{COPLEY2025103951,
title = {A study of risk factors for potential spills in the Galveston Bay Area},
journal = {Regional Studies in Marine Science},
volume = {81},
pages = {103951},
year = {2025},
issn = {2352-4855},
doi = {https://doi.org/10.1016/j.rsma.2024.103951},
url = {https://www.sciencedirect.com/science/article/pii/S235248552400584X},
author = {Samuel Copley and Mawuli Afenyo and Livingstone D. Caesar},
keywords = {Oil spills, Environmental pollution, Economics, Bayesian networks, Risk-analysis, Maritime policy, Gaps, Threats},
abstract = {Maritime businesses in the Galveston Bay are at risk from environmental and economic damages caused by potential oil spills. Spills disrupt shipping operations, damage surrounding ecosystems, and can require expensive and laborious cleanup efforts. Despite improvements in technology and increased regulations that seek to mitigate the occurrence and risks of spills, they will continue to happen and cause damage. As such, new, sustainable, and proactive approaches to solving the spill problem must be created. We studied the risk-related factors contributing to spills in Galveston Bay, Texas and the resulting environmental and economic impacts. The extant literature was reviewed to assess these factors, and qualitative and quantitative questionnaires were distributed to experts on spills in the Galveston Bay area. The factors solicited from the literature and questionnaires were used to construct an Environmental and Economic Impact Model (E.E.I.M.) using the Bayesian theory. Critical risk-related factors were found to be spill location, spill type, environmental conditions, seasonality, and the specific cleanup/recovery methods deployed. In addition, costs of the environmental and economic impacts were quantified for several potential spill scenarios. The information obtained in this study can be used to develop transferrable tools, guidelines, and frameworks to support decision-making in the Galveston Bay area and elsewhere. Further, the paper highlights the need for a holistic and proactive evaluation of the different dimensions of a spill; this includes policy, economic, health, social and social implications, and the most effective remediation mechanisms for particular scenarios implemented.}
}
@article{WANG2025101343,
title = {A clinical treatment recommender system optimizing adjuvant chemoradiotherapy benefit in Chinese women with breast cancer using interpretable causal Bayesian networks},
journal = {The Lancet Regional Health - Western Pacific},
volume = {55},
pages = {101343},
year = {2025},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2024.101343},
url = {https://www.sciencedirect.com/science/article/pii/S2666606524003377},
author = {Ruoyu Wang and Simiao Tian and Zhe Wang and Yiou Wang and Dianlong Zhang and Jianing Jiang and Yongqiang Yao and Hongshen Chen and Hong Fang and Mingqian Cao},
abstract = {Background
Breast cancer (BC) has surpassed lung cancer in becoming the most common cancer among women worldwide. However, few studies have investigated multivariate BC prognosis outcomes. This study aimed to develop and validate a causal model based on Bayesian networks (BN) that can simultaneously predict long-term risk of all-cause mortality and recurrence among Chinese women with BC, and further to estimate individualised expected benefits of model-based treatment recommendations.
Methods
This study used data from adult women who were diagnosed with primary invasive T1-4 N0-3 M0 BC at the Affiliated Zhongshan Hospital of Dalian University, Dalian, Liaoning, China, between January 2011 and December 2017. After the whole cohort was randomly divided into 70% development and 30% validation cohorts, a causal BN (CBN) model was built by aggregating extensive expert knowledge and data-driven approaches to arrive at a consensus structure for causal inference, and subsequently validated its ability to predict both overall survival (OS) and recurrence free survival (RFS) by using the area under the curve (AUC), Brier score (BS), accuracy and predicted/observation ratio. Variables included sociodemographics; preoperative clinical, histopathological and molecular biomarkers; operative variables; and postoperative treatment variables. OS and RFS in women with BC were the main outcomes of this study and were assessed at the most recent follow-up on December 31, 2022.
Findings
A total of 3512 patients, which consisted of 2458 patients in the development cohort (mean [SD] age, 54.60 [10.68] years) and 1054 patients in the validation cohort (mean [SD] age, 53.77 [11.08] years), were eligible for this study. A total of 175 (5.0%) patients with BC died, and 344 (9.8%) experienced recurrence after at least 5-year follow-up. A CBN model was developed that included the following predictors: age group; preoperative CEA, CA 125 and CA15-3 serum levels; neoadjuvant chemoradiotherapy; surgery type; tumour grade and histology; immunohistochemical expression of oestrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) amplification, and Ki-67; pT and pN stages; and postoperative treatment with chemotherapy, radiotherapy and endocrine therapy. In the validation cohort, the CBN model had excellent performance with desirable AUCs of 0.92 (95% CI 0.87-0.98) and 0.73 (0.62-0.83) and low BSs of 0.025 and 0.062 for OS and RFS, respectively. The model also achieved excellent accuracy and model calibration for OS and RFS, with accuracies of 96.4% and 92.8% and predicted/observed ratios of 1.01 (0.92-1.11) and 1.05 (0.95-1.15), respectively. Furthermore, the chemoradiotherapy treatment according to our CBN model recommendations was associated with notably better prognosis, with a hazard ratio of 0.63 (0.44-0.90) for OS and 0.81 for RFS (0.75-0.87), respectively.
Interpretation
In this prognostic study based on clinical real-world data, a CBN model was developed that accurately predicted two important outcomes for women with BC, and it has the potential to identify patients who could benefit from chemoradiotherapy. The CBN can effectively capture complex interrelationships among risk factors and identify potential causal pathways underlying tumour progression through its graphical representation, and could also provide an adjuvant treatment recommendation system for BC women.}
}
@article{BROMLEY2005231,
title = {The use of Hugin® to develop Bayesian networks as an aid to integrated water resource planning},
journal = {Environmental Modelling & Software},
volume = {20},
number = {2},
pages = {231-242},
year = {2005},
note = {Policies and Tools for Sustainable Water Management in the European Union},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2003.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000404},
author = {J. Bromley and N.A. Jackson and O.J. Clymer and A.M. Giacomello and F.V. Jensen},
keywords = {Integrated water resource management, Stakeholder participation, Bayesian network, Uncertainty},
abstract = {Integrated management is the key to the sustainable development of Europe's water resources. This means that decisions need to be taken in the light of not only environmental considerations, but also their economic, social, and political impacts; it also requires the active participation of stakeholders in the decision making process. The problem is to find a practical way to achieve these aims. One approach is to use Bayesian networks (Bns): networks allow a range of different factors to be linked together, based on probabilistic dependencies, and at the same time provide a framework within which the contributions of stakeholders can be taken into account. A further strength is that Bns explicitly include the element of uncertainty related to any strategy or decision. The links are based on whatever data are available. This may be an extensive data set, output from a model or, in the absence of data, can be based on expert opinion. Networks are being developed for four catchments in Europe as part of the MERIT project; these are in the UK, Denmark, Italy and Spain. In each case stakeholder groups are contributing to the design of the networks that are used as a focus for the consultation process. As an example, the application to water management of a UK basin is discussed.}
}
@article{ZWIRGLMAIER2024102034,
title = {Linking urban structure types and Bayesian network modelling for an integrated flood risk assessment in data-scarce mega-cities},
journal = {Urban Climate},
volume = {56},
pages = {102034},
year = {2024},
issn = {2212-0955},
doi = {https://doi.org/10.1016/j.uclim.2024.102034},
url = {https://www.sciencedirect.com/science/article/pii/S221209552400230X},
author = {Veronika Zwirglmaier and Matthias Garschagen},
keywords = {Integrated flood risk assessment, Data-scarcity, Adaptation to climate change, Bayesian network modelling, Urban structure types, Mega-cities},
abstract = {Urban flood risk increases under rapid urbanization and climate change. Thus, it becomes crucial to assess current and future risk and potential adaptation strategies to minimize the consequences for society, ecology and economy, especially in the Global South where urbanization and vulnerabilities are particularly high. However, current assessment tools oftentimes struggle to perform integrated assessments of flood risk due to reasons like data scarcity, complexity of cities or the integration of different domains. Hence, current approaches usually apply a reduced perspective, e.g. in terms of the urban extent covered or the domains included. Here we propose an approach using urban structure types in combination with Bayesian networks to represent different environmental and socio-economic conditions throughout a city. The approach facilitates integrative flood risk assessments and allows to address questions of uncertainty, variability and explainability in complex and data-scare urban areas. The implementation of this new approach is presented and discussed. Results from our pilot in Mumbai, show that the approach is suitable for scenario evaluation in data-scarce contexts. The flexibility offered by the approach makes it relevant for policy and urban planning since different key drivers of urban flood risk can be integrated in assessments of adaptation strategies and decision-making.}
}
@article{YUCESAN2023110759,
title = {A holistic failure modes and effects analysis for university plastic injection laboratory under Bayesian Network},
journal = {Applied Soft Computing},
volume = {147},
pages = {110759},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110759},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623007779},
author = {Melih Yucesan and Muhammet Gul and Dragan Pamučar},
keywords = {Failure modes and effects analysis, Transportation, Best-worst method, Bayesian network, Rule-based, Plastic injection},
abstract = {University Research & Development (R&D) laboratories are facilities used for research and manufacturing activities in engineering and basic science fields. Various filling materials are used in plastic R&D laboratories to improve the mechanical properties of thermoplastics or to recycle waste materials. The use of filling materials causes changes in plastic production parameters. This study dealt with the risks arising from extrusion, injection and shredder in a plastic injection laboratory, which led to the inability to manufacture the appropriate product under a holistic methodology. The proposed holistic methodology merges the concepts of failure mode and effects analysis (FMEA), fuzzy best-worst method (FBWM), and a rule-based three-hierarchical Bayesian network (RB-THBN). First, 15 root risks have been identified under a three-stage hierarchy with the support of laboratory decision makers. Then, questionnaires were created using the FMEA concept. The importance values (degree of belief-DoB) of three different FMEA parameters (severity, occurrence and detection) by experts for each risk have been calculated with FBWM. And finally, the RB-THBN has been created, and the final risk values have been computed using the linear utility function for the network. The system has been analyzed via a Bayesian modeling software. A validity test and a control measures planning has also been executed. According to the results, the highest root risk has been determined as “E2-The prepared mixture is not homogeneous” with a crisp risk score of 0.639559. In addition, “Failure due to extrusion” has been determined as the highest risk category with a score of 0.564059. It was discussed in detail that these risks arise from homogenization and how the used mixture should be homogenized. A comparative study among traditional FMEA, FMEA extended under FBWM and the proposed approach is performed to observe the differences in final ranking of root risks and to highlight the arguments that strengthen the advantages of the proposed method over the other two.}
}

@article{WIEGERINCK19991231,
title = {Approximate inference for medical diagnosis},
journal = {Pattern Recognition Letters},
volume = {20},
number = {11},
pages = {1231-1239},
year = {1999},
issn = {0167-8655},
doi = {https://doi.org/10.1016/S0167-8655(99)00090-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167865599000902},
author = {W.A.J.J. Wiegerinck and H.J. Kappen and E.W.M.T. {ter Braak} and W.J.P.P. {ter Burg} and M.J. Nijman and Y.L. O and J.P. Neijt},
keywords = {Medical decision support, Bayesian belief networks, Variational approximations},
abstract = {Computer-based diagnostic decision support systems (DSSs) will play an increasingly important role in health care. Due to the inherent probabilistic nature of medical diagnosis, a DSS should preferably be based on a probabilistic model. In particular, Bayesian networks provide a powerful and conceptually transparent formalism for probabilistic modeling. A drawback is that Bayesian networks become intractable for exact computation if a large medical domain is to be modeled in detail. This has obstructed the development of a useful system for internal medicine. Advances in approximation techniques, e.g. using variational methods with tractable structures, have opened new possibilities to deal with the computational problem. However, the only way to assess the usefulness of these methods for a DSS in practice is by actually building such a system and evaluating it by users. In the coming years, we aim to build a DSS for anaemia based on a detailed probabilistic model, and equipped with approximate methods to study the practical feasibility and the usefulness of this approach in medical practice. In this paper, we will sketch how variational techniques with tractable structures can be used in a typical model for medical diagnosis. We provide numerical results on artificial problems. In addition, we describe our approach to develop the Bayesian network for the DSS and show some preliminary results.}
}
@article{SZOKE2022181,
title = {Investigating Successor Features in the Domain of Autonomous Vehicle Control},
journal = {Transportation Research Procedia},
volume = {62},
pages = {181-188},
year = {2022},
note = {24th Euro Working Group on Transportation Meeting},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522001508},
author = {Laszlo Szoke and Szilard Aradi and Tamas Tettamanti},
keywords = {reinforcement learning, autonomous vehicle motion control, highway assist, successor features, general policy improvement, general policy evaluation},
abstract = {In this article, the basic Reinforcement Learning (RL) concepts are discussed, continued with a brief explanation of Markov Decision Processes (MDPs). Reasoning for the application of RL in the autonomous vehicle control domain is accompanied with a developed basic environment for simulation-based training of agents. Furthermore, we look at the available literature of successor features, and the recent achievements of its utilization. Our motivation is to tackle the problem of credit assignment with reward decomposition by using successor features, because the complex tasks while driving can cause unsuccessful training and can be challenging. After explaining the applied methodology and showing how it works, state-of-the-art ideas are investigated and infused into the vehicle control realm. Moreover, the paper details how these features can be tailored to the highway driving scenarios and what is the secret behind its capability to boost the performance of the RL agent. In order to investigate the proposed problems in a credible way we applied a high-fidelity traffic simulator (SUMO) as our environment, and concluded different trainings based on various scenarios. We present successor features applied to an autonomous vehicle control setting, such as highway commute. Our results imply that learned skills can help with the multi-objective rewarding problem, and agents applied to changing reward systems can adapt quickly to the new tasks. The only thing to find is the correct decomposition and selection of successor features.}
}
@article{TREMBLAY20041,
title = {Choice and development of decision support tools for the sustainable management of deer–forest systems},
journal = {Forest Ecology and Management},
volume = {191},
number = {1},
pages = {1-16},
year = {2004},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2003.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378112703005206},
author = {Jean-Pierre Tremblay and Alison Hester and Jim Mcleod and Jean Huot},
keywords = {Plant–herbivore interactions, Boreal forest, Deer browsing, Ecological modelling, Decision support tools, Uncertainty},
abstract = {Situations where a natural resource is both an asset, as well as a threat, to the integrity of ecosystem function and biodiversity are difficult to manage sustainably. One such situation happens when native deer populations, which are managed for sport are overexploiting forests to a point where they severely compromise natural forest regeneration. Managers facing those situations need support from the scientific community to analyse and synthesise information on deer–forest relationships and thus help to predict the potential outcomes of different management options for both the deer and the forests. Research scientists are increasingly expected to provide expertise and support into the decision-making process. One way to achieve this is to develop decision support tools (DSTs) based upon sound, scientific understanding of the deer–forest systems. Our objective is to explore a range of approaches that have been used for the development of DSTs for deer–forest management and to propose criteria for selecting a specific approach or combination of approaches for specific situations. DST and research-oriented models were catalogued according to two modelling paradigms: bottom-up models, which simulate systems through inductive inference, by scaling up from fundamental processes to the inherent behaviour of the system—the best known applications are forest gap and individual-based models; and top-down models which proceed by deductive, rule-based inference—they include expert systems, qualitative simulation models, frame-based models, Markovian process models and Bayesian networks. Uncertainty assessment in both modelling paradigms is discussed. The analysis is put in the context of two very different examples of deer–forest systems currently requiring DST development to guide their management: (1) the upland red/roe deer—fragmented temperate/boreal forest system of Scotland; and (2) the white-tailed deer—eastern boreal forest system of Anticosti Island, Québec, Canada. We conclude that a top-down approach with explicit uncertainty assessment should be aimed for, as a deliverable product to the end-users, keeping in mind that simulation models from the bottom-up family may be required to gain insights about the underlying mechanisms.}
}
@article{LI201949,
title = {Analysis on accident-causing factors of urban buried gas pipeline network by combining DEMATEL, ISM and BN methods},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {61},
pages = {49-57},
year = {2019},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018306375},
author = {Feng Li and Wenhe Wang and Stevan Dubljevic and Faisal Khan and Jiang Xu and Jun Yi},
keywords = {Accident-causing analysis, DEMATEL, ISM, Bayesian network (BN), Safety engineering, Gas pipeline safety},
abstract = {The development of natural gas industry relies on safe and dependable pipeline networks. Gas pipe leakages can easily escalate to catastrophic events and result in tremendous losses of life and property in the urban context. It is therefore imperative to reduce the risk associated with urban buried gas pipeline network using reliable theoretical accident analysis. This study addresses this issue by systematic combination of three different approaches which include decision making trial and evaluation laboratory (DEMATEL), interpretive structure modelling (ISM) and Bayesian network (BN). The analysis methodology follows a two-stage procedure. First, a hierarchical network model represented by a cause-effect diagram is obtained using the combined DEMATEL-ISM method, which clearly confirms the coupling relationships among various accident-causing factors and the BN structure. It also identifies the most critical factors, which enables the owner/operators of the pipeline system to make decisions regarding the allocation of security management resources to reduce risks. Next, the hierarchical network model is mapped to a BN and expert judgments are further transformed into the conditional probability distribution, in order to quantify the strength of the coupling relationships among the accident-causing system, and determine main paths resulting in system failure. Moreover, it facilitates the analysis process by updating the developed BN model with given new information. The effectiveness and applicability of the proposed model has been validated in a case study, which indicates that the model is plausible in providing explicit risk information to support better safety management by prioritizing actions to prevent interrelated accidents.}
}
@article{HE2022939,
title = {Multi-objective optimization of the textile manufacturing process using deep-Q-network based multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {939-949},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521000728},
author = {Zhenglei He and Kim Phuc Tran and Sebastien Thomassey and Xianyi Zeng and Jie Xu and Changhai Yi},
keywords = {Deep reinforcement learning, Deep Q-networks, Multi-objective, Optimization, Decision, Process, Textile Manufacturing},
abstract = {Multi-objective optimization, such as quality, productivity, and cost, of the textile manufacturing process is increasingly challenging because of the growing complexity involved in the development of textile industry in the upcoming big data era. It is hard for traditional methods to deal with high-dimension decision space in this issue, and prior experts’ knowledge is required as well as human intervention. This paper proposed a novel framework that transformed the textile process optimization problem into a stochastic game, and introduced deep Q-networks algorithm instead of current methods to approach it in a multi-agent system. The developed multi-agent reinforcement learning system applied a utilitarian selection mechanism to maximize the sum of all agents’ rewards (obeying the increasing ε-greedy policy) in each state, to avoid the interruption of multiple equilibria and achieve the correlated equilibrium optimal solutions of the textile process. The case study result reflects that the proposed MARL system can achieve the optimal solutions for the textile ozonation process, and it performs better than the traditional approaches.}
}
@article{SABBADIN201266,
title = {A framework and a mean-field algorithm for the local control of spatial processes},
journal = {International Journal of Approximate Reasoning},
volume = {53},
number = {1},
pages = {66-86},
year = {2012},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2011.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X11001435},
author = {Régis Sabbadin and Nathalie Peyrard and Nicklas Forsell},
keywords = {Decision-theoretic planning, Factored Markov decision processes, Approximate policy iteration, Mean-field principle, Approximate linear programming},
abstract = {The Markov Decision Process (MDP) framework is a tool for the efficient modelling and solving of sequential decision-making problems under uncertainty. However, it reaches its limits when state and action spaces are large, as can happen for spatially explicit decision problems. Factored MDPs and dedicated solution algorithms have been introduced to deal with large factored state spaces. But the case of large action spaces remains an issue. In this article, we define graph-based Markov Decision Processes (GMDPs), a particular Factored MDP framework which exploits the factorization of the state space and the action space of a decision problem. Both spaces are assumed to have the same dimension. Transition probabilities and rewards are factored according to a single graph structure, where nodes represent pairs of state/decision variables of the problem. The complexity of this representation grows only linearly with the size of the graph, whereas the complexity of exact resolution grows exponentially. We propose an approximate solution algorithm exploiting the structure of a GMDP and whose complexity only grows quadratically with the size of the graph and exponentially with the maximum number of neighbours of any node. This algorithm, referred to as MF-API, belongs to the family of Approximate Policy Iteration (API) algorithms. It relies on a mean-field approximation of the value function of a policy and on a search limited to the suboptimal set of local policies. We compare it, in terms of performance, with two state-of-the-art algorithms for Factored MDPs: SPUDD and Approximate Linear Programming (ALP). Our experiments show that SPUDD is not generally applicable to solving GMDPs, due to the size of the action space we want to tackle. On the other hand, ALP can be adapted to solve GMDPs. We show that ALP is faster than MF-API and provides solutions of similar quality for most problems. However, for some problems MF-API provides significantly better policies, and in all cases provides a better approximation of the value function of approximate policies. These promising results show that the GMDP model offers a convenient framework for modelling and solving a large range of spatial and structured planning problems, that can arise in many different domains where processes are managed over networks: natural resources, agriculture, computer networks, etc.}
}
@article{FOUCART2023112381,
title = {Deep reinforcement learning for adaptive mesh refinement},
journal = {Journal of Computational Physics},
volume = {491},
pages = {112381},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112381},
url = {https://www.sciencedirect.com/science/article/pii/S002199912300476X},
author = {Corbin Foucart and Aaron Charous and Pierre F.J. Lermusiaux},
keywords = {Reinforcement learning, Adaptive mesh refinement, Finite element methods, Computational fluid dynamics, Ocean modeling, Atmospheric forecasting},
abstract = {Finite element discretizations of problems in computational physics often rely on adaptive mesh refinement (AMR) to preferentially resolve regions containing important features during simulation. However, these spatial refinement strategies are often heuristic and rely on domain-specific knowledge or trial-and-error. We treat the process of adaptive mesh refinement as a local, sequential decision-making problem under incomplete information, formulating AMR as a partially observable Markov decision process. Using a deep reinforcement learning (RL) approach, we train policy networks for AMR strategy directly from numerical simulation. The training process does not require an exact solution or a high-fidelity ground truth to the partial differential equation (PDE) at hand, nor does it require a pre-computed training dataset. The local nature of our deep RL (DRL) allows the policy network to be trained inexpensively on much smaller problems than those on which they are deployed. The new DRL-AMR method is not specific to any particular PDE, problem dimension, or numerical discretization. The RL policy networks, trained on simple examples, can generalize to more complex problems, and can flexibly incorporate diverse problem physics. To that end, we apply the method to a range of PDEs, using a variety of high-order discontinuous Galerkin and hybridizable discontinuous Galerkin finite element discretizations. We show that the resultant DRL policies are competitive with common AMR heuristics and strike a favorable balance between accuracy and cost such that they often lead to a higher accuracy per problem degree of freedom, and are effective across a wide class of PDEs and problems.}
}
@incollection{DEARDEN1994162,
title = {Integrating Planning and Execution in Stochastic Domains},
editor = {Ramon Lopez {de Mantaras} and David Poole},
booktitle = {Uncertainty in Artificial Intelligence},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {162-169},
year = {1994},
isbn = {978-1-55860-332-5},
doi = {https://doi.org/10.1016/B978-1-55860-332-5.50026-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603325500262},
author = {Richard Dearden and Craig Boutilier},
abstract = {We investigate planning in time-critical domains represented as Markov Decision Processes, showing that search based techniques can be a very powerful method for finding close to optimal plans. To reduce the computational cost of planning in these domains, we execute actions as we construct the plan, and sacrifice optimality by searching to a fixed depth and using a heuristic function to estimate the value of states. Although this paper concentrates on the search algorithm, we also discuss ways of constructing heuristic functions suitable for this approach. Our results show that by interleaving search and execution, close to optimal policies can be found without the computational requirements of other approaches.}
}
@article{LIU2022105044,
title = {Constructing growth evolution laws of arteries via reinforcement learning},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {168},
pages = {105044},
year = {2022},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2022.105044},
url = {https://www.sciencedirect.com/science/article/pii/S002250962200223X},
author = {Minliang Liu and Liang Liang and Hai Dong and Wei Sun and Rudolph L. Gleason},
keywords = {Vascular growth, Reinforcement learning, Hypertensive remodeling, Residual stress, Kinematic growth},
abstract = {Growth evolution laws, which mathematically describe how living tissues change their shape and properties in response to external stimuli, are required for modeling arterial growth. Traditionally, specific forms of growth laws are devised by domain experts. Since in vivo animal studies usually provide limited experimental data, generalization and inference are often employed to prescribe the functional form of growth laws. In this work, we employed the finite growth theory and developed a reinforcement learning (RL) approach to construct growth evolution laws by formulating the arterial growth problem under the framework of the Markov decision process (MDP). To maintain homeostatic stress levels in an optimal manner, RL agents were employed to determine stress-modulated anisotropic growth evolution at each time step. We illustrate the capabilities of the RL-based growth laws in two representative applications: 1) predicting homogenous growth of a thin-walled artery in response to hypertensive blood pressure, and 2) generating residual stress with heterogeneous growth in a thick-walled bi-layer aorta via distributed growth policies. Experimental data, where available, were used to compare expert-prescribed and RL-based growth laws. Our results demonstrated the capabilities of RL to effectively control the growth processes in response to hypertension, and the predictions are in good agreement with experimental observations. In particular, the RL growth laws captured the reduction of in vivo axial stretch without using experimental data for training. Moreover, the distributed RL growth policies achieved residual stress generation in a collaborative manner, which may pave the way for implementation in a finite element setting. This study sheds light on a new avenue to uncover growth evolution laws via RL, perhaps reducing the need for large experimental datasets and expert intelligence during growth law construction.}
}
@article{KOEN201729,
title = {An expert-driven causal model of the rhino poaching problem},
journal = {Ecological Modelling},
volume = {347},
pages = {29-39},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016307621},
author = {Hildegarde Koen and J.P. {de Villiers} and Henk Roodt and Alta {de Waal}},
keywords = {Rhino poaching, Predictive modeling, Insufficient data, Expert knowledge},
abstract = {A significant challenge in ecological modelling is the lack of complete sets of high-quality data. This is especially true in the rhino poaching problem where data is incomplete. Although there are many poaching attacks, they can be spread over a vast surface area such as in the case of the Kruger National Park in South Africa, which is roughly the size of Israel. Bayesian networks are useful reasoning tools and can utilise expert knowledge when data is insufficient or sparse. Bayesian networks allow the modeller to incorporate data, expert knowledge, or any combination of the two. This flexibility of Bayesian networks makes them ideal for modelling complex ecological problems. In this paper an expert-driven model of the rhino poaching problem is presented. The development as well as the evaluation of the model is performed from an expert perspective. Independent expert evaluation is performed in the form of queries that test different scenarios. Structuring the rhino poaching problem as a causal network yields a framework that can be used to reason about the problem, as well as inform the modeller of the type of data that has to be gathered.}
}
@article{VILIZZI20131,
title = {Model development of a Bayesian Belief Network for managing inundation events for wetland fish},
journal = {Environmental Modelling & Software},
volume = {41},
pages = {1-14},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212002757},
author = {L. Vilizzi and A. Price and L. Beesley and B. Gawne and A.J. King and J.D. Koehn and S.N. Meredith and D.L. Nielsen},
keywords = {Decision Support Tool, Environmental water, Fish populations, Murray-Darling Basin},
abstract = {Wetlands are essential components of floodplain–river ecosystems that often suffer degradation due to river regulation. To this end, the application of environmental water is increasingly being seen as an important amelioration strategy. However, decisions regarding the delivery of water to maximise environmental benefits, including native fish population health, are complex and difficult. This paper describes the development of a Bayesian Belief Network (BBN) model as part of a Decision Support Tool for assessing inundation strategies to benefit native wetland fish. Separate, albeit closely related, BBNs were developed for three native (golden perch Macquaria ambigua, carp gudgeon Hypseleotris spp., Australian smelt Retropinna semoni) and one alien fish species (common carp Cyprinus carpio carpio). The model structure was based on a conceptualisation of the relationships between wetland habitats, hydrology and fish responses, with emphasis on the types of inundation activities undertaken by managers. Conditional probability tables for fish responses were constructed from expert opinion and the model was validated against field data. The predictive ability and sensitivity of the model reflected the inherent high variability in relationships between wetland characteristics, hydrology and fish responses, but was nonetheless able to address satisfactorily such complexities within a holistic framework. As the model was designed in conjunction with managers and evaluated by them, its application will be enhanced by on-going engagement between managers and scientists.}
}
@article{TAN201051,
title = {Scalable approach for effective control of gene regulatory networks},
journal = {Artificial Intelligence in Medicine},
volume = {48},
number = {1},
pages = {51-59},
year = {2010},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2009.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0933365709001389},
author = {Mehmet Tan and Reda Alhajj and Faruk Polat},
keywords = {Gene regulatory networks, Control policy, Scalability technique, Feature reduction, Probabilistic Boolean networks, Markov decision problems},
abstract = {Objective: Interactions between genes are realized as gene regulatory networks (GRNs). The control of such networks is essential for investigating issues like different diseases. Control is the process of studying the states and behavior of a given system under different conditions. The system considered in this study is a gene regulatory network (GRN), and one of the most important aspects in the control of GRNs is scalability. Consequently, the objective of this study is to develop a scalable technique that facilitates the control of GRNs. Method: As the approach described in this paper concentrates on the control of GRNs, we argue that it is possible to improve scalability by reducing the number of genes to be considered by the control policy. Consequently, we propose a novel method that considers gene relevancy to estimate genes that are less important for control. This way, it is possible to get a reduced model after identifying genes that can be ignored in model-building. The latter genes are located based on a threshold value which is expected to be provided by a domain expert. Some guidelines are listed to help the domain expert in setting appropriate threshold value. Results: We run experiments using both synthetic and real data, including metastatic melanoma and budding yeast (Saccharomyces cerevisiae). The reported test results identified genes that could be eliminated from each of the investigated GRNs. For instance, test results on budding yeast identified the two genes SWI5 and MCM1 as candidates to be eliminated. This considerably reduces the computation cost and hence demonstrate the applicability and effectiveness of the proposed approach. Conclusion: Employing the proposed reduction strategy results in close to optimal solutions to the control of GRNs, which are otherwise intractable due to the huge state space implied by the large number of genes.}
}
@article{ZHU202154,
title = {Recovery preparedness of global air transport influenced by COVID-19 pandemic: Policy intervention analysis},
journal = {Transport Policy},
volume = {106},
pages = {54-63},
year = {2021},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X21000718},
author = {Chunli Zhu and Jianping Wu and Mingyu Liu and Linyang Wang and Duowei Li and Anastasios Kouvelas},
keywords = {Air transport, Post-recovery, Policy intervention, COVID-19, Causal Bayesian network (CBN)},
abstract = {The outbreak of COVID-19 constitutes an unprecedented disruption globally, in which risk management framework is on top priority in many countries. Travel restriction and home/office quarantine are some frequently utilized non-pharmaceutical interventions, which bring the worst crisis of airline industry compared with other transport modes. Therefore, the post-recovery of global air transport is extremely important, which is full of uncertainty but rare to be studied. The explicit/implicit interacted factors generate difficulties in drawing insights into the complicated relationship and policy intervention assessment. In this paper, a Causal Bayesian Network (CBN) is utilized for the modelling of the post-recovery behaviour, in which parameters are synthesized from expert knowledge, open-source information and interviews from travellers. The tendency of public policy in reaction to COVID-19 is analyzed, whilst sensitivity analysis and forward/backward belief propagation analysis are conducted. Results show the feasibility and scalability of this model. On condition that no effective health intervention method (vaccine, medicine) will be available soon, it is predicted that nearly 120 days from May 22, 2020, would be spent for the number of commercial flights to recover back to 58.52%–60.39% on different interventions. This intervention analysis framework is of high potential in the decision making of recovery preparedness and risk management for building the new normal of global air transport.}
}
@article{DENG2021106854,
title = {Build complementary models on human feedback for simulation to the real world},
journal = {Knowledge-Based Systems},
volume = {217},
pages = {106854},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106854},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121001179},
author = {Zixuan Deng and Yanping Xiang and Zhongfeng Kang},
keywords = {Safe reinforcement learning, Human-in-the-loop reinforcement learning, Markov decision processes, Supervised learning},
abstract = {Using simulators is a cost-effective way to meet human needs. Nevertheless, inevitable errors derived from the gap between simulation and the real world sometimes cause great losses and must be taken seriously. This paper focuses on one cause of the gap, which is the incomplete state representation in simulation, and proposes a supervised learning approach, correcting human-unacceptable policies calculated by simulators, based on human feedback. The approach first detects the related blind spots by classifiers which are trained on data from aggregation of noisy human feedback. Then, it corrects the human-unacceptable policies through the complementary model presented based on linear function approximation (LFA) and a policy iteration algorithm FRU-SADPP that uses radial basis functions (RBFs). We evaluate our approach on two simulated domains and demonstrate its higher accuracy of policies than two baselines, in terms of three typical kinds of human suboptimality and human errors, and three types of human feedback. Experiments also show the scalability of our approach.}
}
@article{CEREN202136,
title = {PALO bounds for reinforcement learning in partially observable stochastic games},
journal = {Neurocomputing},
volume = {420},
pages = {36-56},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.054},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220313345},
author = {Roi Ceren and Keyang He and Prashant Doshi and Bikramjit Banerjee},
keywords = {Multiagent systems, Reinforcement learning, POMDP, POSG},
abstract = {A partially observable stochastic game (POSG) is a general model for multiagent decision making under uncertainty. Perkins’ Monte Carlo exploring starts for partially observable Markov decision process (POMDP) (MCES-P) integrates Monte Carlo exploring starts (MCES) into a local search of the policy space to offer an elegant template for model-free reinforcement learning in POSGs. However, multiagent reinforcement learning in POSGs is tremendously more complex than in single agent settings due to the heterogeneity of agents and discrepancy of their goals. In this article, we generalize reinforcement learning under partial observability to self-interested and cooperative multiagent settings under the POSG umbrella. We present three new templates for multiagent reinforcement learning in POSGs. MCES for interactive POMDP (MCESIP) extends MCES-P by maintaining predictions of the other agent’s actions based on dynamic beliefs over models. MCES for multiagent POMDP (MCES-MP) generalizes MCES-P to the canonical multiagent POMDP framework, with a single policy mapping joint observations of all agents to joint actions. Finally, MCES for factored-reward multiagent POMDP (MCES-FMP) has each agent individually mapping joint observations to their own action. We use probabilistic approximate locally optimal (PALO) bounds to analyze sample complexity, thereby instantiating these templates to PALO learning. We promote sample efficiency by including a policy space pruning technique and evaluate the approaches on six benchmark domains as well as compare with the state-of-the-art techniques, which demonstrates that MCES-IP and MCES-FMP yield improved policies with fewer samples compared to the previous baselines.}
}
@article{GONZALEZREDIN201615,
title = {Spatial Bayesian belief networks as a planning decision tool for mapping ecosystem services trade-offs on forested landscapes},
journal = {Environmental Research},
volume = {144},
pages = {15-26},
year = {2016},
note = {The Provision of Ecosystem Services in Response to Global Change},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2015.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0013935115301377},
author = {Julen Gonzalez-Redin and Sandra Luque and Laura Poggio and Ron Smith and Alessandro Gimona},
keywords = {Spatial Bayesian belief networks, Ecosystem services, Trade-offs, Spatial planning, Sustainable forest management, Biodiversity},
abstract = {An integrated methodology, based on linking Bayesian belief networks (BBN) with GIS, is proposed for combining available evidence to help forest managers evaluate implications and trade-offs between forest production and conservation measures to preserve biodiversity in forested habitats. A Bayesian belief network is a probabilistic graphical model that represents variables and their dependencies through specifying probabilistic relationships. In spatially explicit decision problems where it is difficult to choose appropriate combinations of interventions, the proposed integration of a BBN with GIS helped to facilitate shared understanding of the human–landscape relationships, while fostering collective management that can be incorporated into landscape planning processes. Trades-offs become more and more relevant in these landscape contexts where the participation of many and varied stakeholder groups is indispensable. With these challenges in mind, our integrated approach incorporates GIS-based data with expert knowledge to consider two different land use interests – biodiversity value for conservation and timber production potential – with the focus on a complex mountain landscape in the French Alps. The spatial models produced provided different alternatives of suitable sites that can be used by policy makers in order to support conservation priorities while addressing management options. The approach provided provide a common reasoning language among different experts from different backgrounds while helped to identify spatially explicit conflictive areas}
}
@article{LI2024118444,
title = {Risk assessment of maritime autonomous surface ships collisions using an FTA-FBN model},
journal = {Ocean Engineering},
volume = {309},
pages = {118444},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118444},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824017827},
author = {Pengchang Li and Yuhong Wang and Zaili Yang},
keywords = {Maritime autonomous surface ships, Maritime risk, Bayesian network, Fault tree analysis, Fuzzy theory},
abstract = {Maritime autonomous surface ships (MASS), presenting the future of maritime transport, are attracting increasing attention from the international maritime community. The collision risk analysis of MASS reveals unsolved challenges, which without appropriate solutions, will result in the error prone development of the relevant risk control measures and policies. Among the challenges, two significant ones in the existing literature are the lack of historical failure data to realise quantitative risk assessment, and 2) the complex causal relationship among the relevant risk factors. This paper aims to develop a new Fault Tree Analysis-Fuzzy Bayesian Network (FTA-FBN) model to conduct the collision risk assessment of MASS with uncertainty in data. First, it establishes a causal relationship among the risk factors through an FTA. Secondly, mapping the obtained FTA diagram into a BN allows fault diagnosis and the identification of the most important factors influencing MASS collisions. In this process, a survey is conducted to collect the primary data for configuring the conditional probabilities of the relevant influential factors and quantifying the developed BN for risk diagnosis and prediction. Finally, the new model is verified by using sensitivity analysis and three axioms and then applied to conduct scenario-based risk prediction and diagnosis to generate insightful findings to guide MASS navigation safety. The results demonstrate that the FTA-FBN model realizes the simplification of the expert scoring process, reduces computational complexity, and addresses the challenge of constructing causal relationships between MASS collisions and their risk factors due to the scarcity of historical accident data. Additionally, the BN backward reasoning identifies key collision risks, including external physical attacks, inadequate training of shore-based operators, insufficient maintenance of ship equipment and systems, and cyber-security threats. The new model when being adapted, can provide a reference for the formulation of safe navigation policies and provide important insights for shipping companies to ensure the safe navigation of their ships and shipbuilders to optimise ship design.}
}
@article{QIN2022737802,
title = {Factors affecting marine ranching risk in China and their hierarchical relationships based on DEMATEL, ISM, and BN},
journal = {Aquaculture},
volume = {549},
pages = {737802},
year = {2022},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2021.737802},
url = {https://www.sciencedirect.com/science/article/pii/S0044848621014654},
author = {Man Qin and Xinru Wang and Yuanwei Du},
keywords = {Marine ranching, DEMATEL, ISM, Bayesian network, Risk analysis, Risk hierarchical relationships},
abstract = {Marine ranching have attracted wide attention in the marine fishery in recent years owing to numerous benefits for sustainable development. However, existing research efforts on marine ranching risk management are very limited, and no prior in-depth research has focused on studying the risk interdependencies in marine ranching from the perspectives of multiple risks. This paper begins by identifying 29 marine ranching risks using a systematic literature review and experts propose. Next, the cause-effect diagram and hierarchical network model of marine ranching are obtained using the combined DEMATEL - ISM method, which clearly confirms the interdependent relationships among various risk factors. It also identifies the most critical factors, which enables the operators of the marine ranching to make decisions to reduce risks. Then the hierarchical network model is mapped to a BN and expert judgments are further transformed into the conditional probability distribution by software Netica, in order to quantify the strength of the interdependent relationships among the marine ranching risk system, and determine main paths resulting in risk occurrence. According to the research results, the risk occurrence probability of marine ranching is high, with a probability of 85.1%，and the most approximate cause path leading to the occurrence of risk are determined—namely, X27(Long construction cycle) → X10(Financing difficulties) → X15(Safety hazards) → X11(Investment return risk) → Y(Marine ranching risk) and X9(Lack of technical talent) → X8(Monitoring failure) → X2(Habitat destruction) → X26(Large preinvestment) → X10 → X15 → X11(Investment return risk) → Y. Through the established risk analysis model, the key risk factors in the implementation process of marine ranching can be obtained, which is helpful for in-depth understanding of the risk information of marine ranching and more effective risk management of marine ranching.}
}
@article{CHEMWENO201619,
title = {Development of a novel methodology for root cause analysis and selection of maintenance strategy for a thermal power plant: A data exploration approach},
journal = {Engineering Failure Analysis},
volume = {66},
pages = {19-34},
year = {2016},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1350630716301248},
author = {Peter Chemweno and Ido Morag and Mohammad Sheikhalishahi and Liliane Pintelon and Peter Muchiri and James Wakiru},
keywords = {Root cause analysis, Data exploration, Principal component analysis, Cluster analysis, Maintenance strategy selection},
abstract = {Performing root cause analysis in technical systems is usually challenging owing to the complex failure associations which often exist between inter-connected system components. The recent adoption of maintenance management systems in industry has enhanced the collection of maintenance data which could assist practitioners derive meaningful failure associations embedded in the data. However, root cause analysis in the maintenance domain is dominated by the use of qualitative and semi-quantitative approaches. Such approaches, however, rely on expert elicitations whereof this elicitation process often introduces bias in the root cause analysis process. On the other hand, quantitative techniques for root cause analysis, for instance, fault trees and Bayesian networks are often limited to analyzing root causes in fairly simple systems. Moreover, the quantitative techniques seldom model the failure dependencies linked to the empirical failure events. Hence, to address these challenges, a novel data exploration methodology for root cause analysis is proposed which consists of four steps: 1) data collection and standardization step; 2) data exploration framework incorporating multivariate and cluster analysis; 3) causal mapping; and 4) maintenance strategy selection. The methodology is demonstrated in the application case of thermal power maintenance data. Moreover, the methodology is compared with two conventional qualitative root cause analysis techniques – Ishikawa cause-and-effect diagram, and the ‘5-whys’ analysis. A detailed discussion is presented whereof the added value of the methodology for maintenance decision support is demonstrated.}
}
@article{DRENT2020583,
title = {Dynamic dispatching and repositioning policies for fast-response service networks},
journal = {European Journal of Operational Research},
volume = {285},
number = {2},
pages = {583-598},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720301326},
author = {Collin Drent and Minou Olde Keizer and Geert-Jan van Houtum},
keywords = {Logistics, Service engineers, Maintenance, Dynamic policies, Markov decision process},
abstract = {We address the problem of dispatching and pro-actively repositioning service resources in service networks such that fast responses to service requests are realized in a cost-efficient way. By formulating this problem as a Markov decision process, we are able to investigate the structure of the optimal policy in the application domain of service logistics. Using these insights, we then propose scalable dynamic heuristics for both the dispatching and repositioning sub-problem, based on the minimum weighted bipartite matching problem and the maximum expected covering location problem, respectively. The dynamic dispatching heuristic takes into account real-time information about both the state of equipment and the fleet of service engineers, while the dynamic repositioning heuristic maximizes the expected weighted coverage of future service requests. In a test bed with a small network, we show that our most advanced heuristic performs well with an average optimality gap of 4.3% for symmetric instances and 5.8% for asymmetric instances. To show the practical value of our proposed heuristics, extensive numerical experiments are conducted on a large test bed with service logistics networks of real-life size where significant savings of up to 56% compared to a state-of-the-art benchmark policy are attained.}
}
@article{CELEN2020302,
title = {Integrated maintenance and operations decision making with imperfect degradation state observations},
journal = {Journal of Manufacturing Systems},
volume = {55},
pages = {302-316},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300431},
author = {Merve Celen and Dragan Djurdjanovic},
keywords = {Maintenance decision making, Product sequencing, Concurrent maintenance & operation decision making, POMDP, Imperfect observations},
abstract = {In highly flexible and integrated manufacturing systems, such as semiconductor fabs, strong interactions between the equipment condition, operations executed on the various machines and the outgoing product quality necessitate integrated decision making in the domains of maintenance scheduling and production operations. Furthermore, in highly complex manufacturing equipment, the underlying condition is not directly observable and can only be inferred probabilistically from the available sensor readings. In order to deal with interactions between maintenance and production operations in Flexible Manufacturing Systems (FMSs) in which equipment conditions are not perfectly observable, we propose in this paper a decision-making method based on a Partially Observable Markov Decision Processes (POMDP's), yielding an integrated policy in the realms of maintenance scheduling and production sequencing. Optimization was pursued using a metaheuristic method that used the results of discrete-event simulations of the underlying manufacturing system. The new approach is demonstrated in simulations of a generic semiconductor manufacturing cluster tool. The results showed that, regardless of uncertainties in the knowledge of actual equipment conditions, jointly making maintenance and production sequencing decisions consistently outperforms the current practice of making these decisions separately.}
}
@article{ALI2024114007,
title = {Spatial bayesian approach for socio-economic assessment of pumped hydro storage},
journal = {Renewable and Sustainable Energy Reviews},
volume = {189},
pages = {114007},
year = {2024},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.114007},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123008651},
author = {Shahid Ali and Rodney A. Stewart and Oz Sahin and Abel Silva Vieira},
keywords = {Bayesian network, GIS, PHES, Renewable energy site selection, Australia},
abstract = {Pumped hydro energy storage (PHES) is a key enabler for transitioning to 100 % renewable energy sources. However, PHES site selection is multi-faceted and challenging, including from a socio-economic perspective, due to complex economic issues and the heterogeneity of social factors. To overcome this challenge, this study developed a spatial probabilistic decision-making approach to identify and rank the best PHES sites from a shortlisted number of sites determined from a previous techno-environmental assessment. This model was developed in a participatory environment, where both study evidence and experts’ judgements were utilised to determine the key socio-economic factors, develop the Bayesian Network (BN), and populate the conditional probability tables (CPT). The BN parent node data were retrieved from publicly available sources in a high-resolution spatial and statistical format. An ArcGIS 10.3 tool was utilised to process the data with spatial characteristics, whereas GeNIe 2.5 was used to develop the BN model. Forward propagation, sensitivity, and strength analysis were performed to validate the developed BN model. Application of the spatial-BN model to northern Queensland, Australia reduced the fourteen previously determined techno-environmentally suitable PHES sites, to nine sites that are suitable from a socio-economic perspective for future developments. These nine sites could store and generate over 323 TWh of electricity over their life expectancy and at a levelised cost of 0.040–0.274 AU$/kWh. The developed procedure and model streamline the PHES site selection pre-feasibility process, thereby expediting renewable energy transition.}
}
@article{YU2025103134,
title = {Interpretable State-Space Model of Urban Dynamics for Human-Machine Collaborative Transportation Planning},
journal = {Transportation Research Part B: Methodological},
volume = {192},
pages = {103134},
year = {2025},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2024.103134},
url = {https://www.sciencedirect.com/science/article/pii/S0191261524002583},
author = {Jiangbo Yu and Michael F. Hyland},
keywords = {Smart Planning, Human-AI Teaming, Markov Decision Process, Sketch Planning, Machine Learning, Multicriteria, Land Use, Travel Demand, Regional Growth, Reinforcement Learning},
abstract = {Strategic Long-Range Transportation Planning (SLRTP) is pivotal in shaping prosperous, sustainable, and resilient urban futures. Existing SLRTP decision support tools predominantly serve forecasting and evaluative functions, leaving a gap in directly recommending optimal planning decisions. To bridge this gap, we propose an Interpretable State-Space Model (ISSM) that considers the dynamic interactions between transportation infrastructure and the broader urban system. The ISSM directly facilitates the development of optimal controllers and reinforcement learning (RL) agents for optimizing infrastructure investments and urban policies while still allowing human-user comprehension. We carefully examine the mathematical properties of our ISSM; specifically, we present the conditions under which our proposed ISSM is Markovian, and a unique and stable solution exists. Then, we apply an ISSM instance to a case study of the San Diego region of California, where a partially observable ISSM represents the urban environment. We also propose and train a Deep RL agent using the ISSM instance representing San Diego. The results show that the proposed ISSM approach, along with the well-trained RL agent, captures the impacts of coordinating the timing of infrastructure investments, environmental impact fees for new land development, and congestion pricing fees. The results also show that the proposed approach facilitates the development of prescriptive capabilities in SLRTP to foster economic growth and limit induced vehicle travel. We view the proposed ISSM approach as a substantial contribution that supports the use of artificial intelligence in urban planning, a domain where planning agencies need rigorous, transparent, and explainable models to justify their actions.}
}
@article{VANGESTEL20112542,
title = {Probabilistic gait classification in children with cerebral palsy: A Bayesian approach},
journal = {Research in Developmental Disabilities},
volume = {32},
number = {6},
pages = {2542-2552},
year = {2011},
issn = {0891-4222},
doi = {https://doi.org/10.1016/j.ridd.2011.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0891422211002605},
author = {Leen {Van Gestel} and Tinne {De Laet} and Enrico {Di Lello} and Herman Bruyninckx and Guy Molenaers and Anja {Van Campenhout} and Erwin Aertbeliën and Mike Schwartz and Hans Wambacq and Paul {De Cock} and Kaat Desloovere},
keywords = {Cerebral palsy, Machine learning, 3D gait analysis, Walking, Psychomotor development, Classification, Bayesian, Children, Probabilistic},
abstract = {Three-dimensional gait analysis (3DGA) generates a wealth of highly variable data. Gait classifications help to reduce, simplify and interpret this vast amount of 3DGA data and thereby assist and facilitate clinical decision making in the treatment of CP. CP gait is often a mix of several clinically accepted distinct gait patterns. Therefore, there is a need for a classification which characterizes each CP gait by different degrees of membership for several gait patterns, which are considered by clinical experts to be highly relevant. In this respect, this paper introduces Bayesian networks (BN) as a new approach for classification of 3DGA data of the ankle and knee in children with CP. A BN is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph. Furthermore, they provide an explicit way of introducing clinical expertise as prior knowledge to guide the BN in its analysis of the data and the underlying clinically relevant relationships. BNs also enable to classify gait on a continuum of patterns, as their outcome consists of a set of probabilistic membership values for different clinically accepted patterns. A group of 139 patients with CP was recruited and divided into a training- (n=80% of all patients) and a validation-dataset (n=20% of all patients). An average classification accuracy of 88.4% was reached. The BN of this study achieved promising accuracy rates and was found to be successful for classifying ankle and knee joint motion on a continuum of different clinically relevant gait patterns.}
}
@incollection{KAMBHAMPATI1993397,
title = {CHAPTER 12 - Supporting Flexible Plan Reuse},
editor = {Steven Minton},
booktitle = {Machine Learning Methods for Planning},
publisher = {Morgan Kaufmann},
pages = {397-434},
year = {1993},
isbn = {978-1-4832-0774-2},
doi = {https://doi.org/10.1016/B978-1-4832-0774-2.50017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781483207742500172},
author = {SUBBARAO KAMBHAMPATI},
abstract = {Publisher Summary
This chapter discusses a framework called PRIAR that effectively supports retrieval and modification of plans for reuse. PRIAR enables a hierarchical nonlinear planner to improve its planning by reusing previously generated plans. PRIAR uses the causal structures of plans to judge the appropriateness of reusing them in new problem situations and to modify them minimally to make them solve new problems. PRIAR provides an integrated framework for supporting reuse within hierarchical nonlinear planning. Hierarchical nonlinear planning is a dominant method of abstraction and least commitment in domain-independent planning. In this approach, plans are represented as partially ordered network of tasks at various levels of abstraction. PRIAR facilitates the modification of a hierarchical plan by justifying the individual decisions underlying its development in terms of its causal dependency structure. PRIAR also provides a domain-independent similarity metric based on the plan validation structure to measure the utility of modifying a given plan to solve a new problem. The key insight used in developing the similarity metric is that the modification cost depends on the number and types of inconsistencies that are caused by the specifications changes in the plan validation structure.}
}
@article{ABBAS2024102240,
title = {Hierarchical framework for interpretable and specialized deep reinforcement learning-based predictive maintenance},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102240},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102240},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001003},
author = {Ammar N. Abbas and Georgios C. Chasparis and John D. Kelleher},
keywords = {Deep reinforcement learning, Probabilistic modeling, Input–output hidden Markov model, Predictive maintenance, Industry 5.0, Interpretable reinforcement learning},
abstract = {Deep reinforcement learning holds significant potential for application in industrial decision-making, offering a promising alternative to traditional physical models. However, its black-box learning approach presents challenges for real-world and safety-critical systems, as it lacks interpretability and explanations for the derived actions. Moreover, a key research question in deep reinforcement learning is how to focus policy learning on critical decisions within sparse domains. This paper introduces a novel approach that combines probabilistic modeling and reinforcement learning, providing interpretability and addressing these challenges in the context of safety-critical predictive maintenance. The methodology is activated in specific situations identified through the input–output hidden Markov model, such as critical conditions or near-failure scenarios. To mitigate the challenges associated with deep reinforcement learning in safety-critical predictive maintenance, the approach is initialized with a baseline policy using behavioral cloning, requiring minimal interactions with the environment. The effectiveness of this framework is demonstrated through a case study on predictive maintenance for turbofan engines, outperforming previous approaches and baselines, while also providing the added benefit of interpretability. Importantly, while the framework is applied to a specific use case, this paper aims to present a general methodology that can be applied to diverse predictive maintenance applications.}
}
@article{UMAR2001719,
title = {A Markov process in clinical dentistry: the prosthodontic cycle},
journal = {International Congress Series},
volume = {1230},
pages = {719-725},
year = {2001},
note = {Computer Assisted Radiology and Surgery},
issn = {0531-5131},
doi = {https://doi.org/10.1016/S0531-5131(01)00120-0},
url = {https://www.sciencedirect.com/science/article/pii/S0531513101001200},
author = {H. Umar},
keywords = {Markov models, Markov cycle, Prosthodontic cycle, Tunnel states, Probability tree, Bayesian networks},
abstract = {Clinical dentistry offers today a wide range of different therapy concepts, based on multiple factors observed before, during and after dental treatment under varying assumptions particularly important in the prosthodontic domain. Here, the decision-making process is directly linked with extra chair side sessions, revision of existing denture, another type of denture, and finally with rapidly increasing costs. Since this involves risk that is continuous over time, where the timing is important, and where these events happen more than once, conventional decision trees are difficult and may require unrealistic simplifying assumptions. However, Markov models assume that a patient is always in one of a finite number of discrete dental health states, also referred to as Markov states. This paper describes work in progress on the development of a probabilistic causal model for prosthodontic treatment, and is intended for use both in clinical practice and dental training. The network, and particularly its numerical parameters, are based on existing clinical data and on human expert opinion. The Markov model with its state diagram, the probability tree, and the complete Markov cycle tree of the prosthodontic cycle are presented.}
}
@article{MAGNAN201763,
title = {Efficient incremental planning and learning with multi-valued decision diagrams},
journal = {Journal of Applied Logic},
volume = {22},
pages = {63-90},
year = {2017},
note = {SI:Uncertain Reasoning},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2016.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S157086831630091X},
author = {Jean-Christophe Magnan and Pierre-Henri Wuillemin},
abstract = {In the domain of decision theoretic planning, the factored framework (Factored Markov Decision Process, fmdp) has produced optimized algorithms using structured representations such as Decision Trees (Structured Value Iteration (svi), Structured Policy Iteration (spi)) or Algebraic Decision Diagrams (Stochastic Planning Using Decision Diagrams (spudd)). Since it may be difficult to elaborate the factored models used by these algorithms, the architecture sdyna, which combines learning and planning algorithms using structured representations, was introduced. However, the state-of-the-art algorithms for incremental learning, for structured decision theoretic planning or for reinforcement learning require the problem to be specified only with binary variables and/or use data structures that can be improved in term of compactness. In this paper, we propose to use Multi-Valued Decision Diagrams (mdds) as a more efficient data structure for the sdyna architecture and describe a planning algorithm and an incremental learning algorithm dedicated to this new structured representation. For both planning and learning algorithms, we experimentally show that they allow significant improvements in time, in compactness of the computed policy and of the learned model. We then analyzed the combination of these two algorithms in an efficient sdyna instance for simultaneous learning and planning using mdds.}
}
@article{ZERVOUDAKIS2021100018,
title = {OpinionMine: A Bayesian-based framework for opinion mining using Twitter Data},
journal = {Machine Learning with Applications},
volume = {3},
pages = {100018},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2020.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2666827020300189},
author = {Stefanos Zervoudakis and Emmanouil Marakakis and Haridimos Kondylakis and Stefanos Goumas},
keywords = {Statistical Relational Learning, Probabilistic rules, Bayesian reasoning, Twitter Data, Incremental learning},
abstract = {This article studies opinion mining from social media with probabilistic logic reasoning. As it is known, Twitter is one of the most active social networks, with millions of tweets sent daily, where multiple users express their opinion about traveling, economic issues, political decisions etc. As such, it offers a valuable source of information for opinion mining. In this paper we present OpinionMine, a Bayesian-based framework for opinion mining, exploiting Twitter Data. Initially, our framework imports Tweets massively by using Twitter’s API. Next, the imported Tweets are further processed automatically for constructing a set of untrained rules and random variables. Then, a Bayesian Network is derived by using the set of untrained rules, the random variables and an evidence set. After that, the trained model can be used for the evaluation of new Tweets. Finally, the constructed model can be retrained incrementally, thus becoming more robust. As application domain for the development of our methodology we have selected tourism because it is one of the most popular topics in social media. Our framework can predict users’ intention to visit a place. Among the advantages of our framework is that it follows an incremental learning strategy. That is, the derived model can be retrained incrementally with new training sets thus becoming more robust. Further, our framework can be easily adapted to opinion mining from social media on other topics, whereas the rules of the derived model are constructed in an efficient way and automatically.}
}
@article{BUYS2014184,
title = {Creating a Sustainability Scorecard as a predictive tool for measuring the complex social, economic and environmental impacts of industries, a case study: Assessing the viability and sustainability of the dairy industry},
journal = {Journal of Environmental Management},
volume = {133},
pages = {184-192},
year = {2014},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2013.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0301479713007548},
author = {L. Buys and K. Mengersen and S. Johnson and N. {van Buuren} and A. Chauvin},
keywords = {Sustainability, Triple bottom line, Environmental management, Bayesian network modelling, , Sustainable industry},
abstract = {Sustainability is a key driver for decisions in the management and future development of industries. The World Commission on Environment and Development (WCED, 1987) outlined imperatives which need to be met for environmental, economic and social sustainability. Development of strategies for measuring and improving sustainability in and across these domains, however, has been hindered by intense debate between advocates for one approach fearing that efforts by those who advocate for another could have unintended adverse impacts. Studies attempting to compare the sustainability performance of countries and industries have also found ratings of performance quite variable depending on the sustainability indices used. Quantifying and comparing the sustainability of industries across the triple bottom line of economy, environment and social impact continues to be problematic. Using the Australian dairy industry as a case study, a Sustainability Scorecard, developed as a Bayesian network model, is proposed as an adaptable tool to enable informed assessment, dialogue and negotiation of strategies at a global level as well as being suitable for developing local solutions.}
}
@article{BONNEAU201430,
title = {Reinforcement learning-based design of sampling policies under cost constraints in Markov random fields: Application to weed map reconstruction},
journal = {Computational Statistics & Data Analysis},
volume = {72},
pages = {30-44},
year = {2014},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2013.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167947313003551},
author = {Mathieu Bonneau and Sabrina Gaba and Nathalie Peyrard and Régis Sabbadin},
keywords = {Sampling design, Markov decision process, Dynamic programming, Gibbs sampling, Least-squares linear regression, Weed mapping},
abstract = {Weeds are responsible for yield losses in arable fields, whereas the role of weeds in agro-ecosystem food webs and in providing ecological services has been well established. Innovative weed management policies have to be designed to handle this trade-off between production and regulation services. As a consequence, there has been a growing interest in the study of the spatial distribution of weeds in crops, as a prerequisite to management. Such studies are usually based on maps of weed species. The issues involved in building probabilistic models of spatial processes as well as plausible maps of the process on the basis of models and observed data are frequently encountered and important. As important is the question of designing optimal sampling policies that make it possible to build maps of high probability when the model is known. This optimization problem is more complex to solve than the pure reconstruction problem and cannot generally be solved exactly. A generic approach to spatial sampling for optimizing map construction, based on Markov Random Fields (MRF), is provided and applied to the problem of weed sampling for mapping. MRF offer a powerful representation for reasoning on large sets of random variables in interaction. In the field of spatial statistics, the design of sampling policies has been largely studied in the case of continuous variables, using tools from the geostatistics domain. In the MRF case with finite state space variables, some heuristics have been proposed for the design problem but no universally accepted solution exists, particularly when considering adaptive policies as opposed to static ones. The problem of designing an adaptive sampling policy in an MRF can be formalized as an optimization problem. By combining tools from the fields of Artificial Intelligence (AI) and Computational Statistics, an original algorithm is then proposed for approximate resolution. This generic procedure, referred to as Least-Squares Dynamic Programming (LSDP), combines an approximation of the value of a sampling policy based on a linear regression, the construction of a batch of MRF realizations and a backwards induction algorithm. Based on an empirical comparison of the performance of LSDP with existing one-step-look-ahead sampling heuristics and solutions provided by classical AI algorithms, the following conclusions can be derived: (i) a naïve heuristic consisting of sampling sites where marginals are the most uncertain is already an efficient sampling approach; (ii) LSDP outperforms all the classical approaches we have tested; and (iii) LSDP outperforms the naïve heuristic approach in cases where sampling costs are not uniform over the set of variables or where sampling actions are constrained.}
}
@article{CLEMPNER2019108587,
title = {Observer and control design in partially observable finite Markov chains},
journal = {Automatica},
volume = {110},
pages = {108587},
year = {2019},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2019.108587},
url = {https://www.sciencedirect.com/science/article/pii/S0005109819304480},
author = {Julio B. Clempner and Alexander S. Poznyak},
keywords = {Observer design, Partially observed, Optimal control, Markov chains},
abstract = {The controllable Partially Observable Markov Decision Process (POMDP) framework has proven to be useful in different domains where one is constrained to provide incomplete information of the structure and the parameters of the problem. Sometimes, it is not easy to track and measure accurately some state variables, and it may be more effective to make decisions based on imprecise information. This paper is focused on the design of an observer (which is unknown) for a class of ergodic homogeneous finite Markov chains with partially observable states. The main goal of the proposed method is the derivation of formulas for computing an observer, and as a result, on optimal control policy. For solving the problem, we introduce a new variable, which involves the product of the policy, the observation kernel and the distribution vector. We derive the formulas to recover the variables of interest. This work considers a dynamic environment for learning the parameters of the POMDP model. The construction of the adaptive policies is based on an identification approach, where we estimate the elements of the transition matrices and utility matrices by counting the number of unobserved experiences. A numerical example is presented to illustrate the practical implications of the theoretical issues applied to a portfolio optimization problem. These findings are important and new to the literature.}
}
@article{BAKHSHI2023109611,
title = {Multi-provider NFV network service delegation via average reward reinforcement learning},
journal = {Computer Networks},
volume = {224},
pages = {109611},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109611},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623000567},
author = {Bahador Bakhshi and Josep Mangues-Bafalluy and Jorge Baranda},
keywords = {Multi-provider service delegation, Admission control, MDP, Average reward RL, Dynamic programming},
abstract = {In multi-provider 5G/6G networks, service delegation enables administrative domains to federate in provisioning NFV network services. Admission control in selecting the appropriate domain for service deployment, without prior knowledge of service requests’ statistical distributions, is fundamental to maximize average profit. This paper analyzes a general federation contract model for service delegation in various ways. First, under the assumption of known system dynamics, we obtain the theoretically optimal performance bound by formulating the admission control problem as an infinite-horizon Markov decision process (MDP) and solving it through dynamic programming, which is used as a benchmark to evaluate practical solutions. Second, we apply Reinforcement Learning (RL) to practically tackle the problem when the arrival and departure rates are not known. For the first time in this context, we analyze the performance of the widely used Q-Learning algorithm, and prove as it maximizes the discounted rewards, it is not an efficient solution due to its sensitivity to the discount factor. Then, we propose the average reward reinforcement learning approach (named “R-Learning”) to find the policy that directly maximizes the average profit. Finally, we evaluate different solutions through extensive simulations and experimentally using the 5Growth management and orchestration platform. Results confirm that the proposed R-Learning solution always outperforms Q-Learning and the greedy policies. Furthermore, while there is at most a 9% optimality gap in the ideal simulation environment, it competes with the MDP solution in the experimental assessment.}
}
@article{SUGISAWA2022477,
title = {Machining sequence learning via inverse reinforcement learning},
journal = {Precision Engineering},
volume = {73},
pages = {477-487},
year = {2022},
issn = {0141-6359},
doi = {https://doi.org/10.1016/j.precisioneng.2021.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S0141635921002440},
author = {Yasutomo Sugisawa and Keigo Takasugi and Naoki Asakawa},
keywords = {Machining sequence decision, Inverse reinforcement learning, Neural network, Graph representation},
abstract = {In recent years, process planning automation has been strongly promoted in conjunction with the development of information technology (IT). In manufacturing industries, machining sequencing is one of the elements of computer-aided process planning (CAPP) systems where it has a significant impact on the quality and cost of machined components. Therefore, effective and robust planning rules are essential for practical CAPP systems, and various metrics and constraints have been proposed to facilitate the creation of those rules. However, since it is challenging to address explicit factors such as interference between rules, processing difficulties, and manageability, discrepancies that require manual corrections often arise between the generated sequences and the planner's intentions. To resolve this problem, we propose a method of acquiring rules that reproduce the planner's decisions by inverse reinforcement learning (IRL). To apply the IRL process, we focus on identifying a machining sequence that characterizes the planner's decision based on past production processes and interviews with experts. This machining sequence can then be represented using a Markov decision process (MDP) when changing the workpiece shape, which enables the application of IRL. Additionally, to reflect the drawing information in the sequence decision, the workpiece shape is represented as a graph with attached tolerance and roughness values. The graphed machining sequence is then inputted to the graph, where convolutional networking and training are performed. We verified the validity of our proposed method using a small dataset.}
}
@article{BUGALIA2021105368,
title = {A system dynamics model for near-miss reporting in complex systems},
journal = {Safety Science},
volume = {142},
pages = {105368},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105368},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521002125},
author = {Nikhil Bugalia and Yu Maemura and Kazumasa Ozawa},
keywords = {Near-miss reporting, System Dynamics, High-Speed Rail, Construction, Worker’s burnout, Safety},
abstract = {For organizations managing ultra-safe complex systems, near-miss reports provide a valuable opportunity to improve safety. Recent academic studies have emphasized the necessity of considering the interactions among several factors influencing the reporting behavior of employees. This study develops a model of near-miss reporting behavior that considers interactions among factors such as an employee's individual characteristics and decisions by organizational management. The current study adopts a System Dynamics (SD) methodology for model development and provides an example of how modelling can be used as an organizational policy tool. A cross-industry literature review is used to develop the causal structure of the model. To evaluate the model’s generalizability, the causal structure is then verified with expert interviews from two different systems, the High-Speed Railway and the construction industry. Factors common across the two near-miss reporting systems are the worker’s fatigue and a positive utility of reporting. The causal structure is then converted to a simulation SD model and is calibrated to simulate the trends in the number of near-miss reports with unique data on good-observations obtained from the construction industry. The simulations were able to capture the trade-off between the number of near-miss reports and working hours. Such management policy simulations are particularly relevant for ultra-safe complex systems, and help emphasize how systems will continue to face safety-related trade-offs despite overcoming the more commonly reported trade-offs associated with tangible production losses from incidents. The discussion emphasizes that management policies to influence worker fatigue must adequately consider its current system state.}
}
@article{UCHIBE2021138,
title = {Forward and inverse reinforcement learning sharing network weights and hyperparameters},
journal = {Neural Networks},
volume = {144},
pages = {138-153},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003221},
author = {Eiji Uchibe and Kenji Doya},
keywords = {Reinforcement learning, Inverse reinforcement learning, Imitation learning, Entropy regularization},
abstract = {This paper proposes model-free imitation learning named Entropy-Regularized Imitation Learning (ERIL) that minimizes the reverse Kullback–Leibler (KL) divergence. ERIL combines forward and inverse reinforcement learning (RL) under the framework of an entropy-regularized Markov decision process. An inverse RL step computes the log-ratio between two distributions by evaluating two binary discriminators. The first discriminator distinguishes the state generated by the forward RL step from the expert’s state. The second discriminator, which is structured by the theory of entropy regularization, distinguishes the state–action–next-state tuples generated by the learner from the expert ones. One notable feature is that the second discriminator shares hyperparameters with the forward RL, which can be used to control the discriminator’s ability. A forward RL step minimizes the reverse KL estimated by the inverse RL step. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy. Our experimental results on MuJoCo-simulated environments and vision-based reaching tasks with a robotic arm show that ERIL is more sample-efficient than the baseline methods. We apply the method to human behaviors that perform a pole-balancing task and describe how the estimated reward functions show how every subject achieves her goal.}
}
@article{WANG2022107598,
title = {FlotGAIL: An operational adjustment framework for flotation circuits using generative adversarial imitation learning},
journal = {Minerals Engineering},
volume = {183},
pages = {107598},
year = {2022},
issn = {0892-6875},
doi = {https://doi.org/10.1016/j.mineng.2022.107598},
url = {https://www.sciencedirect.com/science/article/pii/S0892687522002084},
author = {Xu Wang and Junwu Zhou and Tao Song and Daoxi Liu and Qingkai Wang},
keywords = {Operational adjustment, Generative adversarial imitation learning, Bayesian neural networks, Epistemic uncertainty, Flotation},
abstract = {In many industrial flotation processes, the operational adjustment is still manual according to individual experience. The adjustment of operational variables is mainly based on the discrimination of feed conditions and the observation of the appearance of flotation froth. Due to the limitation that partial properties of feed ore cannot be monitored online and the individual-level differences in operation experience, it is difficult to control the concentrate grade and recovery in the qualified range for a long time by manual operational adjustment. To resolve this problem, we propose FlotGAIL, an automatic operational adjustment framework based on generative adversarial imitation learning (GAIL) for the flotation circuits. From the perspective of an algorithm, this paper explores a novel way to learn operational adjustment policy. The epistemic uncertainty estimation of feed conditions provided by Bayesian neural networks is used as an additional guidance for the GAIL model to generate flotation operational trajectories, which it is just as an expert considers reducing the risk of process operation and increasing long-term rewards. In FlotGAIL, learning flotation operational adjustment from expert demonstrations is formulated as an imitation learning problem in a Markov decision process, and then the proposed model uses the signal provided by the discriminator as the reward function to guide the policy generator to complete the training process. From the perspective of engineering application, the experiments have verified that the proposed operational adjustment method has a better effect in controlling concentrate grade and recovery up to standard, and FlotGAIL can be used as an effective tool to provide decision support for operators of other similar industrial processes.}
}
@article{GOSWAMI2021114016,
title = {Decision modeling and analysis in new product development considering supply chain uncertainties: A multi-functional expert based approach},
journal = {Expert Systems with Applications},
volume = {166},
pages = {114016},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114016},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420307892},
author = {Mohit Goswami and Yash Daultani and Arijit De},
keywords = {Decision support systems, New Product Development, Supply chain risk management, Design concept selection},
abstract = {Successful new product development projects and extant research literature advocate for inclusion of inputs pertaining to the supply chain at early stages of product development to proactively identify risk averse product design concepts. To this end, we devise an analytical framework to converge upon product design concept(s) that would be associated with lesser supply chain risks, usually function of both technical and commercialization considerations. The high-level and constituent lower-level supply chain risks are represented by parent and root nodes respectively within the devised Bayesian network driven research framework. Thereafter, a quantitative measure denoted as SCRI (supply chain risk index) is evolved that yields overall composite risk numbers corresponding to respective design concepts at different risk states. Validation and comparison of the devised method with an extant study illustrates the consistency and reliability of the study. It is found that the risk propensity of a particular design concept is inversely related to the probabilistic utility of that particular concept. The case of a construction power tool of a global firm is used to demonstrate the methodology. Our research addresses an important future research pathway as argued by Hosseini et al. (2020) that extant research literature is devoid of decision-making frameworks focused on measurement and analysis the propagation of risks on complex networks.}
}
@article{DJEUMOU2023103856,
title = {Task-guided IRL in POMDPs that scales},
journal = {Artificial Intelligence},
volume = {317},
pages = {103856},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103856},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000024},
author = {Franck Djeumou and Christian Ellis and Murat Cubuktepe and Craig Lennon and Ufuk Topcu},
keywords = {Inverse reinforcement learning, Planning in partially observable environment, Sequential convex optimization},
abstract = {In inverse reinforcement learning (IRL), a learning agent infers a reward function encoding the underlying task using demonstrations from experts. However, many existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs). We address two limitations of existing IRL techniques. First, they require an excessive amount of data due to the information asymmetry between the expert and the learner. Second, most of these IRL techniques require solving the computationally intractable forward problem—computing an optimal policy given a reward function—in POMDPs. The developed algorithm reduces the information asymmetry while increasing the data efficiency by incorporating task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations. Further, the algorithm avoids a common source of algorithmic complexity by building on causal entropy as the measure of the likelihood of the demonstrations as opposed to entropy. Nevertheless, the resulting problem is nonconvex due to the so-called forward problem. We solve the intrinsic nonconvexity of the forward problem in a scalable manner through a sequential linear programming scheme that guarantees to converge to a locally optimal policy. In a series of examples, including experiments in a high-fidelity Unity simulator, we demonstrate that even with a limited amount of data and POMDPs with tens of thousands of states, our algorithm learns reward functions and policies that satisfy the task while inducing similar behavior to the expert by leveraging the provided side information.}
}
@article{WU2016451,
title = {Efficient solutions of interactive dynamic influence diagrams using model identification},
journal = {Neurocomputing},
volume = {216},
pages = {451-459},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.07.052},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216308244},
author = {He Wu and Jian Luo},
keywords = {Interactive dynamic influence diagram, Multi-agent dynamic decision making, Mutual information, Model identification},
abstract = {Interactive dynamic influence diagram (I-DID) is one of the graphical frameworks for sequential decision making in partially observable environment. Subject agent in I-DID maintains beliefs over not only physical states of the environment, but also over models of the other agents. Consequently, solving I-DIDs suffers from the exponential growth of models ascribed to the other agents over time. Previous methods to solve I-DIDs aim at clustering equivalent models by comparing the entire or partial policy trees of the candidate models, which is time-consuming. In this paper, we present a new method for further reducing the model space by identifying the true model of the other agent and pruning the other irrelevant models. Toward this, we use an information-theoretic method—mutual information to measure the relevance between the candidate models and the true model in terms of predicted and observed actions of the other agent. We construct a dynamic Bayesian network to learn the value of parameters needed in the computation of mutual information. This approach bounds the model space by containing only the true model of the other agent. We evaluate our approach on multiple problem domains and empirically demonstrate the efficiency in solving I-DIDs.}
}
@article{WANG2014951,
title = {Probabilistic risk assessment of tunneling-induced damage to existing properties},
journal = {Expert Systems with Applications},
volume = {41},
number = {4, Part 1},
pages = {951-961},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413004570},
author = {Fan Wang and L.Y. Ding and H.B. Luo and Peter E.D. Love},
keywords = {Probabilistic risk assessment, Bayesian networks, Relevance vector machine, Tunneling},
abstract = {There is an intrinsic risk associated with tunnel construction, particularly in urban areas where a number of third party persons and properties are involved. Due to the limited availability of data for accidents and the complexity associated with their causation, it is therefore necessary to combine available historical data and expert judgment to consider all relevant factors to undertake a realistic risk analysis. Thus, this paper presents a hybrid approach that can be used to undertake a probabilistic risk assessment of the risks associated with tunneling and its likelihood to damage to existing properties using the techniques of Bayesian Networks (BN) and a Relevance Vector Machine (RVM). A causal framework that integrates the techniques is also proposed to facilitate the development of the proposed model. The developed risk model is applied to a real tunnel construction project in Wuhan, China. The results derived from the project demonstrated the model’s ability to accurately assess risks during tunneling, specifically the identification of accident scenarios and the quantification of the probability and severity of possible accidents. The potential of this risk model to be used as a decision-making support tool was also explored.}
}
@article{KLANN201484,
title = {Decision support from local data: Creating adaptive order menus from past clinician behavior},
journal = {Journal of Biomedical Informatics},
volume = {48},
pages = {84-93},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2013.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046413001962},
author = {Jeffrey G. Klann and Peter Szolovits and Stephen M. Downs and Gunther Schadow},
keywords = {Clinical Decision Support, Data mining, Bayesian analysis},
abstract = {Objective
Reducing care variability through guidelines has significantly benefited patients. Nonetheless, guideline-based Clinical Decision Support (CDS) systems are not widely implemented or used, are frequently out-of-date, and cannot address complex care for which guidelines do not exist. Here, we develop and evaluate a complementary approach – using Bayesian Network (BN) learning to generate adaptive, context-specific treatment menus based on local order-entry data. These menus can be used as a draft for expert review, in order to minimize development time for local decision support content. This is in keeping with the vision outlined in the US Health Information Technology Strategic Plan, which describes a healthcare system that learns from itself.
Materials and methods
We used the Greedy Equivalence Search algorithm to learn four 50-node domain-specific BNs from 11,344 encounters: abdominal pain in the emergency department, inpatient pregnancy, hypertension in the Urgent Visit Clinic, and altered mental state in the intensive care unit. We developed a system to produce situation-specific, rank-ordered treatment menus from these networks. We evaluated this system with a hospital-simulation methodology and computed Area Under the Receiver–Operator Curve (AUC) and average menu position at time of selection. We also compared this system with a similar association-rule-mining approach.
Results
A short order menu on average contained the next order (weighted average length 3.91–5.83 items). Overall predictive ability was good: average AUC above 0.9 for 25% of order types and overall average AUC .714–.844 (depending on domain). However, AUC had high variance (.50–.99). Higher AUC correlated with tighter clusters and more connections in the graphs, indicating importance of appropriate contextual data. Comparison with an Association Rule Mining approach showed similar performance for only the most common orders with dramatic divergence as orders are less frequent.
Discussion and conclusion
This study demonstrates that local clinical knowledge can be extracted from treatment data for decision support. This approach is appealing because: it reflects local standards; it uses data already being captured; and it produces human-readable treatment-diagnosis networks that could be curated by a human expert to reduce workload in developing localized CDS content. The BN methodology captured transitive associations and co-varying relationships, which existing approaches do not. It also performs better as orders become less frequent and require more context. This system is a step forward in harnessing local, empirical data to enhance decision support.}
}
@article{LIMANHAROU2021103014,
title = {Crop modelling in data-poor environments – A knowledge-informed probabilistic approach to appreciate risks and uncertainties in flood-based farming systems},
journal = {Agricultural Systems},
volume = {187},
pages = {103014},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103014},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308751},
author = {Issoufou {Liman Harou} and Cory Whitney and James Kung'u and Eike Luedeling},
keywords = {Complex systems, Crop model, Customized decision support tools, Limited information, Bayesian networks, Monte Carlo simulation},
abstract = {Crop models can support agricultural decisions, yet their reliability is necessarily limited when they do not sufficiently represent the complexity and specific circumstances of the target system. In some cases, models have such prohibitively high data requirements that they are only applicable with far-reaching and often questionable assumptions. In this paper, we demonstrate a customizable solution-oriented approach for crop modelling in situations where data and resources are limited. To address system complexity and produce a probabilistic crop model that does not depend on precise data, we used participatory analysis to describe system components using individual Bayesian networks that formalize expert knowledge into probabilistic causal relationships among important variables. We then used these Bayesian networks to generate inputs for a Monte Carlo model that illustrates the determinants of crop growth and simulates plausible ranges of expected grain and biomass yields at various stages of crop development. The resulting model accounts for all important variables and their interactions, as examined by local and foreign experts and described in relevant literature. We describe how to develop and customize such a model to specific situations based on case studies related to flood-based farming systems in Ethiopia and Kenya. The model assesses the performance of cropping systems and individual crops, and identifies factors of high importance for system outcomes. This approach to crop modelling paves the way for new opportunities to support agricultural decisions, since it does not require perfect information and can accommodate system complexity and uncertainty in data-poor environments.}
}
@article{FLORIN201381,
title = {Family farmers and biodiesel production: Systems thinking and multi-level decisions in Northern Minas Gerais, Brazil},
journal = {Agricultural Systems},
volume = {121},
pages = {81-95},
year = {2013},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2013.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X13000887},
author = {Madeleine J. Florin and Martin K. {van Ittersum} and Gerrie W.J. {van de Ven}},
keywords = {Bayesian network modelling, Biodiesel, Expert opinion, Family farming, Social inclusion, Systems thinking},
abstract = {This study focuses on family farmer engagement in the Brazilian national programme for Production and use of Biodiesel (PNPB). The Brazilian government has been promoting the role of family farmers as producers of biomass for biodiesel since 2004; however, fewer than expected family farmers have decided to produce biomass for biodiesel. The North of Minas Gerais is one region where a biodiesel plant has been strategically located to source castor beans grown by family farmers. The target family farm type in this region specializes in beef and/or dairy production with low input pasture (approximately 30ha per farm), maize intercropped with beans (approximately 1ha per farm) and sugarcane (approximately 1ha per farm). We selected this region for a case study to explore management decisions of farmers, industry and policy makers that influence family farmer engagement with biodiesel production through cultivation of castor beans. To evaluate outcomes for family farmers engaging with the PNPB, we focused on how cultivation of castor beans impacts family farmers in terms of income levels, income stability and levels of milk production. We used an application of systems thinking known as Bayesian network modelling (BNM). BNM was chosen for its suitability to integrate different types of knowledge and to include quantitative and qualitative variables. The study was built on a body of scientific literature explaining why family farmers have not been cultivating castor beans for biodiesel production and a body of experiential knowledge of local actors (farmers, extension officers, policy makers, biodiesel manufacturers and researchers in North of Minas Gerais). The complete BNM consisted of a ‘cause and effect’ diagram where the strengths of the causal relationships were quantified with elicited opinions from surveyed local actors. We used the complete BNM to explore scenarios that could improve outcomes for family farmers and consequently increase their level of engagement. For example, we addressed subsidy structures of the PNPB, crop management, farm-level trade-offs and value-chain innovations. We demonstrate that decisions to support family farmer engagement with biodiesel are not singular. Engagement by family farmers requires simultaneously: improvements in technical crop management, reductions in farm-level cash constraints and innovations in the production chain such that engagement of family farmers goes beyond cultivation of one more low-value crop. Finally we discuss some methodological issues from this application of BNM to farming systems research.}
}
@article{WARD20145653,
title = {A Bayesian Approach to Model-Development: Design of Continuous Distributions for Infection Variables},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {5653-5658},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02235},
url = {https://www.sciencedirect.com/science/article/pii/S147466701642495X},
author = {Logan Ward and Mads L. Mogensen and Mical Paul and Leonard Leibovici and Steen Andreassen},
keywords = {Model formulation, experiment design, Decision support and control, Quantification of physiological parameters for diagnosis and treatment assessment},
abstract = {Bayesian networks can be used to build models of diseases for diagnosis, and, if complemented by decision theory and utility functions, can also suggest treatments. This paper presents a development framework for such a network that has been used to model sepsis and in particular focuses on incorporating knowledge from the literature and databases as well as expert opinion into the model. Two parameters are presented as examples of the methods used, and the model is validated for a cohort of patients suspected of infection.}
}
@article{KREBSBACH20091347,
title = {Deliberation scheduling using GSMDPs in stochastic asynchronous domains},
journal = {International Journal of Approximate Reasoning},
volume = {50},
number = {9},
pages = {1347-1359},
year = {2009},
note = {Special Track on Uncertain Reasoning of the 19th International Florida Artificial Intelligence Research Symposium (FLAIRS 2006)},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2009.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X09000942},
author = {Kurt D. Krebsbach},
abstract = {We propose a new decision-theoretic approach for solving execution-time deliberation scheduling problems using recent advances in Generalized Semi-Markov Decision Processes (GSMDPs). In particular, we use GSMDPs to more accurately model domains in which planning and execution occur concurrently, plan improvement actions have uncertain effects and duration, and events (such as threats) occur asynchronously and stochastically. In this way, agents develop a continuous-time deliberation policy offline which can then be consulted to dynamically select deliberation-level and domain-level actions at plan execution-time. We demonstrate a significant improvement in expressibility over previous discrete-time approximate models in which mission phase duration was fixed, failure events were synchronized with phase transitions, and planning time was discretized into constant-sized planning quanta.}
}
@article{YILDIRIM2019151,
title = {Canadian Traveler Problem with Neutralizations},
journal = {Expert Systems with Applications},
volume = {132},
pages = {151-165},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419303082},
author = {S. Yildirim and V. Aksakalli and A.F. Alkaya},
keywords = {Autonomous navigation, Path planning, Canadian traveler problem, Markov decision process, AO* search, Reinforcement learning},
abstract = {The Canadian Traveler Problem (CTP) and the Obstacle Neutralization Problem (ONP) are two well-studied graph-theoretic path planning problems in the literature and both problems have been shown to be computationally intractable. In CTP, certain edges in a graph are blocked by a known probability and their status is revealed only when the traversing agent is at either end of these edges using the agent’s limited disambiguation capability. The goal is to minimize the expected length of the traversal between a starting and a termination vertex by devising a policy that dictates in real-time which edge to disambiguate. In ONP, an agent needs to safely and swiftly navigate from a given source location to a destination through an arrangement of obstacles in the plane. The agent has a limited neutralization capability and uses it to safely pass through an obstacle at a cost of increased traversal length. The agent’s goal is to find the sequence of obstacles to be neutralized en route which minimizes the overall traversal length subject to the agent’s limited neutralization capability. Both of these problems have important and practical applications within the context of expert and intelligent systems. These include: autonomous robot navigation, adaptive transportation systems, naval and land minefield countermeasures, and navigation inside disaster areas for emergency relief operations. In this study, we consider a new path planning problem in the simultaneous presence of disambiguation and neutralization capabilities. This appears to be the first of its kind in the literature despite the close and inherent relationship between CTP and ONP. We call this problem the Canadian Traveler Problem with Neutralizations (CTPN). We present a Markov decision process formulation of CTPN and propose an optimal algorithm. This is based on an extension of the well-known AO* search algorithm. We provide computational experiments on Delaunay graphs to assess the relative performance of this algorithm in comparison to the well-known value iteration and AO* algorithms. We then investigate the relative utility and importance of the disambiguation and neutralization capabilities in order to assist decision-makers with financial constraints as well as navigation performance decisions.}
}
@article{MITCHELL199141,
title = {Issues in the development and use of expert systems for marketing decisions},
journal = {International Journal of Research in Marketing},
volume = {8},
number = {1},
pages = {41-50},
year = {1991},
note = {Special Issue on Expert Systems in Marketing},
issn = {0167-8116},
doi = {https://doi.org/10.1016/0167-8116(91)90006-S},
url = {https://www.sciencedirect.com/science/article/pii/016781169190006S},
author = {Andrew A. Mitchell and J.Edward Russo and Dick R. Wittink},
abstract = {We discuss the distinguishing advantages and disadvantages of using human judgement, an expert system or a statistical/optimization model for marketing decisions. We question the inherent assumption that an expert system can and should replace the human expert. Experts' incomplete knowledge, the loose causal structures in marketing, and the need for world knowledge suggest that it will prove more useful to design system as collaborators in managerial decisions.}
}
@article{PERKUSICH2020106241,
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {119},
pages = {106241},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106241},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302587},
author = {Mirko Perkusich and Lenardo {Chaves e Silva} and Alexandre Costa and Felipe Ramos and Renata Saraiva and Arthur Freire and Ednaldo Dilorenzo and Emanuel Dantas and Danilo Santos and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence},
abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}
@article{ORELLANA2011231,
title = {SAIFA: A web-based system for Integrated Production of olive cultivation},
journal = {Computers and Electronics in Agriculture},
volume = {78},
number = {2},
pages = {231-237},
year = {2011},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2011.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0168169911001773},
author = {Francisco Javier Orellana and José {del Sagrado} and Isabel María {del Águila}},
keywords = {Olive crop, Expert system, Integrated Production, Pest control, Georreferentiation},
abstract = {One of the essential requirements of current agricultural practice is the need to evolve towards crop techniques that make better use of productive factors behaving in sustainable and environmentally respectful ways. Pest control is one of the most important problems to take into account, due to the significant production losses that pests may cause. The Integrated Production for olive crop defines a set of rules that has to be followed in order to ensure a production of higher quality, and promotes an environmentally respectful model of Olive cultivation. This work presents SAIFA (spanish acronym for Sistema de Alerta e Información Fitosanitaria Andaluz – Andalusian Phitosanitary Information and Alert System) a web-based information system which allows monitoring the Integrated Production for the olive crop in Andalusia, Spain. SAIFA has been built as a tool to assist agricultural technicians to comply with the quality standards of Integrated Production, and to help decision makers to choose the actions to be performed in the crop. Also, it assists coordinators to choose an Integrated Production strategy applicable for the whole region with the main objective of ensuring the safety of crops and reporting the current phitosanitary state to the authorities.}
}
@article{GARCES20161104,
title = {New product acceptability evaluation and improvement model with knowledge reuse**This work is financed by a grant from the French Ministry of Higher Education and Research (Ministere de l’Enseignement superieur et de 1a Recherche, MESR).},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1104-1109},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.639},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309120},
author = {Giovanny Arbelaez Garces and Eric Bonjour and Auguste Rakotondranaivo},
keywords = {Acceptability evaluation, Project management, Bayesian Network, Knowledge engineering, Decision making},
abstract = {Evaluating and improving product acceptability is an important step to minimize the risk of a new product not being accepted. Existing approaches do not integrate any acceptability evaluation during the New Product Development process (NPD). They do not make it possible to evaluate different improvement scenarios built from experts knowledge or past projects experiences. This paper proposes a method that evaluates and improves product acceptability by allowing the project manager to find out an improvement scenario. The proposed method is based on the evaluation of the users’ concept perception. It exploits the inference properties of Bayesian networks (BN) allowing to make useful estimations of improvement scenarios. Furthermore, those scenarios are composed of actions that enable to improve different dimensions of users’ acceptability. The modeled actions are stocked in a knowledge base allowing this knowledge to be reused in other projects. The method is applied to the design case of a medical-stocking threading device in order to illustrate its interest.}
}
@article{ZHANG2023677,
title = {Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system},
journal = {Applied Mathematical Modelling},
volume = {119},
pages = {677-697},
year = {2023},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23001269},
author = {Yiming Zhang and Dingyang Zhang and Xiaoge Zhang and Lemiao Qiu and Felix T.S. Chan and Zili Wang and Shuyou Zhang},
keywords = {Deep Reinforcement Learning, Multi-component System, Probabilistic Machine Learning, Maintenance Scheduling, Sampling-Efficient Learning},
abstract = {In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.}
}
@article{HASAN2019107,
title = {Dynamic multi-objective optimisation using deep reinforcement learning: benchmark, algorithm and an application to identify vulnerable zones based on water quality},
journal = {Engineering Applications of Artificial Intelligence},
volume = {86},
pages = {107-135},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619302003},
author = {Md Mahmudul Hasan and Khin Lwin and Maryam Imani and Antesar Shabut and Luiz Fernando Bittencourt and M.A. Hossain},
keywords = {Dynamic environment, Reinforcement learning, Deep Q network, Water quality resilience, Meta-policy selection, Artificial intelligence},
abstract = {Dynamic multi-objective optimisation problem (DMOP) has brought a great challenge to the reinforcement learning (RL) research area due to its dynamic nature such as objective functions, constraints and problem parameters that may change over time. This study aims to identify the lacking in the existing benchmarks for multi-objective optimisation for the dynamic environment in the RL settings. Hence, a dynamic multi-objective testbed has been created which is a modified version of the conventional deep-sea treasure (DST) hunt testbed. This modified testbed fulfils the changing aspects of the dynamic environment in terms of the characteristics where the changes occur based on time. To the authors’ knowledge, this is the first dynamic multi-objective testbed for RL research, especially for deep reinforcement learning. In addition to that, a generic algorithm is proposed to solve the multi-objective optimisation problem in a dynamic constrained environment that maintains equilibrium by mapping different objectives simultaneously to provide the most compromised solution that closed to the true Pareto front (PF). As a proof of concept, the developed algorithm has been implemented to build an expert system for a real-world scenario using Markov decision process to identify the vulnerable zones based on water quality resilience in São Paulo, Brazil. The outcome of the implementation reveals that the proposed parity-Q deep Q network (PQDQN) algorithm is an efficient way to optimise the decision in a dynamic environment. Moreover, the result shows PQDQN algorithm performs better compared to the other state-of-the-art solutions both in the simulated and the real-world scenario.}
}
@article{KARLSSON20081689,
title = {Detection and interactive isolation of faults in steam turbines to support maintenance decisions},
journal = {Simulation Modelling Practice and Theory},
volume = {16},
number = {10},
pages = {1689-1703},
year = {2008},
note = {The Analysis of Complex Systems},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2008.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X08001639},
author = {Christer Karlsson and Jaime Arriagada and Magnus Genrup},
keywords = {Steam turbine maintenance, Artificial neural network fault detection, Bayesian network fault isolation, Decision support},
abstract = {The maintenance of steam turbines is expensive, particularly if dismantling is required. A concept for the provision of support for the maintenance engineer in determining steam turbine status in relation to the recommended maintenance interval is presented here. The concept embodies an artificial neural network which is conditioned to recognise patterns known to be related to faults. The faults simulated are not known to be recognized on-line and the concept is in an early stage of development. An example of a Bayesian network structure containing expert knowledge is proposed to be used, in a dialogue with the operator, to isolate the root causes of a number of fault types. The aim is to be well informed about the statue of the turbine in order to take earlier and better informed maintenance actions. The detection procedure has been validated in a simulation environment.}
}
@article{PANDA2024408,
title = {Dynamic resource matching in manufacturing using deep reinforcement learning},
journal = {European Journal of Operational Research},
volume = {318},
number = {2},
pages = {408-423},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724003862},
author = {Saunak Kumar Panda and Yisha Xiang and Ruiqi Liu},
keywords = {Assignment, Matching problem, Manufacturing, Markov decision process, Deep reinforcement learning},
abstract = {Matching plays an important role in the logical allocation of resources across a wide range of industries. The benefits of matching have been increasingly recognized in manufacturing industries. In particular, capacity sharing has received much attention recently. In this paper, we consider the problem of dynamically matching demand-capacity types of manufacturing resources. We formulate the multi-period, many-to-many manufacturing resource-matching problem as a sequential decision process. The formulated manufacturing resource-matching problem involves large state and action spaces, and it is not practical to accurately model the joint distribution of various types of demands. To address the curse of dimensionality and the difficulty of explicitly modeling the transition dynamics, we use a model-free deep reinforcement learning approach to find optimal matching policies. Moreover, to tackle the issue of infeasible actions and slow convergence due to initial biased estimates caused by the maximum operator in Q-learning, we introduce two penalties to the traditional Q-learning algorithm: a domain knowledge-based penalty based on a prior policy and an infeasibility penalty that conforms to the demand–supply constraints. We establish theoretical results on the convergence of our domain knowledge-informed Q-learning providing performance guarantee for small-size problems. For large-size problems, we further inject our modified approach into the deep deterministic policy gradient (DDPG) algorithm, which we refer to as domain knowledge-informed DDPG (DKDDPG). In our computational study, including small- and large-scale experiments, DKDDPG consistently outperformed traditional DDPG and other RL algorithms, yielding higher rewards and demonstrating greater efficiency in time and episodes.}
}
@article{KUMAR20137221,
title = {Integrated modelling for Sustainability Appraisal of urban river corridors: Going beyond compartmentalised thinking},
journal = {Water Research},
volume = {47},
number = {20},
pages = {7221-7234},
year = {2013},
note = {Urban Water Management to Increase Sustainability of Cities},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2013.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0043135413008154},
author = {Vikas Kumar and J.R. Rouquette and David N. Lerner},
keywords = {Integrated modelling, Sustainability Appraisal, Urban river corridor, Bayesian Network},
abstract = {Sustainability Appraisal (SA) is a complex task that involves integration of social, environmental and economic considerations and often requires trade-offs between multiple stakeholders that may not easily be brought to consensus. Classical SA, often compartmentalised in the rigid boundary of disciplines, can facilitate discussion, but can only partially inform decision makers as many important aspects of sustainability remain abstract and not interlinked. A fully integrated model can overcome compartmentality in the assessment process and provides opportunity for a better integrative exploratory planning process. The objective of this paper is to explore the benefit of an integrated modelling approach to SA and how a structured integrated model can be used to provide a coherent, consistent and deliberative platform to assess policy or planning proposals. The paper discusses a participative and integrative modelling approach to urban river corridor development, incorporating the principal of sustainability. The paper uses a case study site in Sheffield, UK, with three alternative development scenarios, incorporating a number of possible riverside design features. An integrated SA model is used to develop better design by optimising different design elements and delivering a more sustainable (re)-development plan. We conclude that participatory integrated modelling has strong potential for supporting the SA processes. A high degree of integration provides the opportunity for more inclusive and informed decision-making regarding issues of urban development. It also provides the opportunity to reflect on their long-term dynamics, and to gain insights on the interrelationships underlying persistent sustainability problems. Thus the ability to address economic, social and environmental interdependencies within policies, plans, and legislations is enhanced.}
}
@article{SANNER2009748,
title = {Practical solution techniques for first-order MDPs},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {748-788},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001884},
author = {Scott Sanner and Craig Boutilier},
keywords = {MDPs, First-order logic, Planning},
abstract = {Many traditional solution approaches to relationally specified decision-theoretic planning problems (e.g., those stated in the probabilistic planning domain description language, or PPDDL) ground the specification with respect to a specific instantiation of domain objects and apply a solution approach directly to the resulting ground Markov decision process (MDP). Unfortunately, the space and time complexity of these grounded solution approaches are polynomial in the number of domain objects and exponential in the predicate arity and the number of nested quantifiers in the relational problem specification. An alternative to grounding a relational planning problem is to tackle the problem directly at the relational level. In this article, we propose one such approach that translates an expressive subset of the PPDDL representation to a first-order MDP (FOMDP) specification and then derives a domain-independent policy without grounding at any intermediate step. However, such generality does not come without its own set of challenges—the purpose of this article is to explore practical solution techniques for solving FOMDPs. To demonstrate the applicability of our techniques, we present proof-of-concept results of our first-order approximate linear programming (FOALP) planner on problems from the probabilistic track of the ICAPS 2004 and 2006 International Planning Competitions.}
}
@article{MOGLIA201250,
title = {Assessing the likelihood of realizing idealized goals: The case of urban water strategies},
journal = {Environmental Modelling & Software},
volume = {35},
pages = {50-60},
year = {2012},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212000539},
author = {M. Moglia and P. Perez and S. Burn},
keywords = {Bayesian Networks (BNs), Subjective logic, Water aid, Integrated Urban Water Management},
abstract = {Urban water management can be challenging, but in Small Island Developing States it is particularly difficult due to resource constraints and isolation. This is the situation in the town of Tarawa in Kiribati, where attempts to improve water services have often not led to the desired outcomes. The reasons are varied, and include widely a lack of consideration of local circumstances, process requirements, and inadequate involvement of affected stakeholders, and inadequate cross-sectoral coordination. In light of the tendency in urban water planning to assume only the idealized performance of strategies, the authors argue that there is a need to also formally consider the likelihood of realizing this idealized performance. It is difficult to assess such likelihoods, other than via the use of judgments by expert and local stakeholders. Such judgments are typically qualitative and fairly abstract and often not directly concerning a particular strategy. The current paper provides a methodology to assess the likelihood of the idealized performance of strategies, based on Bayesian Networks (BNs) and Subjective Logic (SL) utilizing expert and local knowledge, creating a capacity to capture and apply previous experiences, and dispersed knowledge in decision making and planning. The methodology has been developed and tested on water management strategies in the town of Tarawa, Kiribati. As such, this paper provides a method for mapping the causal explanations for why developments do not achieve their set goals, and the approach may form the basis for assessments to be more widely applied when evaluating urban water strategies in similar contexts. In this paper, the approach has been applied by using existing data from interviews and literature to evaluate one strategy, reserve extensions and groundwater extraction. Other strategies, i.e. rainwater harvesting, desalination and have also been evaluated but have not been described in this paper because of limited space.}
}
@article{BUSACCA2021108330,
title = {Designing a multi-layer edge-computing platform for energy-efficient and delay-aware offloading in vehicular networks},
journal = {Computer Networks},
volume = {198},
pages = {108330},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108330},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003315},
author = {Fabio Busacca and Giuseppe Faraci and Christian Grasso and Sergio Palazzo and Giovanni Schembra},
keywords = {5G, Edge Computing, Vehicular Networks, Reinforcement Learning, Markov Models},
abstract = {Vehicular networks are expected to support many time-critical services requiring huge amounts of computation resources with very low delay. However, such requirements may not be fully met by vehicle on-board devices due to their limited processing and storage capabilities. The solution provided by 5G is the application of the Multi-Access Edge Computing (MEC) paradigm, which represents a low-latency alternative to remote clouds. Accordingly, we envision a multi-layer job-offloading scheme based on three levels, i.e., the Vehicular Domain, the MEC Domain and Backhaul Network Domain. In such a view, jobs can be offloaded from the Vehicular Domain to the MEC Domain, and even further offloaded between MEC Servers for load balancing purposes. We also propose a framework based on a Markov Decision Process (MDP) to model the interactions among stakeholders working at the three different layers. Such a MDP model allows a Reinforcement Learning (RL) algorithm to take optimal decisions on both the number of jobs to offload between MEC Servers, and on the amount of computing power to allocate to each job. An extensive numerical analysis is presented to demonstrate the effectiveness of our algorithm in comparison with static policies not applying RL.}
}
@article{CHEMWENO2015663,
title = {Development of a risk assessment selection methodology for asset maintenance decision making: An analytic network process (ANP) approach},
journal = {International Journal of Production Economics},
volume = {170},
pages = {663-676},
year = {2015},
note = {Current Research Issues in Production Economics},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2015.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925527315000857},
author = {Peter Chemweno and Liliane Pintelon and Adriaan {Van Horenbeek} and Peter Muchiri},
keywords = {Asset maintenance, Risk assessment, Selection methodology, ANP},
abstract = {Risk assessment performs a critical decision support role in maintenance decision making. This is through assisting maintenance practitioners systematically identify, analyze, evaluate and mitigate equipment failures. Often, such failures are mitigated through formulating effective maintenance strategies. In asset maintenance, well-known risk assessment techniques include the Failure Mode and Effect Analysis (FMEA), Fault Tree Analysis (FTA), and Bayesian Networks (BN). In recent years, considerable research attention has been directed towards improving existing techniques, often at the expense of a structured framework for selecting suitable risk assessment techniques. Often, several criteria influence the selection process. Moreover, the criteria are closely linked to specific organizational competencies that vary from one firm to another. In this study, a selection methodology for risk assessment techniques in the maintenance decision making domain is proposed. In the methodology, generic selection criteria for the FMEA, FTA and BN are derived based on the risk assessment process outlined in the ISO 31000:2009 standard. The criteria are prioritized using the Analytic Network Process (ANP), taking into account the judgment and opinion of academic and industrial domain experts. The results illustrate the usefulness of the proposed methodology towards assisting maintenance practitioners discern important competencies relevant to the specific technique and as such select the technique best suited for the organization.}
}
@article{TSOUKALAS2015,
title = {From Data to Optimal Decision Making: A Data-Driven, Probabilistic Machine Learning Approach to Decision Support for Patients With Sepsis},
journal = {JMIR Medical Informatics},
volume = {3},
number = {1},
year = {2015},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.3445},
url = {https://www.sciencedirect.com/science/article/pii/S2291969415000125},
author = {Athanasios Tsoukalas and Timothy Albertson and Ilias Tagkopoulos},
keywords = {sepsis, clinical decision support tool, probabilistic modeling, Partially Observable Markov Decision Processes, POMDP, CDSS},
abstract = {Background
A tantalizing question in medical informatics is how to construct knowledge from heterogeneous datasets, and as an extension, inform clinical decisions. The emergence of large-scale data integration in electronic health records (EHR) presents tremendous opportunities. However, our ability to efficiently extract informed decision support is limited due to the complexity of the clinical states and decision process, missing data and lack of analytical tools to advice based on statistical relationships.
Objective
Development and assessment of a data-driven method that infers the probability distribution of the current state of patients with sepsis, likely trajectories, optimal actions related to antibiotic administration, prediction of mortality and length-of-stay.
Methods
We present a data-driven, probabilistic framework for clinical decision support in sepsis-related cases. We first define states, actions, observations and rewards based on clinical practice, expert knowledge and data representations in an EHR dataset of 1492 patients. We then use Partially Observable Markov Decision Process (POMDP) model to derive the optimal policy based on individual patient trajectories and we evaluate the performance of the model-derived policies in a separate test set. Policy decisions were focused on the type of antibiotic combinations to administer. Multi-class and discriminative classifiers were used to predict mortality and length of stay.
Results
Data-derived antibiotic administration policies led to a favorable patient outcome in 49% of the cases, versus 37% when the alternative policies were followed (P=1.3e-13). Sensitivity analysis on the model parameters and missing data argue for a highly robust decision support tool that withstands parameter variation and data uncertainty. When the optimal policy was followed, 387 patients (25.9%) have 90% of their transitions to better states and 503 patients (33.7%) patients had 90% of their transitions to worse states (P=4.0e-06), while in the non-policy cases, these numbers are 192 (12.9%) and 764 (51.2%) patients (P=4.6e-117), respectively. Furthermore, the percentage of transitions within a trajectory that lead to a better or better/same state are significantly higher by following the policy than for non-policy cases (605 vs 344 patients, P=8.6e-25). Mortality was predicted with an AUC of 0.7 and 0.82 accuracy in the general case and similar performance was obtained for the inference of the length-of-stay (AUC of 0.69 to 0.73 with accuracies from 0.69 to 0.82).
Conclusions
A data-driven model was able to suggest favorable actions, predict mortality and length of stay with high accuracy. This work provides a solid basis for a scalable probabilistic clinical decision support framework for sepsis treatment that can be expanded to other clinically relevant states and actions, as well as a data-driven model that can be adopted in other clinical areas with sufficient training data.}
}
@article{KOTAS2018328,
title = {Bayesian learning of dose–response parameters from a cohort under response-guided dosing},
journal = {European Journal of Operational Research},
volume = {265},
number = {1},
pages = {328-343},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717306616},
author = {Jakob Kotas and Archis Ghate},
keywords = {OR in medicine, Dynamic programming, Markov decision processes, Convex optimization, Medical treatment planning},
abstract = {There has been a surge of clinical interest in the idea of response-guided dosing (RGD). The goal in RGD is to tailor drug-doses to the stochastic evolution of each individual patient’s disease condition over the treatment course. The hope is that this form of individualized therapy will deliver the right dose to the right patient at the right time. Several expert panels have observed that despite the excitement surrounding RGD, quantitative, data-driven decision-making approaches that learn patients’ dose–response and incorporate this information into adaptive dosing strategies are lagging behind. This situation is particularly exacerbated in clinical trials. For instance, fixed design clinical studies for estimating the key parameter of a dose–response function might not treat trial patients optimally. Similarly, the dosing strategies employed in clinical trials for RGD often appear ad-hoc.We study the problem of finding optimal RGD policies while learning the distribution of a dose–response parameter from a cohort of patients. We provide a Bayesian stochastic dynamic programming (DP) formulation of this problem. Exact solution of Bellman’s equations for this problem is computationally intractable. We therefore present two approximate control schemes and mathematically analyze the monotonicity, stationarity, and separability structures of the resulting dosing strategies. These structures are then exploited in efficient, approximate solution of our problem. Computer simulations using the Michaelis–Menten dose–response function are included as an example wherein we study the effect of cohort size and of prior misspecification.}
}
@article{TILAHUN2024111432,
title = {Fuzzy-based predictive deep reinforcement learning for robust and constrained optimal control of industrial solar thermal plants},
journal = {Applied Soft Computing},
volume = {159},
pages = {111432},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111432},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624002060},
author = {Fitsum Bekele Tilahun},
keywords = {Distributed solar field, Constrained optimal control, Fuzzy, Deep deterministic policy gradient, Industrial plant},
abstract = {Integrating distributed solar fields (DSFs) into conventional heat and power plants (CHPs) of industries is mostly constrained by the availability of a real-time capable control scheme. Safe and efficient operation of industrial DSF requires the supply of a fluctuating and periodically available energy at the required temperature while reducing losses and ensuring operational constraint. Reinforcement learning (RL), specifically Q-learning methods, are being recently applied for such control tasks. However, existing RL approaches cannot be directly applied to continuous domain control tasks or tasks with operation constraints. While using deep learning in model-free RL can remove these limitations, tractability and scalability boundaries on performing the deep learning are becoming significant, with scarce data and multi-objective concerns. In contrast, model-based deep RL schemes can be applied for sample efficiency and satisfying operational constraints, but their modeling framework is not general and accurate. To address these challenges, the work here develops a hybrid fuzzy convolution model (HFCM) that takes full advantages of data, models (dynamic and steady-state), and prior knowledge on industrial DSF. The HFCM is then extended for use in deep deterministic policy gradient (DDPG) algorithm to learn the DSF control task. This is done so by solving a multi-objective optimization problem, which is formulated as a constrained Markov decision process (CMDP) with continuous state and actions. Some practical and relevant findings were made with this HFCM. Firstly, it allowed the DDPG agent to learn independently a temperature state and a disturbance state, using only temperature measurement and single realizations of state vector. It also permitted the testing of an operational state violation strategy on a DDPG actions, and at the same time, the correction of the effects at a safety layer, if needed. Furthermore, the proposed control strategy simultaneously reduced mean temperature tracking error by 24%–51% and energy gain by 13.9%–17.35% when compared respectively to model-free DDPG and MPC baselines.}
}
@article{NADIM2023106853,
title = {Learn-to-supervise: Causal reinforcement learning for high-level control in industrial processes},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106853},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106853},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623010370},
author = {Karim Nadim and Mohamed-Salah Ouali and Hakim Ghezzaz and Ahmed Ragab},
keywords = {Supervisory control, Causality analysis, Deep reinforcement learning, Process mining, Discrete event systems, Energy efficiency},
abstract = {Possessing efficient supervisory control systems is crucial for maintaining the desired operational performance of complex industrial processes. Several challenges face the developers of these systems, such as requiring accurate physical models, dealing with the variability and uncertainty of process operating conditions and coordinating between local controllers to reach desired global performance. This paper proposes an intelligent supervisory control approach based on causal reinforcement learning (CRL) to effectively manipulate the controllers’ setpoints of the process in a way that optimizes its key performance indicators (KPIs), thereby improving the energy efficiency of the process. The approach adopts deep reinforcement learning (DRL) to develop an efficient control policy through interaction with a process simulation. The DRL training history is then exploited using interpretable machine learning and process mining to build a discrete event system (DES) model, in the form of a state-event graph. The DES model identifies causal relationships between events and provides interpretability to the control policy developed by the DRL method. The DES discovered is exploited as a Markov decision process to apply the Q-learning algorithm as a CRL supervisor. The supervisor incorporates causal knowledge into its training process, thus improving the DRL control policy developed and identifying the event paths that optimize the process’s KPIs. The proposed approach is validated using two heat recovery systems in a pulp & paper mill. It successfully achieves a control policy that reduces energy consumption by up to 15.6% for the first system and 5.02% for the second, compared to the expert’s baseline methods.}
}
@article{TEGELTIJA2018457,
title = {Exploring Deep Uncertainty Approaches for Application in Life Cycle Engineering},
journal = {Procedia CIRP},
volume = {69},
pages = {457-462},
year = {2018},
note = {25th CIRP Life Cycle Engineering (LCE) Conference, 30 April – 2 May 2018, Copenhagen, Denmark},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117309381},
author = {Miroslava Tegeltija and Josef Oehmen and Igor Kozin and Jan Kwakkel},
keywords = {deep uncertainty, life cycle engineering, risk, long-term planning, sustainability},
abstract = {Uncertainty assessment and management, as well as the associated decision making are increasingly important in a variety of scientific fields. While uncertainty analysis has a long tradition, meeting sustainable development goals through long-term Life Cycle Engineering (LCE) decision making demands addressing Deep Uncertainty (DU). DU characterizes situations where there is no agreement on exact causal structures, let alone probabilities. In this case traditional, probability based approaches cannot produce reliable results, as there is a lack of information and experts are unlikely to agree upon probabilities. Due to the nature of LCE, this paper argues that methods to better cope with DU can make a significant contribution to the management of LCE. We introduce a set of methods that use computational experiments to analyze DU and have been successfully applied in other fields. We describe Robust Decision Making (RDM) as the most promising approach for addressing DU challenges in LCE. We then illustrate the difference between applying traditional risk management approaches and RDM through an example, complemented with the interview findings from a company using RDM. We conclude with a discussion on future research directions.}
}
@article{WANG2024102482,
title = {A deep reinforcement learning control strategy to improve the operating flexibility of CHP units under variable load conditions},
journal = {Thermal Science and Engineering Progress},
volume = {49},
pages = {102482},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2024.102482},
url = {https://www.sciencedirect.com/science/article/pii/S2451904924001008},
author = {Xin Wang and Chenggang Cui and Chunjian Pan and Chuanlin Zhang and Hongbo Ren and Amer M.Y.M. Ghias},
keywords = {Combined heat and power, Deep reinforcement learning, Expert demonstrations, Operating flexibility, Steam extraction heating},
abstract = {With the renewable energy generation into the power grid, the flexibility in operation and control becomes vital for combined heat and power (CHP) units under variable load conditions. This paper proposes a model-free deep reinforcement learning (DRL) strategy to enhance the operational flexibility and load regulation capability of CHP units in electricity-heat coordinated control. Firstly, the electricity-heat coordination problem is modeled as a Markov decision process (MDP), with the main steam pressure and electric power serving as the state space, the valve opening accounting for dynamic characteristics forming the action space, and the deviation of the main steam pressure being the reward function. Secondly, the training and solution for the electricity-heat coordinated control strategy of CHP units is accomplished by the deep deterministic policy gradient from demonstrations (DDPGfD) algorithm combining pre-training and offline training. Finally, a case study of a 300MW CHP unit under variable load conditions demonstrates that the proposed model-free DRL control strategy improves the power ramp rate of CHP units while maintaining the stability of the main steam pressure and heat supply, showcasing enhanced exploration capabilities of the agent.}
}
@article{QING2025106889,
title = {CoSD: Balancing behavioral consistency and diversity in unsupervised skill discovery},
journal = {Neural Networks},
volume = {182},
pages = {106889},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106889},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008189},
author = {Shuai Qing and Yi Sun and Kun Ding and Hui Zhang and Fei Zhu},
keywords = {Hierarchical reinforcement learning, Unsupervised skill discovery, Semi-Markov decision process, Mutual information, Behavioral consistency},
abstract = {In hierarchical reinforcement learning, unsupervised skill discovery holds promise for overcoming the challenge of sparse rewards commonly encountered in traditional reinforcement learning. Although previous unsupervised skill discovery methods excelled at maximizing intrinsic rewards, they often overly prioritized skill diversity. Unrestrained pursuit of diversity leads skills to concentrate attention on unexplored domains, overlooking the internal consistency of skills themselves, resulting in the state visit distribution of individual skills lacking concentration. To address this problem, the Constrained Skill Discovery (CoSD) algorithm is proposed to balance the diversity and behavioral consistency of skills. CoSD integrates both the forward and the reverse decomposition forms of mutual information and uses the maximum entropy policy to maximize the information-theoretic objective of skill learning while requiring that each skill maintain low state entropy internally, which enhances the behavioral consistency of the skills while pursuing the diversity of the skills and ensures that the learned skills have a high degree of stability. Experimental results demonstrated that, compared with other skill discovery methods based on mutual information, skills from CoSD exhibited a more concentrated state visit distribution, indicating higher behavioral consistency and stability. In some complex downstream tasks, the skills with higher behavioral consistency exhibit superior performance.}
}
@article{RAMICIC2023456,
title = {Uncertainty maximization in partially observable domains: A cognitive perspective},
journal = {Neural Networks},
volume = {162},
pages = {456-471},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023001168},
author = {Mirza Ramicic and Andrea Bonarini},
keywords = {Partially observable Markov decision process, Cognitive modeling, Entropy, Reinforcement learning, Attention mechanisms and development, Neural networks for development},
abstract = {Faced with an ever-increasing complexity of their domains of application, artificial learning agents are now able to scale up in their ability to process an overwhelming amount of data. However, this comes at the cost of encoding and processing an increasing amount of redundant information. This work exploits the possibility of learning systems, applied in partially observable domains, to selectively focus on the specific type of information that is more likely related to the causal interaction among transitioning states. A temporal difference displacement criterion is defined to implement adaptive masking of the observations. It can enable a significant improvement of convergence of temporal difference algorithms applied to partially observable Markov processes, as shown by experiments performed under a variety of machine learning problems, ranging from highly complex visuals as Atari games to simple textbook control problems such as CartPole. The proposed framework can be added to most RL algorithms since it only affects the observation process, selecting the parts more promising to explain the dynamics of the environment and reducing the dimension of the observation space.}
}
@article{MAZZI2023103987,
title = {Risk-aware shielding of Partially Observable Monte Carlo Planning policies},
journal = {Artificial Intelligence},
volume = {324},
pages = {103987},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103987},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001339},
author = {Giulio Mazzi and Alberto Castellini and Alessandro Farinelli},
keywords = {POMDP, POMCP, SMT, Risk-awareness, Shielding},
abstract = {Partially Observable Monte Carlo Planning (POMCP) is a powerful online algorithm that can generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. However, the lack of an explicit policy representation hinders interpretability and a proper evaluation of the risks an agent may incur. In this work, we propose a methodology based on Maximum Satisfiability Modulo Theory (MAX-SMT) for analyzing POMCP policies by inspecting their traces, namely, sequences of belief-action pairs generated by the algorithm. The proposed method explores local properties of the policy to build a compact and informative summary of the policy behaviour. Moreover, we introduce a rich and formal language that a domain expert can use to describe the expected behaviour of a policy. In more detail, we present a formulation that directly computes the risk involved in taking actions by considering the high-level elements specified by the expert. The final formula can identify risky decisions taken by POMCP that violate the expert indications. We show that this identification process can be used offline (to improve the policy's explainability and identify anomalous behaviours) or online (to shield the risky decisions of the POMCP algorithm). We present an extended evaluation of our approach on four domains: the well-known tiger and rocksample benchmarks, a problem of velocity regulation in mobile robots, and a problem of battery management in mobile robots. We test the methodology against a state-of-the-art anomaly detection algorithm to show that our approach can be used to identify anomalous behaviours in faulty POMCP. We also show, comparing the performance of shielded and unshielded POMCP, that the shielding mechanism can improve the system's performance. We provide an open-source implementation of the proposed methodologies at https://github.com/GiuMaz/XPOMCP.}
}
@article{DURSUN2022108665,
title = {Data pooling for multiple single-component systems under population heterogeneity},
journal = {International Journal of Production Economics},
volume = {250},
pages = {108665},
year = {2022},
note = {Special Issue celebrating Volume 250 of the International Journal of Production Economics},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2022.108665},
url = {https://www.sciencedirect.com/science/article/pii/S092552732200247X},
author = {İpek Dursun and Alp Akçay and Geert-Jan {van Houtum}},
keywords = {Maintenance optimization, Population heterogeneity, Bayesian learning, Partially observable Markov decision processes, Data pooling},
abstract = {We consider multiple newly designed single-component systems with a known finite lifespan. The component in each system can be replaced preventively to avoid a costly failure. This component is also in use for the first time, and therefore, there is no historical data on the lifetime of the component. In this case, the probability distribution of the component lifetime can be estimated based on expert opinions. However, there can be different opinions on the lifetime distribution. There are two populations where components can come from: a weak and a strong population. We assume that the components always come from the same population. However, the true type of the population is unknown. We build a discrete-time partially observable Markov decision process model to find the optimal replacement policy which minimizes the expected total cost throughout the lifespan. To resolve the uncertainty regarding population heterogeneity, we update the belief by using the data collected from all systems, allowing us to investigate the effect of so-called data pooling. First, we generate insights about the structure of the optimal policy. We then compare the cost per system under the optimal policy with the cost per system under two benchmark heuristics that follow the single-system optimal policy with and without data pooling, respectively. In our numerical experiments, we show that the cost reduction relative to the worst benchmark heuristic can be up to 5.6% for data pooling with two systems, and this increases up to 14% for 20 systems. Additionally, the effect of various input parameters on the costs is analyzed.}
}
@article{XIA20161,
title = {Neural inverse reinforcement learning in autonomous navigation},
journal = {Robotics and Autonomous Systems},
volume = {84},
pages = {1-14},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015301652},
author = {Chen Xia and Abdelkader {El Kamel}},
keywords = {Inverse reinforcement learning, Learning from demonstration, Neural network, Autonomous navigation, Markov decision processes, Dynamic environments},
abstract = {Designing intelligent and robust autonomous navigation systems remains a great challenge in mobile robotics. Inverse reinforcement learning (IRL) offers an efficient learning technique from expert demonstrations to teach robots how to perform specific tasks without manually specifying the reward function. Most of existing IRL algorithms assume the expert policy to be optimal and deterministic, and are applied to experiments with relatively small-size state spaces. However, in autonomous navigation tasks, the state spaces are frequently large and demonstrations can hardly visit all the states. Meanwhile the expert policy may be non-optimal and stochastic. In this paper, we focus on IRL with large-scale and high-dimensional state spaces by introducing the neural network to generalize the expert’s behaviors to unvisited regions of the state space and an explicit policy representation is easily expressed by neural network, even for the stochastic expert policy. An efficient and convenient algorithm, Neural Inverse Reinforcement Learning (NIRL), is proposed. Experimental results on simulated autonomous navigation tasks show that a mobile robot using our approach can successfully navigate to the target position without colliding with unpredicted obstacles, largely reduce the learning time, and has a good generalization performance on undemonstrated states. Hence prove the robot intelligence of autonomous navigation transplanted from limited demonstrations to completely unknown tasks.}
}
@article{MALLAMPALLI20167,
title = {Methods for translating narrative scenarios into quantitative assessments of land use change},
journal = {Environmental Modelling & Software},
volume = {82},
pages = {7-20},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216301037},
author = {Varun Rao Mallampalli and Georgia Mavrommati and Jonathan Thompson and Matthew Duveneck and Spencer Meyer and Arika Ligmann-Zielinska and Caroline Gottschalk Druschke and Kristen Hychka and Melissa A. Kenney and Kasper Kok and Mark E. Borsuk},
keywords = {Socio-ecological scenarios, Participatory process, Agent-based model, Fuzzy cognitive map, Bayesian network, System dynamics},
abstract = {In the land use and land cover (LULC) literature, narrative scenarios are qualitative descriptions of plausible futures associated with a combination of socio-economic, policy, technological, and climate changes. LULC models are then often used to translate these narrative descriptions into quantitative characterizations of possible future societal and ecological impacts and conditions. To respect the intent of the underlying scenario descriptions, this process of translation needs to be thoughtful, transparent, and reproducible. This paper evaluates the current state of the art in scenario translation methods and outlines their relative advantages and disadvantages, as well as the respective roles of stakeholders and subject matter experts. We summarize our findings in the form of a decision matrix that can assist land use planners, scientists, and modelers in choosing a translation method appropriate to their situation.}
}
@article{KNUDSEN2016319,
title = {A Knowledge and Analytics-Based Framework and Model for Forecasting Program Schedule Performance},
journal = {Procedia Computer Science},
volume = {95},
pages = {319-326},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.341},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325145},
author = {Kevin T. Knudsen and Mark Blackburn},
keywords = {project scheduling, Bayesian Networks, causal factors, schedule performance},
abstract = {This paper discusses a mixed methodological approach to address cost and schedule delays in large-scale engineering programs (LSEP). Research has identified potential causes or factors for schedule delays, such as: ineffective human resources policies and practices, consolidation of the aerospace industry, too many stakeholders, and lack of knowledge-based acquisition practices. Current methods and tools are ineffective in helping project managers to accurately predict schedule performance during LSEP development. The authors describe research to investigate the feasibility of: (1) deriving quantitative and qualitative causal factors correlating to schedule performance during LSEP development; (2) developing a framework and a tailorable predictive model using the causal factors in a Bayesian Network (BN) model; and (3) using the resultant framework and BN model, with expert knowledge elicited from subject matter experts, to predict schedule performance and inform decision makers on actions needed to manage schedule performance. Finally, this paper discusses a version of a BN model developed by mapping a conceptualization of the framework created using a systemigram with a BN pattern that includes dependencies from causal and control factors to schedule performance. The BN incorporates a direct causal dependence of schedule performance and mitigation actions to the consequences of schedule performance.}
}
@article{KUMAR2012687,
title = {Integrated modelling for Sustainability Appraisal for Urban River Corridor (re)-development},
journal = {Procedia Environmental Sciences},
volume = {13},
pages = {687-697},
year = {2012},
note = {18th Biennial ISEM Conference on Ecological Modelling for Global Change and Coupled Human and Natural System},
issn = {1878-0296},
doi = {https://doi.org/10.1016/j.proenv.2012.01.062},
url = {https://www.sciencedirect.com/science/article/pii/S1878029612000631},
author = {Vikas Kumar and J.R. Rouquette and D.N. Lerner},
keywords = {Integrated Modelling, Sustainability Appraisal, Urban River Corridor, Bayesian Network},
abstract = {Sustainability Appraisal (SA) is mandatory under the relevant legislation of UK (DCLG, 2008a) and applies to the preparation of Regional Spatial Strategies, Development Plans and Supplementary Planning documents. SA is a complex task that involves integration of social, environmental and economic considerations into formal plans and often requires trade-offs between multiple stakeholders that may not easily be brought to consensus. Classical assessment can facilitate discussion, but these can only partially inform decision makers as many important aspects of sustainability are abstract and not quantifiable. Such abstract criteria however can be modelled using a Bayesian Network (BN), combining expert opinions, empirical evidence and other information such as model simulation, survey etc. This paper discusses the work of the URSULA project at the University of Sheffield, in which a participative and integrative approach to urban river corridor development, incorporating the principal of sustainability was used. The project used a case study site in Sheffield, UK, and three alternative scenarios were developed, incorporating a number of possible riverside design features. Scenarios were fully designed and visualised using a variety of different media and a sustainability appraisal was undertaken using a broad range of environmental, social and economic indicators. Experts’ assessment logics were captured through mind mapping and further expert elicitation was used to develop an integrated model for SA. The BN approach allows model complexity to be reduced to a level appropriate for an assessment process, whilst still taking complex system interactions implicitly into account. The integrated SA model is being used to develop better design by optimising different design elements in order to deliver an optimum (re)-development plan.}
}
@article{LIU2022109382,
title = {AR-GAIL: Adaptive routing protocol for FANETs using generative adversarial imitation learning},
journal = {Computer Networks},
volume = {218},
pages = {109382},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109382},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622004169},
author = {Jianmin Liu and Qi Wang and Yongjun Xu},
keywords = {Adaptive routing, Generative adversarial imitation learning (GAIL), Flying ad-hoc networks (FANETs)},
abstract = {Flying ad hoc networks (FANETs), as the emerging communication paradigm, have been widely used in civil and military fields. Packet routing in FANETs is challenging due to dynamic network conditions. Traditional topology-based routing protocols are unsuitable for FANETs with dynamic network topologies. Routing protocols based on reinforcement learning (RL) may be the first choice for FANETs because of their good learning ability. However, existing RL-based routing protocols for FANETs have limited adaptability to network dynamics due to ignoring neighborhood environment states, and are prone to get stuck in suboptimal routing policies owing to inappropriate reward design and delayed reward issues. We propose AR-GAIL, an adaptive routing protocol based on Generative Adversarial Imitation Learning (GAIL), which aims to select the minimal end-to-end delay route according to ongoing network conditions for FANETs. We formulate the routing decision process as a Markov decision process (MDP) and design a novel MDP state which consists of the current node state and the neighborhood environment state. Moreover, we develop an efficient value function-based GAIL learning framework to learn the routing policy from expert routes instead of a predefined reward function. The simulation shows that AR-GAIL can adapt well to network dynamics. Compared with state-of-the-art routing protocols, AR-GAIL shows outstanding performance in terms of the end-to-end delay and packet delivery ratio.}
}
@article{LIN200359,
title = {Reinforcement learning based on local state feature learning and policy adjustment},
journal = {Information Sciences},
volume = {154},
number = {1},
pages = {59-70},
year = {2003},
note = {Introduction to Multimedia and Mobile Agents},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(03)00006-9},
url = {https://www.sciencedirect.com/science/article/pii/S0020025503000069},
author = {Ya-Ping Lin and Xue-Yong Li},
keywords = {Reinforcement learning, Agent, Markov decision processes, Temporal-difference learning, Local state feature},
abstract = {The extension of reinforcement learning (RL) to large state space has inevitably encountered the problem of the curse of dimensionality. Improving the learning efficiency of the agent is much more important to the practical application of RL. Consider learning to optimally solve Markov decision problems in a particular domain, if the domain has particular characteristics that are attributable to each state, the agent might be able to take advantage of these features to direct the future learning. This paper firstly defines the local state feature, then a state feature function is used to generate the local state features of a state. Also a weight function is introduced to adjust current policy to the actions worth exploring. Based on the above, an improved SARSA algorithm, Feature-SARSA, is proposed. We validate our new algorithm by experiment on a complex domain, named Sokoban. The results show that the new algorithm has better performance.}
}
@article{GARCIAALONSO20127929,
title = {A macro-economic model to forecast remittances based on Monte-Carlo simulation and artificial intelligence},
journal = {Expert Systems with Applications},
volume = {39},
number = {9},
pages = {7929-7937},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.01.108},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412001261},
author = {Carlos R. García-Alonso and Esther Arenas-Arroyo and Gabriel M. Pérez-Alcalá},
keywords = {Bayesian networks, Monte Carlo simulation, Fuzzy logic, Remittances},
abstract = {A computer system based on Monte-Carlo simulation and fuzzy logic has been designed, developed and tested to: (i) identify covariates that influence remittances received in a specific country and (ii) explain their behavior throughout the time span involved. The resulting remittance model was designed theoretically, identifying the variables which determined remittances and their dependence relationships, and then developed into a computer cluster. This model aims to be global and is useful for assessing the long term evolution of remittances in scenarios where a rich country is the host (United States of America) while a poor country is the where the migrant is from (El Salvador). By changing the socio-economic characteristics of the countries involved, experts can analyze new socio-economic frameworks to obtain useful conclusions for decision-making processes involving development and sustainability.}
}
@article{JURCICEK2012168,
title = {Reinforcement learning for parameter estimation in statistical spoken dialogue systems},
journal = {Computer Speech & Language},
volume = {26},
number = {3},
pages = {168-192},
year = {2012},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2011.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0885230811000490},
author = {Filip Jurčíček and Blaise Thomson and Steve Young},
keywords = {Spoken dialogue systems, Reinforcement learning, POMDP, Dialogue management},
abstract = {Reinforcement techniques have been successfully used to maximise the expected cumulative reward of statistical dialogue systems. Typically, reinforcement learning is used to estimate the parameters of a dialogue policy which selects the system's responses based on the inferred dialogue state. However, the inference of the dialogue state itself depends on a dialogue model which describes the expected behaviour of a user when interacting with the system. Ideally the parameters of this dialogue model should be also optimised to maximise the expected cumulative reward. This article presents two novel reinforcement algorithms for learning the parameters of a dialogue model. First, the Natural Belief Critic algorithm is designed to optimise the model parameters while the policy is kept fixed. This algorithm is suitable, for example, in systems using a handcrafted policy, perhaps prescribed by other design considerations. Second, the Natural Actor and Belief Critic algorithm jointly optimises both the model and the policy parameters. The algorithms are evaluated on a statistical dialogue system modelled as a Partially Observable Markov Decision Process in a tourist information domain. The evaluation is performed with a user simulator and with real users. The experiments indicate that model parameters estimated to maximise the expected reward function provide improved performance compared to the baseline handcrafted parameters.}
}
@article{GUZZO2022129933,
title = {A system dynamics-based framework for examining Circular Economy transitions},
journal = {Journal of Cleaner Production},
volume = {333},
pages = {129933},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.129933},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621041020},
author = {D. Guzzo and D.C.A. Pigosso and N. Videira and J. Mascarenhas},
keywords = {Circular economy, System dynamics, Simulation models, Sustainability transitions, Policy experimentation},
abstract = {Decision-makers in the public policy and business arenas need tools to deal with multiple sources of complexity in Circular Economy (CE) transitions. System Dynamics (SD) facilitates coping with increased complexity by enabling closed-loop thinking via identifying the causal structures underlying behaviour and permitting to proactively experiment with the system through simulation. This research aims to propose and test an SD-based framework for examining CE transitions to supporting decision-making at the micro-, meso-, and macro-levels. Two inductive model-based cases studies led to formalising the framework, finally tested in a third deductive model-based case study. The framework is built upon the well-known stages for building SD simulation models and complemented with domain-specific activities, guiding questions, and expected outcomes when examining CE transitions. The SD-based framework is the first modelling-oriented prescriptive approach to help researchers and practitioners examining CE transitions on their journeys to understand and facilitate changes through SD simulation models.}
}
@article{FENTON200732,
title = {Predicting software defects in varying development lifecycles using Bayesian nets},
journal = {Information and Software Technology},
volume = {49},
number = {1},
pages = {32-43},
year = {2007},
note = {Most Cited Journal Articles in Software Engineering - 2000},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001194},
author = {Norman Fenton and Martin Neil and William Marsh and Peter Hearty and David Marquez and Paul Krause and Rajat Mishra},
keywords = {Causal models, Dynamic Bayesian networks, Software defects, Decision support},
abstract = {An important decision in software projects is when to stop testing. Decision support tools for this have been built using causal models represented by Bayesian Networks (BNs), incorporating empirical data and expert judgement. Previously, this required a custom BN for each development lifecycle. We describe a more general approach that allows causal models to be applied to any lifecycle. The approach evolved through collaborative projects and captures significant commercial input. For projects within the range of the models, defect predictions are very accurate. This approach enables decision-makers to reason in a way that is not possible with regression-based models.}
}
@article{YOU20191,
title = {Advanced planning for autonomous vehicles using reinforcement learning and deep inverse reinforcement learning},
journal = {Robotics and Autonomous Systems},
volume = {114},
pages = {1-18},
year = {2019},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0921889018302021},
author = {Changxi You and Jianbo Lu and Dimitar Filev and Panagiotis Tsiotras},
keywords = {Reinforcement learning, Inverse reinforcement learning, Deep neural-network, Maximum entropy, Path planning, Autonomous vehicle},
abstract = {Autonomous vehicles promise to improve traffic safety while, at the same time, increase fuel efficiency and reduce congestion. They represent the main trend in future intelligent transportation systems. This paper concentrates on the planning problem of autonomous vehicles in traffic. We model the interaction between the autonomous vehicle and the environment as a stochastic Markov decision process (MDP) and consider the driving style of an expert driver as the target to be learned. The road geometry is taken into consideration in the MDP model in order to incorporate more diverse driving styles. The desired, expert-like driving behavior of the autonomous vehicle is obtained as follows: First, we design the reward function of the corresponding MDP and determine the optimal driving strategy for the autonomous vehicle using reinforcement learning techniques. Second, we collect a number of demonstrations from an expert driver and learn the optimal driving strategy based on data using inverse reinforcement learning. The unknown reward function of the expert driver is approximated using a deep neural-network (DNN). We clarify and validate the application of the maximum entropy principle (MEP) to learn the DNN reward function, and provide the necessary derivations for using the maximum entropy principle to learn a parameterized feature (reward) function. Simulated results demonstrate the desired driving behaviors of an autonomous vehicle using both the reinforcement learning and inverse reinforcement learning techniques.}
}
@article{WANG2024102918,
title = {CGCI: Cross-granularity Causal Inference framework for engineering Change Propagation Analysis},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102918},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102918},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400569X},
author = {Yuxiao Wang and Hongming Cai and Bingqing Shen and Pan Hu and Han Yu and Lihong Jiang},
keywords = {Change Propagation Analysis, Engineering change control, Cross-granularity Influence Algorithm, Probabilistic computing, Causal inference, Decision support},
abstract = {In the dynamic landscape of large-scale and intricate product development, the constant generation and accumulation of configuration data, influenced by factors such as evolving demands and version alterations, exhibit inter-domain and inter-level characteristics. This complexity presents formidable challenges to the management of controlled changes. Central to effective change management is Change Propagation Analysis (CPA), particularly in accurately predicting the potential impacts on affected items. However, conventional CPA methods are insufficient for addressing the challenge of cross-domain, cross-level inference. Therefore, we propose a Cross-granularity Causal Inference Framework (CGCI) tailored for CPA. This framework leverages the diffusion and attenuation of influence, enabling efficient identification of potential configuration items. To assess the feasibility of CGCI, a dataset is constructed using raw industrial configuration data and conducted a comprehensive case study on aircraft configuration change control. The results of our comparative analysis show that CGCI is effective in addressing multi-granularity and multi-hop inference problems, with more comprehensive consideration and less inference overhead in the multi-granularity case.}
}
@article{SOUFIENAYATI2022381,
title = {A methodical interpretation of adaptive robotics: Study and reformulation},
journal = {Neurocomputing},
volume = {512},
pages = {381-397},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.114},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012073},
author = {Amir M. {Soufi Enayati} and Zengjie Zhang and Homayoun Najjaran},
keywords = {Adaptive Robotics, Artificial Intelligence, Machine Adaptability, Machine Learning, Markov Decision Process, Industry 4.0.},
abstract = {The recent development of industrial manufacturing and social services has witnessed a significant trend of automation and intelligentization due to the wide application of robots and the technology of artificial intelligence (AI). While robots liberate humans from tedious and dangerous work in hazardous environments, AI simplifies the programming of robots by automatically inferring patterns and models from the interaction between the robots and the environment. Nevertheless, the application of robots and AI to more general manufacturing and social tasks is still limited by the lack of flexibility and adaptability to the changes in the task and the environment. Thus, a new concept, adaptive robotics, has been proposed to address the desire that an AI-powered robot should be able to properly reprogram itself to these changes without human intervention. Nevertheless, this concept is yet too abstract to provide any specific guidance to the development of robot programs. In this paper, we attempt to provide methodical redefinition and reformulation of adaptive robotics both in conceptual and mathematical manners based on the study of previous results. First of all, we introduce the essential motivation and the conceptual origination of adaptability of mechanical systems. Then, we review the previous literature and explore the related work of adaptive robotics. Based on this, we provide a uniform mathematical formulation of adaptive robotics based on adaptive and robust Markov-decision process (MDP). Through this work, we attempt to inspire the generic framework of adaptive robotics incorporating the existing immature paradigms, by which we are aiming at a clarified and well-defined context of adaptive robotics for future research on related domains.}
}
@article{FERNANDEZ2010866,
title = {Probabilistic Policy Reuse for inter-task transfer learning},
journal = {Robotics and Autonomous Systems},
volume = {58},
number = {7},
pages = {866-871},
year = {2010},
note = {Advances in Autonomous Robots for Service and Entertainment},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2010.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0921889010000655},
author = {Fernando Fernández and Javier García and Manuela Veloso},
keywords = {Reinforcement learning, Transfer learning, Policy Reuse},
abstract = {Policy Reuse is a reinforcement learning technique that efficiently learns a new policy by using past similar learned policies. The Policy Reuse learner improves its exploration by probabilistically including the exploitation of those past policies. Policy Reuse was introduced, and its effectiveness was previously demonstrated, in problems with different reward functions in the same state and action spaces. In this article, we contribute Policy Reuse as transfer learning among different domains. We introduce extended Markov Decision Processes (MDPs) to include domains and tasks, where domains have different state and action spaces, and tasks are problems with different rewards within a domain. We show how Policy Reuse can be applied among domains by defining and using a mapping between their state and action spaces. We use several domains, as versions of a simulated RoboCup Keepaway problem, where we show that Policy Reuse can be used as a mechanism of transfer learning significantly outperforming a basic policy learner.}
}
@article{KUMBURE2024121232,
title = {Causal maps in the analysis and unsupervised assessment of the development of expert knowledge: Quantification of the learning effects for knowledge management purposes},
journal = {Expert Systems with Applications},
volume = {236},
pages = {121232},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121232},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423017347},
author = {Mahinda Mailagaha Kumbure and Anssi Tarkiainen and Jan Stoklasa and Pasi Luukka and Ari Jantunen},
keywords = {Cognitive maps, Knowledge enhancement, Unsupervised assessment, Expert system, Expert knowledge, fsQCA},
abstract = {This study proposes an application of cognitive maps in the representation of cognitive structures of the experts and assessment of their development/modification as a result of a (computer or expert system-assisted) learning process. It strives to identify information needed for the guidance of the process of creation and management of expert knowledge by formal modeling tools. Changes in experts’ cognitive structures are assumed to stem from individual and collaborative (group-level) learning. The novel approach to assessing the outcomes of learning reflected as changes in the cognitive structures of experts or groups of experts, modeled by cognitive maps, does not assume any correct or desired outcome of the learning process to be known in advance. Instead, it identifies and analyzes the changes in (or robustness of) the constituents of the cognitive maps from different points of view and allows for quantifying and visualizing the actual effect of the learning. The proposed methodology can identify changes in cognitive diversity, causal structures in terms of causal relations and concepts, and the perceived importance of strategic issues over the learning period. It can also detect which cause–effect relationships have appeared/disappeared considering the pre-/post-mapping design. Thus, it provides an exploratory account on the changes in the cognitive structures of the expert(s) as a result of learning. The applicability of the proposed methods is illustrated in the assessment of the learning outcomes of a group of 71 graduate students who participated in an eight-week business simulation task. The results of the empirical analysis confirm the viability of the proposed methodology and indicate that the students’ understanding of the utilized concepts and associated relationships in the decision-making process improved throughout the learning activity, ultimately showing that the course learning has considerably improved students’ perception and knowledge. Based on the results, it can be concluded that the proposed approach has the potential to be effective in assessing learning outcomes in teaching–learning activities.}
}
@article{LIEFGREEN2023105382,
title = {Drawing conclusions: Representing and evaluating competing explanations},
journal = {Cognition},
volume = {234},
pages = {105382},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105382},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000161},
author = {Alice Liefgreen and David A. Lagnado},
keywords = {Explanation, Causal models, Evidential reasoning, Simplicity, Mechanism},
abstract = {Despite the increase in studies investigating people's explanatory preferences in the domains of psychology and philosophy, little is known about their preferences in more applied domains, such as the criminal justice system. We show that when people evaluate competing legal accounts of the same evidence, their explanatory preferences are affected by whether they are required to draw causal models of the evidence. In addition, we identify ‘mechanism’ as an explanatory feature that people value when evaluating explanations. Although previous research has shown that people can reason correctly about causality, ours is one of the first studies to show that generating and drawing causal models directly affects people's evaluations of explanations. Our findings have implications for the development of normative models of legal arguments, which have so far adopted a singularly ‘unified’ approach, as well as the development of modelling tools to support people's reasoning and decision-making in applied domains. Finally, they add to the literature on the cognitive basis of evaluating competing explanations in new domains.}
}
@incollection{LITTMAN1995362,
title = {Learning policies for partially observable environments: Scaling up},
editor = {Armand Prieditis and Stuart Russell},
booktitle = {Machine Learning Proceedings 1995},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {362-370},
year = {1995},
isbn = {978-1-55860-377-6},
doi = {https://doi.org/10.1016/B978-1-55860-377-6.50052-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500529},
author = {Michael L. Littman and Anthony R. Cassandra and Leslie Pack Kaelbling},
abstract = {Partially observable Markov decision processes (POMDP's) model decision problems in which an agent tries to maximize its reward in the face of limited and/or noisy sensor feedback. While the study of POMDP's is motivated by a need to address realistic problems, existing techniques for finding optimal behavior do not appear to scale well and have been unable to find satisfactory policies for problems with more than a dozen states. After a brief review of POMDP's, this paper discusses several simple solution methods and shows that all are capable of finding near- optimal policies for a selection of extremely small POMDP'S taken from the learning literature. In contrast, we show that none are able to solve a slightly larger and noisier problem based on robot navigation. We find that a combination of two novel approaches performs well on these problems and suggest methods for scaling to even larger and more complicated domains.}
}
@article{DOSHIVELEZ2012115,
title = {Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {115-132},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000458},
author = {Finale Doshi-Velez and Joelle Pineau and Nicholas Roy},
keywords = {Partially observable Markov decision process, Reinforcement learning, Bayesian methods},
abstract = {Acting in domains where an agent must plan several steps ahead to achieve a goal can be a challenging task, especially if the agentʼs sensors provide only noisy or partial information. In this setting, Partially Observable Markov Decision Processes (POMDPs) provide a planning framework that optimally trades between actions that contribute to the agentʼs knowledge and actions that increase the agentʼs immediate reward. However, the task of specifying the POMDPʼs parameters is often onerous. In particular, setting the immediate rewards to achieve a desired balance between information-gathering and acting is often not intuitive. In this work, we propose an approximation based on minimizing the immediate Bayes risk for choosing actions when transition, observation, and reward models are uncertain. The Bayes-risk criterion avoids the computational intractability of solving a POMDP with a multi-dimensional continuous state space; we show it performs well in a variety of problems. We use policy queries—in which we ask an expert for the correct action—to infer the consequences of a potential pitfall without experiencing its effects. More important for human–robot interaction settings, policy queries allow the agent to learn the reward model without the reward values ever being specified.}
}
@article{JO2024108073,
title = {Airline dynamic pricing with patient customers using deep exploration-based reinforcement learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108073},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108073},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624002318},
author = {Seongbae Jo and Gyu M. Lee and Ilkyeong Moon},
keywords = {Airline revenue management, Dynamic pricing, Deep reinforcement learning, Patient customer},
abstract = {This paper addresses a crucial issue in the airline industry by tackling a dynamic pricing problem in the presence of patient customers, a scenario that has gained significance due to the revenue loss of airlines caused by customers’ non-myopic decision-making. To effectively capture this non-myopic characteristic, we propose a Markov decision process (MDP) including a history of offered prices as a state variable. In contrast to previous studies, distributions of customers’ properties are assumed to be unknown in advance for a more realistic representation of real-world scenarios. To deal with the new challenges of the problem, we propose utilizing a specific learning framework (i.e., deep exploration-based RL) that is unexplored in this domain. The numerical experiments demonstrate that its performance can be improved on the MDP we designed and show that it outperforms the benchmark algorithm. The structures of pricing policies generated from the bootstrapped deep Q-network algorithm imply that airlines should offer high and low prices alternately from the beginning of the sales period rather than increasing prices as time goes on. We also ascertain that more frequent consecutive high-priced periods can increase airlines’ revenue in environments with higher customer patience levels.}
}
@article{PISHDAD2024126570,
title = {Predicting the impacts of anthropogenic drivers on management scenarios using Bayesian belief network in the Zeribar freshwater wetland, Iran},
journal = {Journal for Nature Conservation},
volume = {78},
pages = {126570},
year = {2024},
issn = {1617-1381},
doi = {https://doi.org/10.1016/j.jnc.2024.126570},
url = {https://www.sciencedirect.com/science/article/pii/S1617138124000190},
author = {Leila Pishdad and Amir Sadoddin and Ali Najafinejad},
keywords = {Conservation management, Conceptual framework, Bayesian network, Policy development, Socio-economic factors, Wetland management scenario},
abstract = {The Zeribar wetland in Iran’s western city of Marivan provides a valuable foundation for maintaining various aspects of social and cultural life, from water supply to recreational values, as well as for natural habitats. However, its different services have been put under strong anthropogenic pressure and urbanization processes. The present study is a part of ongoing efforts to improve understanding of the anthropogenic factors potential impact on management status of this vulnerable freshwater ecosystem, aiming to provide integrated management strategies using a Bayesian belief network (BBN) approach. A conceptual framework supporting the BBN was designed according to the main determining factors of the wetland’s management including “human-induced factors”, “economic factors and human capital”, “policy context” and “social and cultural issues”. The BBN model was parameterized using different data sources comprising literature review, face-to-face interviews, filed observation, model simulations, and experts’ knowledge. Stakeholders’ feedback on model accuracy and management strategies application was assessed in different workshops with representatives from locals, executives, and scientific communities. The results showed that the two subsystems of “policy context” and “economic factors and human capital” are the most important key factors influencing the target node, wetland management status. Then, fourteen alternative management scenarios (S2- S15) were generated using the improvement in prior probabilities of influencing factors, followed by evaluation and statistical comparison with the output of the current condition as the base-case scenario (S1). Under the S1 scenario, the probability of occurrence of the “Strong” state, in which the wetland is properly protected from anthropogenic disturbances, for the target node of the network was 25.8 %, while the most significant improvement in the wetland management status (66.4%) was observed after an improvement in all influencing variables was adopted (S15), suggesting a significant potential for improvement in the wetland management. According to the stakeholders’ feedback, the constructed BBN model, and the components included in the management scenarios are useful to fill a critical gap in the current management framework. The results of this work can assist decision-makers in managing the wetland more effectively by considering all socio-economic and anthropogenic dynamics.}
}
@article{BHATTA2023102594,
title = {An integrated control strategy for simultaneous robot assignment, tool change and preventive maintenance scheduling using Heterogeneous Graph Neural Network},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102594},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102594},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000704},
author = {Kshitij Bhatta and Qing Chang},
keywords = {Integrated control, Predictive maintenance, Robot scheduling, Smart manufacturing, Graph neural network, Multi-agent reinforcement learning},
abstract = {There has been a leap in the field of smart manufacturing with the advancement of automation systems, robotic technology, big data analytics, and state-of-the-art Artificial Intelligence (AI) and Machine Learning (ML) algorithms. Three very important aspects of smart manufacturing systems are system productivity, product quality, and maintenance of machines and equipment. These three issues are strongly interrelated and collectively determine the performance of a smart manufacturing system. Although there has been significant studies in production control, quality control and maintenance scheduling to address each of these aspects individually, there has been a lack of sufficient studies taking all of them into consideration in one control scheme. In this paper, a mobile multi-skilled robot operated Flexible Manufacturing System (FMS) is considered and a model that integrates robots, individual workstation processes and product quality is developed using a Heterogeneous Graph Structure. Heterogeneous Graph Neural Network (HGNN) is used to aggregate local information from different nodes of the graph model to create node embeddings that represent global information. A control problem is then formulated in the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) framework to simultaneously consider robot assignment, product quality and maintenance scheduling. The problem is solved using Multi-Agent Reinforcement Learning (MARL). A case study is presented to demonstrate the effectiveness of the HGNN-MARL control strategy by comparing it to three baselines and the naive MARL policy without HGNN.}
}
@article{YOSHINO2015275,
title = {Conversational system for information navigation based on POMDP with user focus tracking},
journal = {Computer Speech & Language},
volume = {34},
number = {1},
pages = {275-291},
year = {2015},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2015.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885230815000042},
author = {Koichiro Yoshino and Tatsuya Kawahara},
keywords = {Spoken dialogue system, Dialogue management, Partially observable Markov decision process (POMDP), Focus in dialogue},
abstract = {We address a spoken dialogue system which conducts information navigation in a style of small talk. The system uses Web news articles as an information source, and the user can receive information about the news of the day through interaction. The goal and procedure of this kind of dialogue are not well defined. An empirical approach based on a partially observable Markov decision process (POMDP) has recently been widely used for dialogue management, but it assumes a definite task goal and information slots, which does not hold in our application system. In this work, we formulate the problem of dialogue management as a selection of modules and optimize it with POMDP by tracking the dialogue state and focus of attention. The POMDP-based dialogue manager receives a user intention that is classified by a spoken language understanding (SLU) component based on logistic regression (LR). The manager also receives a user focus that is detected by the SLU component based on conditional random fields (CRFs). These dialogue states are used for selecting appropriate modules by policy function, which is optimized by reinforcement learning. The reward function is defined by the quality of interaction to encourage long interaction of information navigation with users. The module which responds to user queries is based on a similarity of predicate-argument (P-A) structures that are automatically defined from a domain corpus. It allows for flexible response generation even if the system cannot find exact matching information to the user query. The system also proactively presents information by following the user focus and retrieving a news article based on the similarity measure even if the user does not make any utterance. Experimental evaluations with real dialogue sessions demonstrate that the proposed system outperformed the conventional rule-based system in terms of dialogue state tracking and action selection. Effect of focus detection in the POMDP framework is also confirmed.}
}
@article{ASMONE2020107245,
title = {Development of a design-for-maintainability assessment of building systems in the tropics},
journal = {Building and Environment},
volume = {184},
pages = {107245},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.107245},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320306168},
author = {Ashan Senel Asmone and Michael Yit Lin Chew},
keywords = {Design for maintainability, Maintenance forecasting, Defects, Bayesian belief network, Decision making},
abstract = {The lack of Design for Maintainability (DfM) leads to avoidable building component defects and unproductive building operations. This study reports on the development of a building design assessment system which predicts maintainability impacts of design alternatives. A mixed method approach was used in this study where 1372 DfM benchmarks from 404 national and international building standards were analysed qualitatively to identify benchmarks relating to 123 critical building defects. Five Bayesian Belief Networks (BBN) were developed using five-year defect data and expert judgements from 95 case buildings. Monetary quantification of results was carried out, indicating the complex relationships between DfM benchmarks and building defects to great effect. The Bayesian networks were operationally validated using scenario tests and sensitivity analysis. The predictive power of the developed system is illustrated using an application to a hypothetical case study. This novel consequence-based DfM assessment system is a decision making tool aiding in designing more maintainable building systems which are more productive in terms of materials, labour and cost.}
}
@article{DEAN199535,
title = {Planning under time constraints in stochastic domains},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {35-74},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00086-G},
url = {https://www.sciencedirect.com/science/article/pii/000437029400086G},
author = {Thomas Dean and Leslie Pack Kaelbling and Jak Kirman and Ann Nicholson},
abstract = {We provide a method, based on the theory of Markov decision processes, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods require time at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables). By using information about the starting state, the reward function, and the transition probabilities of the domain, we restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal. Using this restricted set of states, the planner can generate more or less complete plans depending on the time it has available. Our approach employs several iterative refinement routines for solving different aspects of the decision making problem. We describe the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe experimental results for both the precursor and recurrent problems that demonstrate planning times that grow slowly as a function of domain size and compare their performance to other relevant algorithms.}
}
@article{HAN20131310,
title = {A scalable method for DCLC problem using hierarchical MDP model},
journal = {Computer Communications},
volume = {36},
number = {12},
pages = {1310-1316},
year = {2013},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366413001230},
author = {Yue Han and Mingwu Yao and Zengji Liu},
keywords = {DCLC, MDP, Scalability},
abstract = {It is well known that the delay-constrained least-cost (DCLC) routing problem is NP-complete, hence various heuristic methods have been proposed for this problem. However, these heuristic methods have poor scalability as the network scale increases. In this paper we propose a new method based on the Markov Decision Process (MDP) theory and the hierarchical routing scheme to address the scalability issue of the DCLC routing problem. We construct a new two-level hierarchy MDP model and apply an infinite-horizon discounted cost model to the upper level for the end-to-end inter-domain link selection. Since the infinite-horizon discounted cost model is independent of network scale, the scalability problem is resolved. With the proposed model, we further give the algorithm of solving the optimal policy to obtain the DCLC routing. Simulation results show that the proposed method improves the scalability significantly.}
}
@article{PAN2023288,
title = {Reinforcement learning for automatic quadrilateral mesh generation: A soft actor–critic approach},
journal = {Neural Networks},
volume = {157},
pages = {288-304},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2022.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S089360802200418X},
author = {Jie Pan and Jingwei Huang and Gengdong Cheng and Yong Zeng},
keywords = {Reinforcement learning, Mesh generation, Soft actor–critic, Neural networks, Computational geometry, Quadrilateral mesh},
abstract = {This paper proposes, implements, and evaluates a reinforcement learning (RL)-based computational framework for automatic mesh generation. Mesh generation plays a fundamental role in numerical simulations in the area of computer aided design and engineering (CAD/E). It is identified as one of the critical issues in the NASA CFD Vision 2030 Study. Existing mesh generation methods suffer from high computational complexity, low mesh quality in complex geometries, and speed limitations. These methods and tools, including commercial software packages, are typically semiautomatic and they need inputs or help from human experts. By formulating the mesh generation as a Markov decision process (MDP) problem, we are able to use a state-of-the-art reinforcement learning (RL) algorithm called “soft actor-critic” to automatically learn from trials the policy of actions for mesh generation. The implementation of this RL algorithm for mesh generation allows us to build a fully automatic mesh generation system without human intervention and any extra clean-up operations, which fills the gap in the existing mesh generation tools. In the experiments to compare with two representative commercial software packages, our system demonstrates promising performance with respect to scalability, generalizability, and effectiveness.}
}
@article{GOUDARZI2019529,
title = {A hybrid intelligent model for network selection in the industrial Internet of Things},
journal = {Applied Soft Computing},
volume = {74},
pages = {529-546},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618305878},
author = {Shidrokh Goudarzi and Mohammad Hossein Anisi and Abdul Hanan Abdullah and Jaime Lloret and Seyed Ahmad Soleymani and Wan Haslina Hassan},
keywords = {Industrial Internet of Things, Vertical handover, Markov decision process, Heterogeneous wireless networks},
abstract = {Industrial Internet of Things (IIoT) plays an important role in increasing productivity and efficiency in heterogeneous wireless networks. However, different domains such as industrial wireless scenarios, small cell domains and vehicular ad hoc networks (VANET) require an efficient machine learning/intelligent algorithm to process the vertical handover decision that can maintain mobile terminals (MTs) in the preferable networks for a sufficient duration of time. The preferred quality of service parameters can be differentiated from all the other MTs. Hence, in this paper, the problem with the vertical handoff (VHO) decision is articulated as the process of the Markov decision aimed to maximize the anticipated total rewards as well as to minimize the handoffs’ average count. A rewards function is designed to evaluate the QoS at the point of when the connections take place, as that is where the policy decision for a stationary deterministic handoff can be established. The proposed hybrid model merges the biogeography-based optimization (BBO) with the Markov decision process (MDP). The MDP is utilized to establish the radio access technology (RAT) selection’s probability that behaves as an input to the BBO process. Therefore, the BBO determines the best RAT using the described multi-point algorithm in the heterogeneous network. The numerical findings display the superiority of this paper’s proposed schemes in comparison with other available algorithms. The findings shown that the MDP-BBO algorithm is able to outperform other algorithms in terms of number of handoffs, bandwidth availability, and decision delays. Our algorithm displayed better expected total rewards as well as a reduced average account of handoffs compared to current approaches. Simulation results obtained from Monte-Carlo experiments prove validity of the proposed model.}
}
@article{ZHOU2024827,
title = {Optimization of multi-echelon spare parts inventory systems using multi-agent deep reinforcement learning},
journal = {Applied Mathematical Modelling},
volume = {125},
pages = {827-844},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2023.10.039},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X23004936},
author = {Yifan Zhou and Kai Guo and Cheng Yu and Zhisheng Zhang},
keywords = {Inventory optimization, Emergency transshipment, Multi-agent deep reinforcement learning, Twin delayed deep deterministic policy gradient, Value decomposition},
abstract = {Multi-echelon inventory systems are commonly used in practice to satisfy widely distributed random demands of spare parts in an efficient and cost-effective manner. Optimization of a multi-echelon inventory system is a decision-making problem under uncertainties. Classic inventory policies (e.g. (s, S) and (R, Q)) that do not consider the inventory positions of other warehouses become suboptimal due to interrelationships among different warehouses caused by transshipment. The Markov decision process (MDP) is an effective tool for inventory optimization, which does not require a predetermined parameterized policy structure. Unfortunately, both the state and action spaces of MDP suffer from the curse of dimensionality when the number of warehouses increases. This paper optimizes the inventory of a large-scale multi-echelon inventory system using a new multi-agent deep reinforcement learning (MADRL) algorithm named EM-VDTD3 that is developed by introducing value decomposition and experience buffer modification into the twin delayed deep deterministic policy gradient (TD3) algorithm. Each agent in EM-VDTD3 manages a subsystem in the multi-echelon inventory system. Because different agents share the same network parameters, networks are customized to process subsystems with different parameters. Domain knowledge of inventory control is embedded in the learning process of EM-VDTD3 by adding expert experiences to the experience buffer. An efficient approximate method is developed to identify a teacher policy that generates expert experiences. Numerical studies about a spare part inventory system in the wind energy industry show that the proposed EM-VDTD3 outperforms benchmark methods.}
}
@article{KHAKZAD2015289,
title = {Risk-based design of process plants with regard to domino effects and land use planning},
journal = {Journal of Hazardous Materials},
volume = {299},
pages = {289-297},
year = {2015},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2015.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0304389415004690},
author = {Nima Khakzad and Genserik Reniers},
keywords = {Bayesian network, Domino effect, Land use planning, Multi-criteria decision analysis, Fuel storage plant},
abstract = {Land use planning (LUP) as an effective and crucial safety measure has widely been employed by safety experts and decision makers to mitigate off-site risks posed by major accidents. Accordingly, the concept of LUP in chemical plants has traditionally been considered from two perspectives: (i) land developments around existing chemical plants considering potential off-site risks posed by major accidents and (ii) development of existing chemical plants considering nearby land developments and the level of additional off-site risks the land developments would be exposed to. However, the attempts made to design chemical plants with regard to LUP requirements have been few, most of which have neglected the role of domino effects in risk analysis of major accidents. To overcome the limitations of previous work, first, we developed a Bayesian network methodology to calculate both on-site and off-site risks of major accidents while taking domino effects into account. Second, we combined the results of risk analysis with Analytic Hierarchical Process to design an optimal layout for which the levels of on-site and off-site risks would be minimum.}
}
@incollection{KALA2024669,
title = {Chapter 14 - Motion planning using reinforcement learning},
editor = {Rahul Kala},
booktitle = {Autonomous Mobile Robots},
publisher = {Academic Press},
pages = {669-713},
year = {2024},
series = {Emerging Methodologies and Applications in Modelling},
isbn = {978-0-443-18908-1},
doi = {https://doi.org/10.1016/B978-0-443-18908-1.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443189081000169},
author = {Rahul Kala},
keywords = {Reinforcement learning, Q-learning, Deep Q-learning, Value learning, Policy gradients, Inverse Reinforcement Learning, Feature expectation maximization, Generative Adversarial Imitation Learning, Value iteration, Partially observable Markov decision process},
abstract = {In this chapter, we consider the stochastic environments where the agent does not land at the desired state due to noises. In reinforcement learning, an agent learns by acting in the environment and observing the rewards received, planning to maximize the total cumulative rewards called the utility. The Q-learning selects action using a mixture of exploration and exploitation, which is acted in the world to give the next state and reward, which is used to correct the Q-values (utility of a state–action pair). Deep reinforcement learning techniques use deep neural networks that make the utility and/or policy functions and facilitate giving raw sensor readings to compute the best action. The networks are pretrained by using demonstrations provided by an expert called imitation learning. Inverse Reinforcement Learning is used to extract a reward function from a few demonstrations. The Partially Observable Markov Decision Process is used for the partially observable environments.}
}

@article{BORAL2021105768,
title = {Failure analysis of CNC machines due to human errors: An integrated IT2F-MCDM-based FMEA approach},
journal = {Engineering Failure Analysis},
volume = {130},
pages = {105768},
year = {2021},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2021.105768},
url = {https://www.sciencedirect.com/science/article/pii/S1350630721006294},
author = {Soumava Boral and Shankar Chakraborty},
keywords = {CNC machine, Human error, FMEA, Interval type-2 fuzzy sets, IT2F-MARCOS},
abstract = {Computer numerical control (CNC) machining has become one of the salient transformative technologies due to its ability to manufacture precise components, scalable production, constant uptime, uniformity and repeatability. Despite the claim of minimum human intervention during CNC machining operation, operators’ involvement leading to CNC machine failures cannot be totally neglected. In this paper, a modified failure modes and effects analysis (FMEA)-based risk analysis approach is presented to identify the major human errors, their causes and effects during CNC machining operation based on the participation of a team of cross-functional experts. An integrated interval type-2 fuzzy sets (IT2FSs)-based multi-criteria decision making (MCDM) framework is subsequently developed for prioritizing the risks associated with each identified human error with respect to four pertinent risk factors, i.e. safety, economic consequences, occurrence and detection. However, due to lack of exact numerical values, the errors are linguistically assessed by the experts. To deal with subjective uncertainties, IT2FSs are chosen as a viable alternative due to their numerous superiorities over the traditional type-1 fuzzy sets (T1FSs). The IT2F-analytic hierarchy process (IT2F-AHP) is adopted to calculate weights of the risk factors, and IT2F-decision making trial and evaluation laboratory (IT2F-DEMATEL) is employed to model the causal interactions among different errors. For risk ranking, the traditional measurement of alternatives and ranking according to compromise solution (MARCOS) method is extended in IT2F-domain (IT2F-MARCOS). Finally, the computed risk ranking results are contrasted with other popular IT2FSs and T1FSs-based MCDM methods to validate feasibility of the proposed approach. This integrated framework comprising IT2F-AHP, IT2F-DEMATEL and IT2F-MARCOS is novel with respect to its development as well as application context and would surely be an interesting tool helping the entire decision making community.}
}
@article{SONG20241604,
title = {Research on energy management strategy of fuel-cell vehicles based on nonlinear model predictive control},
journal = {International Journal of Hydrogen Energy},
volume = {50},
pages = {1604-1621},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.07.304},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923038570},
author = {Ke Song and Xing Huang and Zhen Cai and Pengyu Huang and Feiqiang Li},
keywords = {Fuel cell vehicle, Energy management strategy, Model predictive control, Markov Monte Carlo method},
abstract = {Fuel cell hybrid electric vehicles (FCHEV) are one of the most promising new energy vehicles. The cost and lifetime of its powertrain have limited its commercial development. This paper proposed an energy management strategy based on nonlinear model predictive control (NMPC) technology to solve the economy and durability problem of FCHEVs. Based on Markov Monte Carlo(MCMC) method, a prediction model of multi-scale operating conditions is established, and dynamic programming(DP) is used to realize the optimal control in the predicted time domain. The “constant speed prediction” is innovatively adopted in the transition stage to improve the prediction accuracy and enable the model to be realized online. The ways to reduce calculating amount of NMPC are also discussed in this paper. This simplification leads to suboptimal fuel economy and durability of control system but can have obvious reduction in calculating time. The simulation results show that, compared with the thermostat strategy and the power following strategy, the degradation cost decrease of 11.1% and 23.9% and the total operation cost of NMPC decrease of 11.0% and 23.5% respectively. The NMPC strategy has better economy and durability than the rule-based energy management strategy, is close to the global optimal result obtained by dynamic programming and can meet the requirements of real-time control.}
}
@article{HUANG2023e13883,
title = {A review of the application of artificial intelligence to nuclear reactors: Where we are and what's next},
journal = {Heliyon},
volume = {9},
number = {3},
pages = {e13883},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13883},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023010903},
author = {Qingyu Huang and Shinian Peng and Jian Deng and Hui Zeng and Zhuo Zhang and Yu Liu and Peng Yuan},
keywords = {Artificial intelligence, Causal learning, Nuclear reactors, SciML, XAI},
abstract = {As a form of clean energy, nuclear energy has unique advantages compared to other energy sources in the present era, where low-carbon policies are being widely advocated. The exponential growth of artificial intelligence (AI) technology in recent decades has resulted in new opportunities and challenges in terms of improving the safety and economics of nuclear reactors. This study briefly introduces modern AI algorithms such as machine learning, deep learning, and evolutionary computing. Furthermore, several studies on the use of AI techniques for nuclear reactor design optimization as well as operation and maintenance (O&M) are reviewed and discussed. The existing obstacles that prevent the further fusion of AI and nuclear reactor technologies so that they can be scaled to real-world problems are classified into two categories: (1) data issues: insufficient experimental data increases the possibility of data distribution drift and data imbalance; (2) black-box dilemma: methods such as deep learning have poor interpretability. Finally, this study proposes two directions for the future fusion of AI and nuclear reactor technologies: (1) better integration of domain knowledge with data-driven approaches to reduce the high demand for data and improve the model performance and robustness; (2) promoting the use of explainable artificial intelligence (XAI) technologies to enhance the transparency and reliability of the model. In addition, causal learning warrants further attention owing to its inherent ability to solve out-of-distribution generalization (OODG) problems.}
}
@article{THOMSEN201612,
title = {A Bayesian belief network approach for assessing uncertainty in conceptual site models at contaminated sites},
journal = {Journal of Contaminant Hydrology},
volume = {188},
pages = {12-28},
year = {2016},
issn = {0169-7722},
doi = {https://doi.org/10.1016/j.jconhyd.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169772216300213},
author = {Nanna I. Thomsen and Philip J. Binning and Ursula S. McKnight and Nina Tuxen and Poul L. Bjerg and Mads Troldborg},
keywords = {Bayesian belief networks, Contaminated sites, Conceptual models, Conceptual uncertainty, Screening level},
abstract = {A key component in risk assessment of contaminated sites is in the formulation of a conceptual site model (CSM). A CSM is a simplified representation of reality and forms the basis for the mathematical modeling of contaminant fate and transport at the site. The CSM should therefore identify the most important site-specific features and processes that may affect the contaminant transport behavior at the site. However, the development of a CSM will always be associated with uncertainties due to limited data and lack of understanding of the site conditions. CSM uncertainty is often found to be a major source of model error and it should therefore be accounted for when evaluating uncertainties in risk assessments. We present a Bayesian belief network (BBN) approach for constructing CSMs and assessing their uncertainty at contaminated sites. BBNs are graphical probabilistic models that are effective for integrating quantitative and qualitative information, and thus can strengthen decisions when empirical data are lacking. The proposed BBN approach facilitates a systematic construction of multiple CSMs, and then determines the belief in each CSM using a variety of data types and/or expert opinion at different knowledge levels. The developed BBNs combine data from desktop studies and initial site investigations with expert opinion to assess which of the CSMs are more likely to reflect the actual site conditions. The method is demonstrated on a Danish field site, contaminated with chlorinated ethenes. Four different CSMs are developed by combining two contaminant source zone interpretations (presence or absence of a separate phase contamination) and two geological interpretations (fractured or unfractured clay till). The beliefs in each of the CSMs are assessed sequentially based on data from three investigation stages (a screening investigation, a more detailed investigation, and an expert consultation) to demonstrate that the belief can be updated as more information becomes available.}
}
@article{FISCHERABAIGAR2024101976,
title = {Bridging the gap: Towards an expanded toolkit for AI-driven decision-making in the public sector},
journal = {Government Information Quarterly},
volume = {41},
number = {4},
pages = {101976},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101976},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000686},
author = {Unai Fischer-Abaigar and Christoph Kern and Noam Barda and Frauke Kreuter},
keywords = {Automated decision-making, Reliable artificial intelligence, Public policy, Causal machine learning},
abstract = {AI-driven decision-making systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, these systems face the challenge of aligning machine learning (ML) models with the complex realities of public sector decision-making. In this paper, we examine five key challenges where misalignment can occur, including distribution shifts, label bias, the influence of past decision-making on the data side, as well as competing objectives and human-in-the-loop on the model output side. Our findings suggest that standard ML methods often rely on assumptions that do not fully account for these complexities, potentially leading to unreliable and harmful predictions. To address this, we propose a shift in modeling efforts from focusing solely on predictive accuracy to improving decision-making outcomes. We offer guidance for selecting appropriate modeling frameworks, including counterfactual prediction and policy learning, by considering how the model estimand connects to the decision-maker's utility. Additionally, we outline technical methods that address specific challenges within each modeling approach. Finally, we argue for the importance of external input from domain experts and stakeholders to ensure that model assumptions and design choices align with real-world policy objectives, taking a step towards harmonizing AI and public sector objectives.}
}
@article{RAHMADI2017687,
title = {Causality on cross-sectional data: Stable specification search in constrained structural equation modeling},
journal = {Applied Soft Computing},
volume = {52},
pages = {687-698},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1568494616305130},
author = {Ridho Rahmadi and Perry Groot and Marianne Heins and Hans Knoop and Tom Heskes},
keywords = {Causal modeling, Structural equation model, Stability selection, Multi-objective evolutionary algorithm, NSGA-II},
abstract = {Causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms. Generally discovery algorithms can be divided into two approaches: constraint-based and score-based. The constraint-based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable. The score-based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships, but it is unable to detect common confounders of the observed variables. A drawback of both score-based and constrained-based approaches is the inherent instability in structure estimation. With finite samples small changes in the data can lead to completely different optimal structures. The present work introduces a new hypothesis-free score-based causal discovery algorithm, called stable specification search, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. Structure search is performed over structural equation models. Our approach uses exploratory search but allows incorporation of prior background knowledge. We validated our approach on one simulated data set, which we compare to the known ground truth, and two real-world data sets for chronic fatigue syndrome and attention deficit hyperactivity disorder, which we compare to earlier medical studies. The results on the simulated data set show significant improvement over alternative approaches and the results on the real-word data sets show consistency with the hypothesis driven models constructed by medical experts.}
}
@article{ISLAM201912,
title = {A knowledge-based expert system to assess power plant project cost overrun risks},
journal = {Expert Systems with Applications},
volume = {136},
pages = {12-32},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304300},
author = {Muhammad Saiful Islam and Madhav P. Nepal and Martin Skitmore and Golam Kabir},
keywords = {Cost overruns, Risk assessment, Power plant projects, Fuzzy logic, Canonical model},
abstract = {Preventing cost overruns of such infrastructure projects as power plants is a global project management problem. The existing risk assessment methods/models have limitations to address the complicated nature of these projects, incorporate the probabilistic causal relationships of the risks and probabilistic data for risk assessment, by taking into account the domain experts’ judgments, subjectivity, and uncertainty involved in their judgments in the decision making process. A knowledge-based expert system is presented to address this issue, using a fuzzy canonical model (FCM) that integrates the fuzzy group decision-making approach (FGDMA) and the Canonical model (i.e. a modified Bayesian belief network model). The FCM overcomes: (a) the subjectivity and uncertainty involved in domain experts’ judgment, (b) significantly reduces the time and effort needed for the domain experts in eliciting conditional probabilities of the risks involved in complex risk networks, and (c) reduces the model development tasks, which also reduces the computational load on the model. This approach advances the applications of fuzzy-Bayesian models for cost overrun risks assessment in a complex and uncertain project environment by addressing the major constraints associated with such models. A case study demonstrates and tests the application of the model for cost overrun risk assessment in the construction and commissioning phase of a power plant project, confirming its ability to pinpoint the most critical risks involved ̶ in this case, the complexity of the lifting and rigging heavy equipment, inadequate work inspection and testing plan, inadequate site/soil investigation, unavailability of the resources in the local market, and the contractor's poor planning and scheduling.}
}
@article{ROWAN2025100164,
title = {A systematic review of machine learning-based microscopic traffic flow models and simulations},
journal = {Communications in Transportation Research},
volume = {5},
pages = {100164},
year = {2025},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2025.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2772424725000046},
author = {Davies Rowan and Haitao He and Fang Hui and Ali Yasir and Quddus Mohammed},
keywords = {Car-following (CF), Lane-changing (LC), Machine learning (ML), Traffic simulation, Microscopic traffic flow model, Traffic flow theory},
abstract = {Microscopic traffic flow models and simulations are crucial for capturing vehicle interactions and analyzing traffic. They can provide critical insights for transport planning, management, and operation through scenario testing and optimization. With the growing availability of high-resolution data and rapid advancements in machine learning (ML) techniques, ML-based microscopic traffic flow models are emerging as promising alternatives to traditional physical models, offering improved accuracy and greater flexibility. Although many models have been developed, comprehensive studies that critically assess the strengths and weaknesses of these models and the overall ML-based approach are lacking. To fill this gap, this study presents a systematic review of ML-based microscopic traffic flow models and simulations, covering both car-following and lane-changing behaviors. This review identifies key areas for future research, including the development of methods to improve model transferability across different operational design domains, the need to capture both driver-specific and location-specific heterogeneity via benchmark datasets, and the incorporation of advanced ML techniques such as meta-learning, federated learning, and causal learning. Additionally, enhancing model interpretability, accounting for mesoscopic and macroscopic traffic impacts, incorporating physical constraints in model training, and developing ML models designed for autonomous vehicles are crucial for the practical adoption of ML-based microscopic models in traffic simulations.}
}
@article{GUL2022101173,
title = {Performance evaluation of Turkish Universities by an integrated Bayesian BWM-TOPSIS model},
journal = {Socio-Economic Planning Sciences},
volume = {80},
pages = {101173},
year = {2022},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2021.101173},
url = {https://www.sciencedirect.com/science/article/pii/S0038012121001658},
author = {Muhammet Gul and Melih Yucesan},
keywords = {Bayesian BWM, Higher education, TOPSIS, Turkish universities, University ranking},
abstract = {This study aims to develop a university ranking model with the aid of performance measures in the “University monitoring and evaluation reports-2019” published by the Council of Higher Education Institution in Turkey. In this context, some of the performance criteria stated in these reports are filtered and 34 sub-criteria under five main criteria are weighted using the Bayesian Best-Worst Method (BBWM). Then, 189 listed public and private universities are ranked using the TOPSIS multicriteria decision-making (MCDM) method. We have adopted the MCDM concept because many evaluation criteria/sub-criteria and alternative universities are taken into account. For this study, we apply an integrated MCDM model. First, we use BBWM to accomplish the first goal and adopt the TOPSIS method for the second purpose, using the BBWM results. Our purpose in using BBWM is due to its probabilistic structure that reduces the loss of information when handling group decisions. In this context, the evaluations of 11 academic experts are combined and a solid weighting is made by obtaining the credal rankings of performance criteria. Using TOPSIS is its logic of proximity to ideal and the ability to evaluate many alternatives. In the context of the study, state-private university-based, nomenclature of territorial units for statistics-2 (NUTS-2)-based and classical geographical regions-based rankings are also discussed. The study seeks to help universities optimize their performance efficiently. The results of the study can be adapted as a reference for other educational institutions and public institutions in their efforts to evaluate, improve their performance and form various policies.}
}
@article{LAW2017142,
title = {Projecting the performance of conservation interventions},
journal = {Biological Conservation},
volume = {215},
pages = {142-151},
year = {2017},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2017.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0006320717305219},
author = {Elizabeth A. Law and Paul J. Ferraro and Peter Arcese and Brett A. Bryan and Katrina Davis and Ascelin Gordon and Matthew H. Holden and Gwenllian Iacona and Raymundo {Marcos Martinez} and Clive A. McAlpine and Jonathan R. Rhodes and Jocelyne S. Sze and Kerrie A. Wilson},
keywords = {Causal inference, Evidence-based policy, Policy evaluation, Prediction, Projection, Transportability},
abstract = {Successful decision-making for environmental management requires evidence of the performance and efficacy of proposed conservation interventions. Projecting the future impacts of prospective conservation policies and programs is challenging due to a range of complex ecological, economic, social and ethical factors, and in particular the need to extrapolate models to novel contexts. Yet many extrapolation techniques currently employed are limited by unfounded assumptions of causality and a reliance on potentially biased inferences drawn from limited data. We show how these restrictions can be overcome by established and emerging techniques from causal inference, scenario analysis, systematic review, expert elicitation, and global sensitivity analysis. These technical advances provide avenues to untangle cause from correlation, evaluate and transfer models between contexts, characterize uncertainty, and address imperfect data. With more rigorous projections of prospective performance of interventions, scientists can deliver policy and program advice that is more scientifically credible.}
}
@article{CHEN2025135115,
title = {Enhanced applicability of reinforcement learning-based energy management by pivotal state-based Markov trajectories},
journal = {Energy},
volume = {319},
pages = {135115},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135115},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225007571},
author = {Jiaxin Chen and Xiaolin Tang and Meng Wang and Cheng Li and Zhangyong Li and Yechen Qin},
keywords = {Hybrid electric vehicle, Energy management, Reinforcement learning, Enhanced applicability},
abstract = {Data- or sample-driven reinforcement learning (RL) is crucial for advancing AI models, enabling supervised learning-based AI to evolve autonomously. However, sample efficiency remains a key challenge, and simply increasing the number of training samples is not a guaranteed solution. More importantly, the focus should be on the breadth and diversity of the data distribution. This paper focuses on hybrid electric vehicles, with an emphasis on energy management. A novel training scheme for RL-based energy-saving policies is proposed, which relies on pivotal Markov transitions as state-based trajectories, significantly enhancing the adaptability of learning-based strategies. Firstly, the contradictions and limitations of the optimization terms in traditional reward functions are highlighted, including the misguidance of cumulative states and the cumbersome adjustment of weights. To address these issues, an unweighted reward is designed to simplify the training process and make it more universal. Secondly, the state-based featured driving cycle, as a novel concept, employs a 'question bank' style environment to expose the RL agent to a more diverse state space. Even with more sources and larger volumes of velocity data, the representative driving cycle can be condensed into customizable lengths of time domain, serving as the pivotal state-based Markov trajectory. Finally, after finishing offline training on the Tencent cloud server, an online driver-in-the-loop test is performed. The core advantage of the proposed strategy lies in completing the training in one go while offering greater applicability, aligning with the training concept more suitable for RL-based agents.}
}
@article{SANTELICES2010713,
title = {Development of a Hybrid Decision Support Model for Optimal Ventricular Assist Device Weaning},
journal = {The Annals of Thoracic Surgery},
volume = {90},
number = {3},
pages = {713-720},
year = {2010},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2010.03.073},
url = {https://www.sciencedirect.com/science/article/pii/S0003497510006855},
author = {Linda C. Santelices and Yajuan Wang and Don Severyn and Marek J. Druzdzel and Robert L. Kormos and James F. Antaki},
abstract = {Background
Despite the small but promising body of evidence for cardiac recovery in patients that have received ventricular assist device (VAD) support, the criteria for identifying and selecting candidates who might be weaned from a VAD have not been established.
Methods
A clinical decision support system was developed based on a Bayesian Belief Network that combined expert knowledge with multivariate statistical analysis. Expert knowledge was derived from interviews of 11 members of the Artificial Heart Program at the University of Pittsburgh Medical Center. This was supplemented by retrospective clinical data from the 19 VAD patients considered for weaning between 1996 and 2004. Artificial Neural Networks and Natural Language Processing were used to mine these data and extract sensitive variables.
Results
Three decision support models were compared. The model exclusively based on expert-derived knowledge was the least accurate and most conservative. It underestimated the incidence of heart recovery, incorrectly identifying 4 of the successfully weaned patients as transplant candidates. The model derived exclusively from clinical data performed better but misidentified 2 patients: 1 weaned successfully, and 1 that needed a cardiac transplant ultimately. An expert-data hybrid model performed best, with 94.74% accuracy and 75.37% to 99.07% confidence interval, misidentifying only 1 patient weaned from support.
Conclusions
A clinical decision support system may facilitate and improve the identification of VAD patients who are candidates for cardiac recovery and may benefit from VAD removal. It could be potentially used to translate success of active centers to those less established and thereby expand use of VAD therapy.}
}
@article{CANKAYA2023102906,
title = {Evidence-based managerial decision-making with machine learning: The case of Bayesian inference in aviation incidents},
journal = {Omega},
volume = {120},
pages = {102906},
year = {2023},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2023.102906},
url = {https://www.sciencedirect.com/science/article/pii/S0305048323000701},
author = {Burak Cankaya and Kazim Topuz and Dursun Delen and Aaron Glassman},
keywords = {Decision support systems, Business analytics, Big data, Aviation incidents, Bayesian belief networks},
abstract = {Understanding the factors behind aviation incidents is essential, not only because of the lethality of the accidents but also the incidents' direct and indirect economic impact. Even minor incidents trigger significant economic damage and create disruptions to aviation operations. It is crucial to investigate these incidents to understand the underlying reasons and hence, reduce the risk associated with physical and financial safety in a precarious industry like aviation. The findings may provide decision-makers with a causally accurate means of investigating the topic while untangling the difficulties concerning the statistical associations and causal effects. This research aims to identify the significant variables and their probabilistic dependencies/relationships determining the degree of aircraft damage. The value and the contribution of this study include (1) developing a fully automatic ML prediction-based DSS for aircraft damage severity, (2) conducting a deep network analysis of affinity between predicting variables using probabilistic graphical modeling (PGM), and (3) implementing a user-friendly dashboard to interpret the business insight coming from the design and development of the Bayesian Belief Network (BBN). By leveraging a large, real-world dataset, the proposed methodology captures the probability-based interrelations among air terminal, flight, flight crew, and air-vehicle-related characteristics as explanatory variables, thereby revealing the underlying, complex interactions in accident severity. This research contributes significantly to the current body of knowledge by defining and proving a methodology for automatically categorizing aircraft damage severity based on flight, aircraft, and PIC (pilot in command) information. Moreover, the study combines the findings of the Bayesian Belief Networks with decades of aviation expertise of the subject matter expert, drawing and explaining the association map to find the root causes of the problems and accident relayed variables.}
}
@article{WANG2020106421,
title = {GRL: Knowledge graph completion with GAN-based reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {209},
pages = {106421},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106421},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120305505},
author = {Qi Wang and Yuede Ji and Yongsheng Hao and Jie Cao},
keywords = {Knowledge graph, Knowledge graph completion, Reinforcement learning, Deep learning},
abstract = {Knowledge graph completion intends to infer the entities that need to be queried through the entities and relations known in the knowledge graphs. It is used in many applications, such as question and answer systems, and searching engines. As the completion process can be represented as a Markov process, existing works would solve this problem with reinforcement learning. However, there are three issues blocking them from achieving high accuracy, which are reward sparsity, missing specific domain rules, and ignoring the generation of knowledge graphs. In this paper, we design a generative adversarial net (GAN)-based reinforcement learning model, named GRL, for knowledge graph completion. First, GRL employs the graph convolutional network to embed the knowledge graphs into the low-dimensional space. Second, GRL employs both GAN and long short-term memory (LSTM) to record trajectory sequences obtained by the agent from traversing the knowledge graph and generate new trajectory sequences if needed. At the same time, GRL applies domain-specific rules accordingly. Finally, GRL employs the deep deterministic policy gradient method to optimize both rewards and adversarial loss. The experiments show that GRL is able to both generate better policies and outperform traditional methods for several tasks.}
}
@article{VANGERVEN200745,
title = {Predicting carcinoid heart disease with the noisy-threshold classifier},
journal = {Artificial Intelligence in Medicine},
volume = {40},
number = {1},
pages = {45-55},
year = {2007},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2006.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0933365706001400},
author = {Marcel A.J. {van Gerven} and Rasa Jurgelenaite and Babs G. Taal and Tom Heskes and Peter J.F. Lucas},
keywords = {Carcinoid heart disease, Bayesian classification, Causal independence, Noisy-threshold model},
abstract = {Summary
Objective
To predict the development of carcinoid heart disease (CHD), which is a life-threatening complication of certain neuroendocrine tumors. To this end, a novel type of Bayesian classifier, known as the noisy-threshold classifier, is applied.
Materials and methods
Fifty-four cases of patients that suffered from a low-grade midgut carcinoid tumor, of which 22 patients developed CHD, were obtained from the Netherlands Cancer Institute (NKI). Eleven attributes that are known at admission have been used to classify whether the patient develops CHD. Classification accuracy and area under the receiver operating characteristics (ROC) curve of the noisy-threshold classifier are compared with those of the naive-Bayes classifier, logistic regression, the decision-tree learning algorithm C4.5, and a decision rule, as formulated by an expert physician.
Results
The noisy-threshold classifier showed the best classification accuracy of 72% correctly classified cases, although differences were significant only for logistic regression and C4.5. An area under the ROC curve of 0.66 was attained for the noisy-threshold classifier, and equaled that of the physician’s decision-rule.
Conclusions
The noisy-threshold classifier performed favorably to other state-of-the-art classification algorithms, and equally well as a decision-rule that was formulated by the physician. Furthermore, the semantics of the noisy-threshold classifier make it a useful machine learning technique in domains where multiple causes influence a common effect.}
}
@article{BRAHMA2024,
title = {Development of a Cohort Analytics Tool for Monitoring Progression Patterns in Cardiovascular Diseases: Advanced Stochastic Modeling Approach},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/59392},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001297},
author = {Arindam Brahma and Samir Chatterjee and Kala Seal and Ben Fitzpatrick and Youyou Tao},
keywords = {healthcare analytics, eHealth, disease monitoring, cardiovascular disease, disease progression model, myocardial, stroke, decision support, continuous-time Markov chain model, stochastic model, stochastic, Markov, cardiology, cardiovascular, heart, monitoring, progression},
abstract = {Background
The World Health Organization (WHO) reported that cardiovascular diseases (CVDs) are the leading cause of death worldwide. CVDs are chronic, with complex progression patterns involving episodes of comorbidities and multimorbidities. When dealing with chronic diseases, physicians often adopt a “watchful waiting” strategy, and actions are postponed until information is available. Population-level transition probabilities and progression patterns can be revealed by applying time-variant stochastic modeling methods to longitudinal patient data from cohort studies. Inputs from CVD practitioners indicate that tools to generate and visualize cohort transition patterns have many impactful clinical applications. The resultant computational model can be embedded in digital decision support tools for clinicians. However, to date, no study has attempted to accomplish this for CVDs.
Objective
This study aims to apply advanced stochastic modeling methods to uncover the transition probabilities and progression patterns from longitudinal episodic data of patient cohorts with CVD and thereafter use the computational model to build a digital clinical cohort analytics artifact demonstrating the actionability of such models.
Methods
Our data were sourced from 9 epidemiological cohort studies by the National Heart Lung and Blood Institute and comprised chronological records of 1274 patients associated with 4839 CVD episodes across 16 years. We then used the continuous-time Markov chain method to develop our model, which offers a robust approach to time-variant transitions between disease states in chronic diseases.
Results
Our study presents time-variant transition probabilities of CVD state changes, revealing patterns of CVD progression against time. We found that the transition from myocardial infarction (MI) to stroke has the fastest transition rate (mean transition time 3, SD 0 days, because only 1 patient had a MI-to-stroke transition in the dataset), and the transition from MI to angina is the slowest (mean transition time 1457, SD 1449 days). Congestive heart failure is the most probable first episode (371/840, 44.2%), followed by stroke (216/840, 25.7%). The resultant artifact is actionable as it can act as an eHealth cohort analytics tool, helping physicians gain insights into treatment and intervention strategies. Through expert panel interviews and surveys, we found 9 application use cases of our model.
Conclusions
Past research does not provide actionable cohort-level decision support tools based on a comprehensive, 10-state, continuous-time Markov chain model to unveil complex CVD progression patterns from real-world patient data and support clinical decision-making. This paper aims to address this crucial limitation. Our stochastic model–embedded artifact can help clinicians in efficient disease monitoring and intervention decisions, guided by objective data-driven insights from real patient data. Furthermore, the proposed model can unveil progression patterns of any chronic disease of interest by inputting only 3 data elements: a synthetic patient identifier, episode name, and episode time in days from a baseline date.}
}
@article{MOHANDES2022105730,
title = {Causal analysis of accidents on construction sites: A hybrid fuzzy Delphi and DEMATEL approach},
journal = {Safety Science},
volume = {151},
pages = {105730},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105730},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000704},
author = {Saeed Reza Mohandes and Haleh Sadeghi and Abdulwahed Fazeli and Amir Mahdiyar and M. Reza Hosseini and Mehrdad Arashpour and Tarek Zayed},
keywords = {Accident causation, Injury, Hazard, Fuzzy sets, Multi-criteria decision-making methods (MCDM), Data analysis},
abstract = {Though some studies have explored the causes of accidents on construction sites, the interdependency among the underlying causes remains elusive. This undermines the efficacy of any decision made by safety experts in reducing accidents’ impact. To fill this gap, a hybrid fuzzy-based framework is developed in this study to comprehensively identify and prioritize critical causes, as well as map interrelationships among these causes. The proposed framework is based on the infusion of the Pentagonal Fuzzy Delphi Method (PFDM) and Fuzzy DEMATEL techniques. Findings show that six main causes and twenty-three corresponding sub-causes (out of forty-seven identified ones) are the major culprits for the occurrence of related accidents. Furthermore, it was revealed that “organizational” and “workplace and environmental” causes turn out to be the most influential causes, while inappropriate safety guidelines and policies, poor safety management system, poor safety culture, poor safety knowledge of management team, the financial instability of firms, and corruption were the predominant sub-causes affecting the related accidents’ impact. To validate findings, several interviews with senior experts are undertaken. The outcomes of this study are vital for the concerned safety decision-makers by highlighting the influential causes debilitating the safety and health of involved workers.}
}
@article{COSTANZO201681,
title = {Experimental analysis of data-driven control for a building heating system},
journal = {Sustainable Energy, Grids and Networks},
volume = {6},
pages = {81-90},
year = {2016},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352467716000138},
author = {G.T. Costanzo and S. Iacovella and F. Ruelens and T. Leurs and B.J. Claessens},
keywords = {Thermostatically controlled load, Batch reinforcement learning, Demand response, Data-driven modeling, Fitted Q-iteration},
abstract = {Driven by the opportunity to harvest the flexibility related to building climate control for demand response applications, this work presents a data-driven control approach building upon recent advancements in reinforcement learning. More specifically, model-assisted batch reinforcement learning is applied to the setting of building climate control subjected to dynamic pricing. The underlying sequential decision making problem is cast into a Markov decision problem, after which the control algorithm is detailed. In this work, fitted Q-iteration is used to construct a policy from a batch of experimental tuples. In those regions of the state space where the experimental sample density is low, virtual support tuples are added using an artificial neural network. Finally, the resulting policy is shaped using domain knowledge. The control approach has been evaluated quantitatively using a simulation and qualitatively in a living lab. From the quantitative analysis it has been found that the control approach converges in approximately 20 days to obtain a control policy with a performance within 90% of the mathematical optimum. The experimental analysis confirms that within 10 to 20 days sensible policies are obtained that can be used for different outside temperature regimes.}
}
@article{WILSON2021109370,
title = {A causal modelling approach to informing woodland caribou conservation policy from observational studies},
journal = {Biological Conservation},
volume = {264},
pages = {109370},
year = {2021},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2021.109370},
url = {https://www.sciencedirect.com/science/article/pii/S0006320721004225},
author = {Steven F. Wilson and Thomas D. Nudds and Andrew {de Vries}},
keywords = {Causal modelling, Habitat disturbance, Linear features, Predation, , Woodland caribou},
abstract = {As global conservation actions become more urgent, informed decision-making requires robust analyses of the costs and benefits of policy options, based on available evidence. Recovery planning for threatened or endangered species must assume a cause-and-effect relationship between proposed management interventions and population responses. However, a significant portion of current knowledge about threatened or endangered species is derived from observational studies because experiments that fully meet random and controlled design criteria are largely infeasible or unethical. Large-scale field experiments are becoming more common, yet the greater uncertainty generated by what remain fundamentally observational studies can lead researchers to weak inferences about causal mechanisms, creating debate and confusion among decision-makers, planners and stakeholders. This has been an acute problem facing conservationists and governments as they struggle with the successful recovery of species in decline. In other domains where experimental evidence is difficult to collect, causal modelling has been adopted to identify causal relationships from observational data, based on a set of strong assumptions and identification rules. In Canada, significant and ongoing efforts have had limited success in reversing the population decline of woodland caribou (Rangifer tarandus caribou). We examine the scientific framework for woodland caribou recovery efforts through the lens of causal modelling, highlighting feasible steps that could be taken to improve the rigour of causal inferences.}
}
@article{LI2024118166,
title = {Condition-based maintenance method for multi-component systems under discrete-state condition: Subsea production system as a case},
journal = {Ocean Engineering},
volume = {306},
pages = {118166},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118166},
url = {https://www.sciencedirect.com/science/article/pii/S002980182401504X},
author = {Rongkang Li and Baoping Cai and Yixin Zhao and Yiliu Liu and Yanping Zhang and Xiangdi Kong and Yonghong Liu},
keywords = {Condition-based maintenance, Multi-component system, Discrete state, Scheme optimization},
abstract = {Components serve as the fundamental units of complex systems when implementing maintenance activities. Predicting the remaining useful life of components, evaluating their condition, and selecting maintenance measures are important factors that affect the accuracy and rationality of maintenance plans. The utilization of simplistic maintenance measures that aim for achieving a perfect state and the reliance on expert experience to obtain failure thresholds have limited the development of condition-based maintenance methods. To address these limitations, a condition-based maintenance method for multi-component systems under discrete-state conditions is proposed. A maintenance list is established based on the historical maintenance data of the system. Additionally, a Markov chain is employed to construct a single-component state transition model, which calculates the component state transition matrix based on historical operational data, and obtains the component state recovery matrix based on historical maintenance data. A single-step predictive model for components is established to study the probability of state transitions for components under different states and at different times. The maintenance optimization model is then applied to determine the maintenance plan of the system. The feasibility of the proposed method is demonstrated through an example involving critical equipment in a subsea production system for offshore oil.}
}
@article{MIDDELHUIS2025102492,
title = {Learning policies for resource allocation in business processes},
journal = {Information Systems},
volume = {128},
pages = {102492},
year = {2025},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102492},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924001509},
author = {Jeroen Middelhuis and Riccardo Lo Bianco and Eliran Sherzer and Zaharah Bukhsh and Ivo Adan and Remco Dijkman},
keywords = {Resource allocation, Business process optimization, Deep reinforcement learning, Bayesian optimization},
abstract = {Efficient allocation of resources to activities is pivotal in executing business processes but remains challenging. While resource allocation methodologies are well-established in domains like manufacturing, their application within business process management remains limited. Existing methods often do not scale well to large processes with numerous activities or optimize across multiple cases. This paper aims to address this gap by proposing two learning-based methods for resource allocation in business processes to minimize the average cycle time of cases. The first method leverages Deep Reinforcement Learning (DRL) to learn policies by allocating resources to activities. The second method is a score-based value function approximation approach, which learns the weights of a set of curated features to prioritize resource assignments. We evaluated the proposed approaches on six distinct business processes with archetypal process flows, referred to as scenarios, and three realistically sized business processes, referred to as composite business processes, which are a combination of the scenarios. We benchmarked our methods against traditional heuristics and existing resource allocation methods. The results show that our methods learn adaptive resource allocation policies that outperform or are competitive with the benchmarks in five out of six scenarios. The DRL approach outperforms all benchmarks in all three composite business processes and finds a policy that is, on average, 12.7% better than the best-performing benchmark.}
}
@article{MCCLOSKEY2011190,
title = {Using Bayesian belief networks to identify potential compatibilities and conflicts between development and landscape conservation},
journal = {Landscape and Urban Planning},
volume = {101},
number = {2},
pages = {190-203},
year = {2011},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2011.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169204611000697},
author = {Jon T. McCloskey and Robert J. Lilieholm and Christopher Cronan},
keywords = {Bayesian networks, Conservation, Development, GIS, Land use, Smart Growth},
abstract = {Experts with different land use interests often use differing definitions of land suitability that can result in competing land use decisions. We use Bayesian belief networks linked to GIS data layers to integrate empirical data and expert knowledge from two different land use interests (development and conservation) in Maine's Lower Penobscot River Watershed. Using ground locations and digital orthoquads, we determined the overall accuracy of the resulting development and conservation suitability maps to be 82% and 89%, respectively. Overlay of the two maps show large areas of land suitable for both conservation protection and economic development and provide multiple options for mitigating potential conflict among these competing land users. The modeling process can be adapted to help prioritize and choose among different alternatives as new information becomes available, or as land use and land-use policies change. The current model structure provides a maximal coverage strategy that allows decision makers to target and prioritize several areas for protection or development and to set specific strategies in the face of changing ecological, social, or economic processes. Having multiple options can generate new hypotheses and decisions at more local scales or for more specific conservation purposes not yet identified by stakeholders and decision makers in the region. Subsequently, new models can be developed using the same process, but with higher resolution data, thereby helping a community evaluate the impacts of alternative land uses between different prioritized areas at finer scales.}
}
@article{TAVAKOLAGHAEI2022580,
title = {A real-world application of Markov chain Monte Carlo method for Bayesian trajectory control of a robotic manipulator},
journal = {ISA Transactions},
volume = {125},
pages = {580-590},
year = {2022},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821003268},
author = {Vahid {Tavakol Aghaei} and Arda Ağababaoğlu and Sinan Yıldırım and Ahmet Onat},
keywords = {Reinforcement learning, Markov chain Monte Carlo, Bayesian learning, Intelligent control, Policy search},
abstract = {Reinforcement learning methods are being applied to control problems in robotics domain. These algorithms are well suited for dealing with the continuous large scale state spaces in robotics field. Even though policy search methods related to stochastic gradient optimization algorithms have become a successful candidate for coping with challenging robotics and control problems in recent years, they may become unstable when abrupt variations occur in gradient computations. Moreover, they may end up with a locally optimal solution. To avoid these disadvantages, a Markov chain Monte Carlo (MCMC) algorithm for policy learning under the RL configuration is proposed. The policy space is explored in a non-contiguous manner such that higher reward regions have a higher probability of being visited. The proposed algorithm is applied in a risk-sensitive setting where the reward structure is multiplicative. Our method has the advantages of being model-free and gradient-free, as well as being suitable for real-world implementation. The merits of the proposed algorithm are shown with experimental evaluations on a 2-Degree of Freedom robot arm. The experiments demonstrate that it can perform a thorough policy space search while maintaining adequate control performance and can learn a complex trajectory control task within a small finite number of iteration steps.}
}
@article{TSOLAKIS2023164,
title = {Scrutinising the interplay between governance and resilience in supply chain management: A systems thinking framework},
journal = {European Management Journal},
volume = {41},
number = {1},
pages = {164-180},
year = {2023},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0263237321001560},
author = {Naoum Tsolakis and Dimitris Zissis and Benny Tjahjono},
keywords = {Supply chain risk management, Governance, Resilience, Systems thinking, System dynamics},
abstract = {Supply chain disruptions recurrently challenge end-to-end operations owing to the ambiguous understanding of the role of governance in impacting supply network resilience. This paper scrutinises the relevant literature to understand the plethora of interpretations in supply chain governance and resilience while further providing a new perspective on the representation of the interplay between governance and resilience in supply chains. In this regard, the Systems Thinking lens is adopted to pull together the typologies and constructs of supply chain governance and resilience from the literature. Methodologically, System Dynamics modelling principles are leveraged to capture the underpinning structural interdependencies in a causal loop diagram. The study reveals that endogenous and exogenous supply chain governance processes and mechanisms support the intrinsic and extrinsic resilience in networks. Overall, this research contributes to the supply chain risk management domain by synthesising the interplay between governance and resilience, identifying pertinent typologies, and articulating research propositions that can inform decision-making at policy and management levels.}
}
@article{WANG20131610,
title = {Markov Random Field modeling, inference & learning in computer vision & image understanding: A survey},
journal = {Computer Vision and Image Understanding},
volume = {117},
number = {11},
pages = {1610-1627},
year = {2013},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1077314213001343},
author = {Chaohui Wang and Nikos Komodakis and Nikos Paragios},
keywords = {Markov Random Fields, Graphical models, MRFs, MAP inference, Discrete optimization, MRF learning},
abstract = {In this paper, we present a comprehensive survey of Markov Random Fields (MRFs) in computer vision and image understanding, with respect to the modeling, the inference and the learning. While MRFs were introduced into the computer vision field about two decades ago, they started to become a ubiquitous tool for solving visual perception problems around the turn of the millennium following the emergence of efficient inference methods. During the past decade, a variety of MRF models as well as inference and learning methods have been developed for addressing numerous low, mid and high-level vision problems. While most of the literature concerns pairwise MRFs, in recent years we have also witnessed significant progress in higher-order MRFs, which substantially enhances the expressiveness of graph-based models and expands the domain of solvable problems. This survey provides a compact and informative summary of the major literature in this research topic.}
}
@article{BOCK20181,
title = {Reliability and test effort analysis of multi-sensor driver assistance systems},
journal = {Journal of Systems Architecture},
volume = {85-86},
pages = {1-13},
year = {2018},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1383762117304289},
author = {Florian Bock and Sebastian Siegl and Peter Bazan and Peter Buchholz and Reinhard German},
keywords = {Discrete-time Markov chain, Reliability analysis, Test effort estimation, Sensor-based, Driver assistance system, Multi-sensor},
abstract = {Modern driver assistance systems for self-driving cars often rely on data collected by different sensors to determine the necessary system decisions. To prevent system failures, different validation techniques are used. The development is often split between car manufacturers and suppliers, whereby the requested test effort is one main project acceptance criterion. Already available effort estimation methods are not applicable, because they rely on implementation details that do not exist at early phases or on project experiences or individual expert expectations, which are not reliable enough to be employed as trustworthy source. Therefore, we provide in this paper an analytic approach for the computation of the error probability of a multi-sensor system. Based on this, we can give estimations for the test effort such that with statistical confidence no errors of the sensor system can be expected during the tests. The approach is able to take both the dependencies between successive sensor errors and the correlation between different sensors into account, mainly by using discrete time Markov chains. The provided approach therefore allows to design multi-sensor systems such that a specified overall error probability can be met and to give an estimation for the upper bound of the test effort.}
}
@article{JALBERT2023101276,
title = {Influence in the right-of-way: Assessing landowners’ risk decision-making in negotiating oil and gas pipeline easements},
journal = {The Extractive Industries and Society},
volume = {14},
pages = {101276},
year = {2023},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2023.101276},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X23000667},
author = {Kirk Jalbert and Katherine L. Dickinson and Jennifer Baka and Natalie Florence},
keywords = {Pipelines, Landowners, Risk perception, Public participation, Oil and gas infrastructure},
abstract = {Pipeline rights-of-way (ROW) are the result of complicated easement negotiations between pipeline operators and landowners, where landowners must frequently weigh the financial benefits of lucrative contracts against the potential risk of living alongside oil and gas infrastructure projects. This study analyzes a survey of landowners both directly and proximally involved in a pipeline construction project to better understand their risk decision-making processes. We examine how landowners in a ROW understand risks across personal, property, and environment related risk domains, then compare these results to neighboring landowners. We additionally assess how landowners’ relationships to pipelines have changed due to direct positive and negative experiences with these projects. Findings suggest that landowners’ assembled knowledge of pipelines and resulting risk calculations are, in part, determined by personal exposure to pipeline development and negotiations with pipeline operators. We also find evidence of risk discounting in instances where landowners benefit from the pipeline project, or seek to justify their historical risk calculations. Results of the study reveal why landowners assume risks of proximity to pipelines. These findings can inform the practices of community interest groups engaging with landowners, as well as policymakers pursuing rulemaking to protect landowner interests.}
}
@article{VERSTEGEN2014121,
title = {Identifying a land use change cellular automaton by Bayesian data assimilation},
journal = {Environmental Modelling & Software},
volume = {53},
pages = {121-136},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213002909},
author = {Judith A. Verstegen and Derek Karssenberg and Floor {van der Hilst} and André P.C. Faaij},
keywords = {Data assimilation, Cellular automata, Calibration, Model structure, Land use change, Particle filter},
abstract = {We present a Bayesian method that simultaneously identifies the model structure and calibrates the parameters of a cellular automaton (CA). The method entails sequential assimilation of observations, using a particle filter. It employs prior knowledge of experts to define which processes might be important in the system, and uses empirical information from observations to identify which ones really are and how these processes should be parameterized. In a case study for the São Paulo state in Brazil, we identify a land use change CA simulating sugarcane cropland expansion from 2003 to 2016. Eight annual observation maps of sugar cane cultivation are used, split over space and time for calibration and validation. It is shown that the identified CA can properly reproduce the observations, and has a minimum reduction factor of 3 in root mean square error compared to a Monte Carlo simulation without particle filter. In the part of the study area where no observational data are assimilated (validation area), there is little reduction in model performance compared to the part with observational data. So, incomplete datasets, regional land survey data, or clouded remote sensing images can still provide useful information for this particle filter method, which is an advantage because good quality land use maps are rare. Another advantage is that in our approach the output uncertainty encompasses errors from expert knowledge, model structure, parameters and observation (calibration) data. This can, in our opinion, be very useful for example to determine up to what future period the results are a secure basis for decisions and policy making.}
}
@article{MANTYKAPRINGLE2017125,
title = {Bridging science and traditional knowledge to assess cumulative impacts of stressors on ecosystem health},
journal = {Environment International},
volume = {102},
pages = {125-137},
year = {2017},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0160412016303385},
author = {Chrystal S. Mantyka-Pringle and Timothy D. Jardine and Lori Bradford and Lalita Bharadwaj and Andrew P. Kythreotis and Jennifer Fresque-Baxter and Erin Kelly and Gila Somers and Lorne E. Doig and Paul D. Jones and Karl-Erich Lindenschmidt},
keywords = {Traditional knowledge, Integration, Multiple stressors, Bayesian belief network, Adaptive co-management, Social-ecological systems},
abstract = {Cumulative environmental impacts driven by anthropogenic stressors lead to disproportionate effects on indigenous communities that are reliant on land and water resources. Understanding and counteracting these effects requires knowledge from multiple sources. Yet the combined use of Traditional Knowledge (TK) and Scientific Knowledge (SK) has both technical and philosophical hurdles to overcome, and suffers from inherently imbalanced power dynamics that can disfavour the very communities it intends to benefit. In this article, we present a ‘two-eyed seeing’ approach for co-producing and blending knowledge about ecosystem health by using an adapted Bayesian Belief Network for the Slave River and Delta region in Canada's Northwest Territories. We highlight how bridging TK and SK with a combination of field data, interview transcripts, existing models, and expert judgement can address key questions about ecosystem health when considerable uncertainty exists. SK indicators (e.g., bird counts, mercury in fish, water depth) were graded as moderate, whereas TK indicators (e.g., bird usage, fish aesthetics, changes to water flow) were graded as being poor in comparison to the past. SK indicators were predominantly spatial (i.e., comparing to other locations) while the TK indicators were predominantly temporal (i.e., comparing across time). After being populated by 16 experts (local harvesters, Elders, governmental representatives, and scientists) using both TK and SK, the model output reported low probabilities that the social-ecological system is healthy as it used to be. We argue that it is novel and important to bridge TK and SK to address the challenges of environmental change such as the cumulative impacts of multiple stressors on ecosystems and the services they provide. This study presents a critical social-ecological tool for widening the evidence-base to a more holistic understanding of the system dynamics of multiple environmental stressors in ecosystems and for developing more effective knowledge-inclusive partnerships between indigenous communities, researchers and policy decision-makers. This represents new transformational empirical insights into how wider knowledge discourses can contribute to more effective adaptive co-management governance practices and solutions for the resilience and sustainability of ecosystems in Northern Canada and other parts of the world with strong indigenous land tenure.}
}
@article{ZHANG2008698,
title = {Predicting the Frequency of Water Quality Standard Violations Using Bayesian Calibration of Eutrophication Models},
journal = {Journal of Great Lakes Research},
volume = {34},
number = {4},
pages = {698-720},
year = {2008},
issn = {0380-1330},
doi = {https://doi.org/10.1016/S0380-1330(08)71612-5},
url = {https://www.sciencedirect.com/science/article/pii/S0380133008716125},
author = {Weitao Zhang and George B. Arhonditsis},
keywords = {Environmental management, process-based models, eutrophication, Bayesian calibration, uncertainty analysis, Markov Chain Monte Carlo, water quality standards},
abstract = {The water quality standard setting process usually relies on mathematical models with strong mechanistic basis, as this provides assurance that the model will more realistically project the effects of alternative management schemes. From an operational standpoint, the interpretation of model results should be coupled with rigorous error analysis and explicit consideration of the predictive uncertainty and natural variability. In this study, our main objective is to attain effective model calibration and rigorous uncertainty assessment by integrating environmental mathematical modeling with Bayesian analysis. We use a complex aquatic biogeochemical model that simulates multiple elemental cycles (org. C, N, P, Si, O), multiple functional phytoplankton (diatoms, green algae and cyanobacteria) and zooplankton (copepods and cladocerans) groups. The Bayesian calibration framework is illustrated using three synthetic datasets that represent oligo-, meso- and eutrophic lake conditions. Scientific knowledge, expert judgment, and observational data were used to formulate prior probability distributions and characterize the uncertainty pertaining to a subset of the model parameters, i.e., a vector comprising the 35 most influential parameters based on an earlier sensitivity analysis of the model. Our study also underscores the lack of perfect simulators of natural system dynamics using a statistical formulation that explicitly accounts for the discrepancy between mathematical models and environmental systems. The model reproduces the key epilimnetic temporal patterns and provides realistic estimates of predictive uncertainty for water quality variables of environmental management interest. Our analysis also demonstrates how the Bayesian parameter estimation can be used for assessing the exceedance frequency and confidence of compliance of different water quality criteria. The proposed methodological framework can be very useful in the policy-making process and can facilitate environmental management decisions in the Laurentian Great Lakes region.}
}
@article{MODARES2024100506,
title = {An integrated Cognitive Reliability and Error Analysis Method (CREAM) and optimization for enhancing human reliability in blockchain},
journal = {Decision Analytics Journal},
volume = {12},
pages = {100506},
year = {2024},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2024.100506},
url = {https://www.sciencedirect.com/science/article/pii/S2772662224001103},
author = {Azam Modares and Vahideh {Bafandegan Emroozi} and Hadi Gholinezhad and Azade Modares},
keywords = {Blockchain technology, Bayesian best worst method, Human reliability, Cognitive Reliability and Error Analysis Method (CREAM), Common performance condition, Smart contract coding},
abstract = {Minor errors in smart contract coding on the blockchain can lead to significant and irreversible economic losses for transaction parties. Therefore, mitigating the risk posed by coding errors is crucial, necessitating the development of approaches to enhance human reliability in coding. The Cognitive Reliability and Error Analysis Method (CREAM) is one such approach, examining how environmental conditions affect the human error probability (HEP). Within CREAM, Common Performance Conditions (CPCs) influence error probability. This study ranks CPCs in smart contract coding based on their importance in coding reliability using the Bayesian Best Worst Method (BWM). Two methods are developed based on basic CREAM. In the first method, experts specify the control mode based on their opinions, and the probability of experts’ coding errors is determined according to the control level. In the second method, an optimization problem is formulated to select the most suitable programs, enhancing experts’ coding reliability. The proposed model considers energy, cost, and organizational budget factors to identify the optimal smart contract while minimizing the risks and costs associated with human errors. A case study in the electronics supply chain validates the applicability and efficacy of the proposed methods. Results from the first method indicate an opportunistic control mode. In contrast, the proposed model shows that improving CPC levels has a more significant effect, shifting the control mode towards a tactical control and reducing HEP to 0.00249.}
}
@article{OATLEY2003569,
title = {Crimes analysis software: ‘pins in maps’, clustering and Bayes net prediction},
journal = {Expert Systems with Applications},
volume = {25},
number = {4},
pages = {569-588},
year = {2003},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00097-6},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403000976},
author = {Giles C Oatley and Brian W Ewart},
keywords = {Bayesian burglary decision support prediction},
abstract = {The OVER Project was a collaboration between West Midlands Police, UK, the Centre for Adaptive Systems, and Psychology Division, from the University of Sunderland. The Project was developed primarily to assist the Police with the high volume crime, burglary from dwelling houses. A developed software system enables the trending of historical data, the testing of ‘short term’ hunches, and the development of ‘medium’ and long term’ strategies to burglary and crime reduction, based upon victim, offender, location and details of victimisations. The software utilises mapping and visualisation tools and is capable of a range of sophisticated predictions, tying together statistical techniques with theories from forensic psychology and criminology. The statistical methods employed (including multi-dimensional scaling, binary logistic regression) and ‘data-mining’ technologies (including neural networks) are used to investigate the impact of the types of evidence available and to determine the causality in this domain. The final predictions on the likelihood of burglary are calculated by combining all of the varying sources of evidence into a Bayesian belief network. This network is embedded in the developed software system, which also performs data cleansing and data transformation for presentation to the developed algorithms. It is important that derived statistics from the software and predictions are interpretable by the intended users of the decision support system, namely Police sector managers, and this paper includes some of the design decisions based upon the forensic psychology and criminology literature, including the graphical representation of geographic data and presentation of results of analyses.}
}
@article{DONOVAN2012168,
title = {The use of belief-based probabilistic methods in volcanology: Scientists' views and implications for risk assessments},
journal = {Journal of Volcanology and Geothermal Research},
volume = {247-248},
pages = {168-180},
year = {2012},
issn = {0377-0273},
doi = {https://doi.org/10.1016/j.jvolgeores.2012.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0377027312002594},
author = {Amy Donovan and Clive Oppenheimer and Michael Bravo},
keywords = {Volcanic risk assessment, Montserrat, Expert elicitation, Bayesian methods, Social studies of science, Soufriere Hills Volcano, Scientific advice, Decision-making during eruptions},
abstract = {This paper constitutes a philosophical and social scientific study of expert elicitation in the assessment and management of volcanic risk on Montserrat during the 1995–present volcanic activity. It outlines the broader context of subjective probabilistic methods and then uses a mixed-method approach to analyse the use of these methods in volcanic crises. Data from a global survey of volcanologists regarding the use of statistical methods in hazard assessment are presented. Detailed qualitative data from Montserrat are then discussed, particularly concerning the expert elicitation procedure that was pioneered during the eruptions. These data are analysed and conclusions about the use of these methods in volcanology are drawn. The paper finds that while many volcanologists are open to the use of these methods, there are still some concerns, which are similar to the concerns encountered in the literature on probabilistic and determinist approaches to seismic hazard analysis.}
}
@article{PRISTROM2016196,
title = {A novel flexible model for piracy and robbery assessment of merchant ship operations},
journal = {Reliability Engineering & System Safety},
volume = {155},
pages = {196-211},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S095183201630206X},
author = {Sascha Pristrom and Zaili Yang and Jin Wang and Xinping Yan},
keywords = {Maritime security, Maritime piracy, Hijacking, Best management practice},
abstract = {Maritime piracy and robbery can not only cause logistics chain disruption leading to economic consequences but also result in loss of lives, and short- and long-term health problems of seafarers and passengers. There is a justified need for further investigation in this area of paramount importance. This study analyses maritime piracy and robbery related incidents in terms of the major influencing factors such as ship characteristics and geographical locations. An analytical model incorporating Bayesian reasoning is proposed to estimate the likelihood of a ship being hijacked in the Western Indian or Eastern African region. The proposed model takes into account the characteristics of the ship, environment conditions and the maritime security measures in place in an integrated manner. Available data collected from the Global Integrated Shipping Information System (GISIS) together with expert judgement is used to develop and demonstrate the proposed model. This model can be used by maritime stakeholders to make cost-effective anti-piracy decisions in their operations under uncertainties. Discussions are given on industrial response to maritime piracy in order to minimize the risk to ships exposed to attacks from pirates. Further recommendations on how maritime security and piracy may be best addressed in terms of maritime security measures are outlined.}
}
@article{GHABAYEN2004191,
title = {Characterization of uncertainties in the operation and economics of the proposed seawater desalination plant in the Gaza Strip},
journal = {Desalination},
volume = {161},
number = {2},
pages = {191-201},
year = {2004},
issn = {0011-9164},
doi = {https://doi.org/10.1016/S0011-9164(04)90054-9},
url = {https://www.sciencedirect.com/science/article/pii/S0011916404900549},
author = {Said Ghabayen and Mac McKee and Mariush Kemblowski},
keywords = {Gaza Strip, Reverse osmosis, Desalination, Uncertainty, Bayesian belief networks, Optimization},
abstract = {In the Gaza Strip, the available freshwater sources are severely polluted and overused. Desalination of seawater through reverse osmosis (RO) has become the most realistic option to meet a rapidly growing water demand. It is estimated that the Gaza Strip will need to develop a seawater desalination capacity of about 120,000 m3/d by the year 2008, and an additional 30,000 m3/d by the year 2016 in order to maintain a fresh water balance in the coastal aquifer and to fulfill the water demand for different uses in a sustainable manner. Cost and reliability of a large RO facility are still subject to much uncertainty. The cost of seawater desalination by RO systems varies with facility size and lifetime, financing conditions, intake type and pre-treatment requirements, power requirements, recovery rate, chemicals cost, spare parts cost, and membrane replacement cost. The permeate salinity is a function of feed water temperature, recovery rate, and permeate flux. The quantity of water produced depends mainly on plant size, recovery rate, and operating load factor. Many of these parameters are subject to a great deal of uncertainty. The objective of this work is to develop a probabilistic model for the simulation of seawater reverse osmosis processes using a Bayesian belief network (BBN) approach. This model represents a new application of probabilistic modeling tools to a large-scale complex system. The model is used to: (1) characterize the different uncertainties involved in the RO process; (2) optimize the RO process reliability and cost; and (3) study how uncertainty in unit capital cost, unit operation and maintenance (O&M) cost, and permeate quality is related to different input variables. The model utilizes information from journal articles, books, expert opinions, and technical reports related to the study area, and can be used to support operators and decision makers in the design of RO systems and formulation of operational policies. The structure of the model is not specific to the Gaza Strip and can be easily populated with data from any large-scale RO plant in any part of the world.}
}
@article{GHOSH201764,
title = {Application of Cellular automata and Markov-chain model in geospatial environmental modeling- A review},
journal = {Remote Sensing Applications: Society and Environment},
volume = {5},
pages = {64-77},
year = {2017},
issn = {2352-9385},
doi = {https://doi.org/10.1016/j.rsase.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352938516300258},
author = {Pramit Ghosh and Anirban Mukhopadhyay and Abhra Chanda and Parimal Mondal and Anirban Akhand and Sandip Mukherjee and S.K. Nayak and Subhajit Ghosh and Debasish Mitra and Tuhin Ghosh and Sugata Hazra},
keywords = {Cellular Automata, Markov-Chain model, Remote sensing and GIS, Environmental modeling},
abstract = {Cellular Automata (CA) & Markov-Chain modeling are concepts that are utilized in numerous branches of science. Powerful as they are independently, these two theoretical concepts can be of immense use when fused together and applied in practical situations. CA and Markov models have spread their wraths over geosciences and with the advancement of remote sensing and GIS technologies along with an exponential increase in computing and modeling power. Over the last few years, these concepts have found a solid ground for research in this domain of geospatial environmental modeling in earth sciences. It is widely used to characterize the dynamics of land use/cover, forest cover, urban sprawl, wetland landscape, plant growth and modeling of watershed management, suitable site selection, coastal zone management and so forth. This paper aims to categorize these researches into broad categories. This paper discusses the concepts of CA-Markov modeling and their backgrounds and is followed by a classification of the researches conducted in this domain into two broad groups, one being the development of concepts and the adopted methodologies, while the other discusses the application of these methods in solving and studying real world scenarios. Recent developments in this domain have been observed which uses concepts and technologies previously unused in conjunction to CA. However, several limitations like non-accountability of human influences, unavailability of high resolution imagery, primary discrepancy between the simulations of CA with GIS, human decision making were also addressed in details. At the same time numerous advancements like inclusion of fuzzy logic, possibility of textural classification, removing biases in simulation and developing a CA-based 2D and 3D land use simulation module are elaborated which are at present showing some promising avenues wherein significant research can be done in future.}
}
@article{PADULA20221063,
title = {Machine Learning Methods in Health Economics and Outcomes Research—The PALISADE Checklist: A Good Practices Report of an ISPOR Task Force},
journal = {Value in Health},
volume = {25},
number = {7},
pages = {1063-1080},
year = {2022},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2022.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S1098301522001917},
author = {William V. Padula and Noemi Kreif and David J. Vanness and Blythe Adamson and Juan-David Rueda and Federico Felizzi and Pall Jonsson and Maarten J. IJzerman and Atul Butte and William Crown},
keywords = {artificial intelligence, machine learning},
abstract = {Advances in machine learning (ML) and artificial intelligence offer tremendous potential benefits to patients. Predictive analytics using ML are already widely used in healthcare operations and care delivery, but how can ML be used for health economics and outcomes research (HEOR)? To answer this question, ISPOR established an emerging good practices task force for the application of ML in HEOR. The task force identified 5 methodological areas where ML could enhance HEOR: (1) cohort selection, identifying samples with greater specificity with respect to inclusion criteria; (2) identification of independent predictors and covariates of health outcomes; (3) predictive analytics of health outcomes, including those that are high cost or life threatening; (4) causal inference through methods, such as targeted maximum likelihood estimation or double-debiased estimation—helping to produce reliable evidence more quickly; and (5) application of ML to the development of economic models to reduce structural, parameter, and sampling uncertainty in cost-effectiveness analysis. Overall, ML facilitates HEOR through the meaningful and efficient analysis of big data. Nevertheless, a lack of transparency on how ML methods deliver solutions to feature selection and predictive analytics, especially in unsupervised circumstances, increases risk to providers and other decision makers in using ML results. To examine whether ML offers a useful and transparent solution to healthcare analytics, the task force developed the PALISADE Checklist. It is a guide for balancing the many potential applications of ML with the need for transparency in methods development and findings.}
}
@article{LUCAS2000251,
title = {A probabilistic and decision-theoretic approach to the management of infectious disease at the ICU},
journal = {Artificial Intelligence in Medicine},
volume = {19},
number = {3},
pages = {251-279},
year = {2000},
note = {Knowledge-based Information Management in Intensive Care and Anaesthesia},
issn = {0933-3657},
doi = {https://doi.org/10.1016/S0933-3657(00)00048-8},
url = {https://www.sciencedirect.com/science/article/pii/S0933365700000488},
author = {Peter J.F. Lucas and Nicolette C. {de Bruijn} and Karin Schurink and Andy Hoepelman},
keywords = {Medical decision support, Probabilistic networks, Bayesian networks, Decision theory, Temporal probabilistic models, Infectious diseases, Intensive care},
abstract = {The medical community is presently in a state of transition from a situation dominated by the paper medical record to a future situation where all patient data will be available on-line by an electronic clinical information system. In data-intensive clinical environments, such as intensive care units (ICUs), clinical patient data are already fully managed by such systems in a number of hospitals. However, providing facilities for storing and retrieving patient data to clinicians is not enough; clinical information systems should also offer facilities to assist clinicians in dealing with hard clinical problems. Extending an information system’s capabilities by integrating it with a decision-support system may be a solution. In this paper, we describe the development of a probabilistic and decision-theoretic system that aims to assist clinicians in diagnosing and treating patients with pneumonia in the intensive-care unit. Its underlying probabilistic-network model includes temporal knowledge to diagnose pneumonia on the basis of the likelihood of laryngotracheobronchial-tree colonisation by pathogens, and symptoms and signs actually present in the patient. Optimal antimicrobial therapy is selected by balancing the expected efficacy of treatment, which is related to the likelihood of particular pathogens causing the infection, against the spectrum of antimicrobial treatment. The models were built on the basis of expert knowledge. The patient data that were available were of limited value in the initial construction of the models because of problems of incompleteness. In particular, detailed temporal information was missing. By means of a number of different techniques, among others from the theory of linear programming, these data have been used to check the probabilistic information elicited from infectious-disease experts. The results of an evaluation of a number of slightly different models using retrospective patient data are discussed as well.}
}
@article{TADIC2005152,
title = {TWO TIME-SCALE FEASIBLE DIRECTION METHOD},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {152-157},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.00378},
url = {https://www.sciencedirect.com/science/article/pii/S147466701636390X},
author = {Vladislav B. Tadić and Nenad Vlajković and Efstratios C. Kyriakopoulos},
keywords = {Optimization, stochastic approximation, Markov decision problems, Monte Carlo simulation},
abstract = {Stochastic constrained optimization problems with non-convex objective and convex feasible domain are considered for the case where the objective and constraint functions are available only throuth noisy observations. A general algorithm of the two time-scale stochastic approximation type is proposed for these problems. The proposed algorithm is applied to Markov decision problems with average cost, average constraints and parameterized randomized policy. The asymptotic behavior of the proposed algorithm is analyzed for the case where the algorithm step-sizes are constant and the noise in the observations of the objective and constraint functions depends on the algorithm iterates.}
}
@article{LAMBRAKI2021,
title = {Building Social-Ecological System Resilience to Tackle Antimicrobial Resistance Across the One Health Spectrum: Protocol for a Mixed Methods Study},
journal = {JMIR Research Protocols},
volume = {10},
number = {6},
year = {2021},
issn = {1929-0748},
doi = {https://doi.org/10.2196/24378},
url = {https://www.sciencedirect.com/science/article/pii/S1929074821003887},
author = {Irene Anna Lambraki and Shannon Elizabeth Majowicz and Elizabeth Jane Parmley and Didier Wernli and Anaïs Léger and Tiscar Graells and Melanie Cousins and Stephan Harbarth and Carolee Carson and Patrik Henriksson and Max Troell and Peter Søgaard Jørgensen},
keywords = {antimicrobial resistance, One Health, resilience, transdisciplinary, participatory, interventions, systems dynamics, social-ecological system},
abstract = {Background
Antimicrobial resistance (AMR) is an escalating global crisis with serious health, social, and economic consequences. Building social-ecological system resilience to reduce AMR and mitigate its impacts is critical.
Objective
The aim of this study is to compare and assess interventions that address AMR across the One Health spectrum and determine what actions will help to build social and ecological capacity and readiness to sustainably tackle AMR.
Methods
We will apply social-ecological resilience theory to AMR in an explicit One Health context using mixed methods and identify interventions that address AMR and its key pressure antimicrobial use (AMU) identified in the scientific literature and in the gray literature using a web-based survey. Intervention impacts and the factors that challenge or contribute to the success of interventions will be determined, triangulated against expert opinions in participatory workshops and complemented using quantitative time series analyses. We will then identify indicators using regression modeling, which can predict national and regional AMU or AMR dynamics across animal and human health. Together, these analyses will help to quantify the causal loop diagrams (CLDs) of AMR in the European and Southeast Asian food system contexts that are developed by diverse stakeholders in participatory workshops. Then, using these CLDs, the long-term impacts of selected interventions on AMR will be explored under alternate future scenarios via simulation modeling and participatory workshops. A publicly available learning platform housing information about interventions on AMR from a One Health perspective will be developed to help decision makers identify promising interventions for application in their jurisdictions.
Results
To date, 669 interventions have been identified in the scientific literature, 891 participants received a survey invitation, and 4 expert feedback and 4 model-building workshops have been conducted. Time series analysis, regression modeling of national and regional indicators of AMR dynamics, and scenario modeling activities are anticipated to be completed by spring 2022. Ethical approval has been obtained from the University of Waterloo’s Office of Research Ethics (ethics numbers 40519 and 41781).
Conclusions
This paper provides an example of how to study complex problems such as AMR, which require the integration of knowledge across sectors and disciplines to find sustainable solutions. We anticipate that our study will contribute to a better understanding of what actions to take and in what contexts to ensure long-term success in mitigating AMR and its impact and provide useful tools (eg, CLDs, simulation models, and public databases of compiled interventions) to guide management and policy decisions.
International Registered Report Identifier (IRRID)
DERR1-10.2196/24378}
}
@article{MOKTADIR2022131329,
title = {Antecedents for circular bioeconomy practices towards sustainability of supply chain},
journal = {Journal of Cleaner Production},
volume = {348},
pages = {131329},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.131329},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200957X},
author = {Md. Abdul Moktadir and Ashish Dwivedi and Towfique Rahman},
keywords = {Antecedents, Bioeconomy, Circular economy, Grey-DEMATEL, Sustainability, Waste management},
abstract = {The current emerging economy of the leather industry is primarily linear. Increasingly rigorous legislative specifications for environmental preservation in developed countries have created huge pressure for the leather industry. In light of international efforts to achieve the Sustainable Development Goals (SDGs), a circular bioeconomy approach has gained compelling traction in addressing waste management challenges in the leather industry. The concept of circular bioeconomy encourages recycling of wastes yielded from leather processing. However, as of now, there is still a lack of research on bioeconomy concepts in the leather industry context. Hence, the purpose of this study is to put forth the antecedents for circular bioeconomy practices (BEP) in the leather processing industry (LPI), along which a firm can enhance its sustainability. Seventeen antecedents of BEP have been identified from the extensive literature review and experts survey; and their effects on each other are assessed using a blended grey based decision-making trial and evaluation laboratory model. Four major antecedents such as sustainable infrastructure for leather industry, biodiversity, advancement in sustainable products, sustainability threshold levels for bioeconomy technologies are identified as the most crucial key antecedents for BEP in the context of the LPI. In the present study, six antecedents are identified as causal, whereas eleven antecedents are identified as effect group antecedents. The sustainable infrastructure for leather industry is identified as the most crucial driving antecedent in ensuring circular BEP. The findings of this study provide a structural decision support model to the industry managers, practitioners, and relevant stakeholders that assists in knowing the cause-and-effect group antecedents of BEP towards attaining sustainability in the supply chain.}
}
@article{NGUYEN2023103186,
title = {The toxic waste management towards corporates’ sustainable development: A causal approach in Vietnamese industry},
journal = {Environmental Technology & Innovation},
volume = {31},
pages = {103186},
year = {2023},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2023.103186},
url = {https://www.sciencedirect.com/science/article/pii/S2352186423001827},
author = {Tran Thai Ha Nguyen and Lien Thi Bich Bui and Kien Trung Tran and Dang Thanh Minh Tran and Khuong Vinh Nguyen and Ha Manh Bui},
keywords = {Toxic waste, Sustainable development, Environmental regulations, Environmental perceptions, Financial resources, FDEMATEL, Vietnamese industry},
abstract = {Effective toxic waste management is critical for sustainable development, as toxic waste has a severe impact on both ecosystems and human health. However, the relationship between toxic waste management and corporate effectiveness and its impact on various aspects of sustainable development has remained unclear due to a lack of previous studies examining the interrelationships of these attributes under uncertainty, particularly in emerging countries. This study addresses this gap by presenting a cause–effect model that utilizes qualitative information and linguistic preferences to explore perceptions of toxic waste management and its influence on Vietnamese industry practices. To ensure the reliability and validity of the selected attributes, the study employs the fuzzy Delphi method, which combines fuzzy set theory with crisp numbers to transform qualitative information and linguistic preferences into meaningful data. At the same time, this study applies the decision-making trial and evaluation method (DEMATEL) to address the interrelationships among the proposed attributes. From a set of 7 aspects and 33 criteria, which are identified from previous documents and experts’ preferences, the study finds that environmental protection regulations, environmental perceptions of corporations, and financial resources are the key drivers of enhancement in corporate prospects for sustainable development, corporate social responsibility, and the adoption of new technologies and policies for toxic waste management. The study also identifies five critical criteria, including comprehensive institutional vision, legislation enforcement, municipal operations, financial resources, technical capacity and competitive advantages, that can play crucial roles in enhancing corporate toxic waste management practices. Based on these significant findings, this study discusses both theoretical and practical implications.}
}
@article{CATENACCI201387,
title = {Integrated assessment of sea-level rise adaptation strategies using a Bayesian decision network approach},
journal = {Environmental Modelling & Software},
volume = {44},
pages = {87-100},
year = {2013},
note = {Thematic Issue on Innovative Approaches to Global Change Modelling},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212002708},
author = {Michela Catenacci and Carlo Giupponi},
keywords = {Bayesian decision network, Uncertainty, Global change, Sea-level rise, Adaptation},
abstract = {The exposure to sea-level rise (SLR) risks emerges as a challenging issue in the broader debate about the possible consequences of global environmental change for at least four reasons: the potentially serious impacts, the very high uncertainty regarding future projections of SLR and their effects on the environmental and socio-economic system, the multiple scales involved, and the need to take effective management decisions in terms of climate change adaptation. Unfortunately, mechanistic models generally demonstrated a limited ability to characterise in appropriate detail how complex coastal systems and their constituent parts may respond to climate change drivers and to possible adaptation initiatives. The research reported here develops an innovative methodological framework, which integrates different research areas – participatory and probabilistic modelling, and decision analysis – within a coordinated process aimed at decision support. The effectiveness of alternative adaptation measures in a lagoon in north-east Italy is assessed by means of Bayesian Decision Network (BDN) models, developed upon judgments elicited from selected experts. A concept map of the system was first developed in a group brainstorming context and was later evolved into BDN models, thus providing a simplified quantitative structure. Conditional probabilities, quantifying the causal links between the direct and indirect consequences of SLR on the area of study, are elicited from the experts. The proposed methodological framework allows the integrated assessment of factors and processes belonging to different domains of knowledge. Moreover, it activates an informed and transparent participatory process involving disciplinary experts and policy makers, where the main risk factors are considered together with the expected effects of the adaptation options, with effective treatment and communication of the uncertainty pervading the SLR issue. Finally, the framework shows potentials for being further developed and applied to consider new evidences and/or different adaptation strategies, and it results sufficiently flexible to be adopted and effectively reused in other similar case studies.}
}
@article{DANG2021112485,
title = {Integrated methods and scenarios for assessment of sand dunes ecosystem services},
journal = {Journal of Environmental Management},
volume = {289},
pages = {112485},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.112485},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721005478},
author = {Kinh Bac Dang and Thu Thuy Nguyen and Huu Hao Ngo and Benjamin Burkhard and Felix Müller and Van Bao Dang and Hieu Nguyen and Van Liem Ngo and Thi Phuong Nga Pham},
keywords = {Bayesian belief network, Structural equation modeling, Ecosystem service matrix, Scenarios analysis, Coastal dunes development},
abstract = {Anthropogenic and natural ecosystems in coastal dunes provide considerable benefits to human well-being. However, to date, we still lack a good understanding of how ecosystem services (ES) supply varies from young dunes (e.g., embryo and fore dunes) to mature dunes (e.g., brown and red dunes). This study proposed a novel modelling methodology by integrating an expert-based matrix, a Bayesian Belief Network (BBN), a structural equation model, and a scenario development method. It aims at evaluating dune ecosystem services for the sustainable development of coastal areas. The model was tested using data collected from dunes in Vietnam. An expert-based matrix to assess the supply capacity of 18 ES in different types of dunes was generated with the participation of 21 interdisciplinary scientists. It was found that red dune ecosystems could supply the most regulation and cultural ecosystem services, while gray dunes provided the least amount. Results from a scenario analysis recommended that decision-making is able to optimize multiple ES by: (i) keeping embryo/fore dunes in their natural state instead of using them for mineral mining and urbanization; (ii) enlarging certified and protected forests areas in gray and yellow dunes; and (iii) optimizing cultural ES supply in red dunes.}
}
@article{DING2024115241,
title = {Chaos synchronization of two coupled map lattice systems using safe reinforcement learning},
journal = {Chaos, Solitons & Fractals},
volume = {186},
pages = {115241},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.115241},
url = {https://www.sciencedirect.com/science/article/pii/S0960077924007938},
author = {Jianpeng Ding and Youming Lei and Jianfei Xie and Michael Small},
keywords = {Chaos synchronization, Safe reinforcement learning, Model-free method, Coupled map lattice},
abstract = {When synchronizing two continuous typical chaotic systems, the state variables of the response system are usually bounded and satisfy the Lipschitz condition. This permits a universal synchronization method. In the synchronization of two high-dimensional discrete chaotic systems such as the coupled map lattice systems, the state variables of the response system often diverge, which makes it difficult to seek a universal synchronization method. To overcome this difficulty, bounded and hard constraints, which correspond to the concept of safety level III in reinforcement learning terms, on the state variables must be imposed, when the discrete response system is perturbed. We propose a universal method for synchronizing two discrete systems based on safe reinforcement learning (RL). In this method, the RL agent’s policy is used to reach the goal of synchronization and a safety layer added directly on top of the policy is used to satisfy safety level III. The safety layer consists of a one-step predictor for the perturbed response system and an action correction formulation. The one-step predictor, based on a next generation reservoir computing, is used to identify whether the next state of the perturbed system is within the chaotic domain, and if not, the action correction formula is activated to modify the corresponding perturbing force component to zero. In this way, the state of the perturbed system will remain in the bounded chaotic domain. We demonstrate that the proposed method succeeds in synchronization without divergence through a numerical example with two coupled map lattice systems, and the average synchronization time is 12.66 iteration steps over 1000 different initial conditions. The method works even when parameter mismatches and disturbances exist in the system. We compare the performance in both cases with and without the safety layer and find that successful synchronization is only possible in the former case, emphasizing the significance of the safety layer. The performance of the algorithm with optimal hyper-parameters obtained by Bayesian optimization is robust and stable.}
}
@article{MURAD2024242,
title = {Uncertainty-aware autonomous sensing with deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {156},
pages = {242-253},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000918},
author = {Abdulmajid Murad and Frank Alexander Kraemer and Kerstin Bach and Gavin Taylor},
keywords = {Internet of Things, Deep reinforcement learning, Autonomous sensing, Uncertainty-aware prediction, Bayesian neural networks},
abstract = {Constructing an accurate representation model of phenomena with fewer measurements is a fundamental challenge in the Internet of Things. Leveraging sparse sensing policies to select the most informative measurements is a prominent technique for addressing resource constraints. However, designing such sensing policies requires significant domain knowledge and involves manually fine-tuned heuristics that are task-specific and often non-adaptive. In this work, we propose reducing manual-engineering efforts in designing sensing policies by using an automated approach based on deep reinforcement learning. Guided by an uncertainty-aware prediction model, the sensors learn sensing behaviors autonomously by optimizing an application goal formulated in the reward function based on the measured peaks-over-threshold. We apply the proposed approach in two use cases of monitoring air quality and indoor noise and show the adaptability and transferability of the learned policies. Compared to conventional periodic sensing methods, our results achieve, on average, an increased detection in periods of interest by 78.5% and 357.3% while reducing energy expenditure by 14.3% and 7.6% for air quality and noise monitoring, respectively. Additionally, the resulting representation models are more credible, as measured by various metrics of probabilistic modeling.}
}
@article{MANTYKAPRINGLE201680,
title = {Prioritizing management actions for the conservation of freshwater biodiversity under changing climate and land-cover},
journal = {Biological Conservation},
volume = {197},
pages = {80-89},
year = {2016},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2016.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0006320716300763},
author = {Chrystal S. Mantyka-Pringle and Tara G. Martin and David B. Moffatt and James Udy and Jon Olley and Nina Saxton and Fran Sheldon and Stuart E. Bunn and Jonathan R. Rhodes},
keywords = {Freshwater conservation planning, Management actions, Costs, Bayesian decision network, Climate change, Land-cover change},
abstract = {Freshwater ecosystems are declining under climate change and land-use change. To maximize the return on investment in freshwater conservation with limited financial resources, managers must prioritize management actions that are most cost-effective. However, little is known about what these priorities may be under the combined effects of climate and land-cover change. We present a novel decision-making framework for prioritizing conservation resources to different management actions for the conservation of freshwater biodiversity. The approach is novel in that it has the ability to model interactions, rank management options for dealing with conservation threats from climate and land-cover change, and integrate empirical data with expert knowledge. We illustrate the approach using a case study in South East Queensland (SEQ), Australia under climate change, land-cover change and their combined effects. Our results show that the explicit inclusion of multiple threats and costs results in quite different priorities than when costs and interactions are ignored. When costs are not considered, stream and riparian restoration, as a single management strategy, provides the greatest overall protection of macroinvertebrate and fish richness in rural and urban areas of SEQ in response to climate change and/or urban growth. Whereas, when costs are considered, farm/land management with stream and riparian restoration are the most cost-effective strategies for macroinvertebrate and fish conservation. Our findings support riparian restoration as the most effective adaptation strategy to climate change and urban development, but because it is expensive it may often not be the most cost-efficient strategy. Our approach allows for these decisions to be evaluated explicitly.}
}
@article{REITSMA201093,
title = {Geoscience explanations: Identifying what is needed for generating scientific narratives from data models},
journal = {Environmental Modelling & Software},
volume = {25},
number = {1},
pages = {93-99},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209001753},
author = {Femke Reitsma},
keywords = {Data models, Scientific narrative, Scientific explanation},
abstract = {As models of geoscience systems grow in number and size, so grows the need for tools to help express the output of those models in usable forms. In this paper the utility of the output of a model is defined as its ability to support scientific explanation. Commonly the output of a model might include statistics, graphs, maps, images and animations, which require expert interpretation and evaluation in the context of the model setup or implementation. Here the narrative is presented as a type of output from a model that can present the results of a model in a form that is more useful for the non-expert. Narratives provide a rich medium for expressing causal chains of events that form the basis for explanation and its future use in policy and decision making. This paper reviews research on narratives and their role in scientific explanation. The principles of narrative construction for the Geosciences are identified, which forms the basis for determining the key components needed in explanatory statements for communicating the output of geoscience models. The potential of existing data models used in model output for generating narratives are explored, followed by the conceptual presentation of an extended data model, which supports the narrative unit and has the potential to automate aspects of the generation of scientific explanation in narrative form.}
}
@article{ALAM2024100132,
title = {Development of landfill macro management policy: A case study},
journal = {Cleaner Waste Systems},
volume = {7},
pages = {100132},
year = {2024},
issn = {2772-9125},
doi = {https://doi.org/10.1016/j.clwas.2024.100132},
url = {https://www.sciencedirect.com/science/article/pii/S2772912524000046},
author = {Abu Md Tuhin-Ul Alam and Islam M. Rafizul},
keywords = {Policy, Landfill, Authority, Viability, Reasoning, Management},
abstract = {Development of landfill macro management policy is very important to sort out the MSW disposal drawbacks for any cities as a part of cleaner waste systems. This study emphasized on implementation of macro management policy of landfill for south asia zone through risk evaluation by using the approaches of Fuzzy Analytical Hierarchy Process (Fuzzy AHP), and Decision-Making Trial and Evaluation Laboratory (DEMATEL). For this purpose, Khulna city of Bangladesh was selected as case study area. To perform this study, drawbacks of MSW disposal of this city has taken into considerations from findings of the research gap, through focus group discussion (FGD) and expert’s opinions. Basing on the analysis, to achieve the desired goal four domains, twelve criterions and thirty factors were considered. Authority, reasoning, viability, and management were the four selected key domains. From the software analysis it is found that, among twelve selected criterions the most important criterions were legislative authority, negotiation with environmental act, value evaluation and risk management. Same way, among the thirty selected factors the most important influential factors were environmental assessment, approval of detail area plan, international environmental act, 40 year’s goal, and national risk found out through risk evaluation with respect to all domains. Study found out with the macro management policy of landfill is, authority> viability>management>reasoning. The developed policy is validated through casual relationship.}
}
@article{REVERBERI1999289,
title = {A probabilistic model for interactive decision-making},
journal = {Decision Support Systems},
volume = {25},
number = {4},
pages = {289-308},
year = {1999},
issn = {0167-9236},
doi = {https://doi.org/10.1016/S0167-9236(99)00013-5},
url = {https://www.sciencedirect.com/science/article/pii/S0167923699000135},
author = {Pierfrancesco Reverberi and Maurizio Talamo},
keywords = {Decision-making under uncertainty, Information-gathering strategy, Myopic policy, Interactive solution procedure, Bayesian belief networks},
abstract = {A probabilistic reasoning model is defined where the decision maker (d.m.) is engaged in a sequential information-gathering process facing the trade-off between the reliability of the achieved solution and the associated observation cost. The d.m. is directly involved in the proposed flexible control strategy, which is based on information-theoretic principles. The devised strategy works on a Bayesian belief network that allows the efficient representation and manipulation of the knowledge base relevant to the problem domain. It is shown that this strategy guarantees a constant factor approximate solution with respect to the optimum of the decision problem. Some application examples are also discussed.}
}
@article{MUSHTAQ2014188,
title = {Decision Making Framework with Stochastic Dynamicity: A Step Forward Towards SDN},
journal = {Procedia Computer Science},
volume = {34},
pages = {188-195},
year = {2014},
note = {The 9th International Conference on Future Networks and Communications (FNC'14)/The 11th International Conference on Mobile Systems and Pervasive Computing (MobiSPC'14)/Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.07.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914009430},
author = {Sajjad Ali Mushtaq and Naheed Sajjad and Sohail Razzaq},
keywords = {Policy-Based Network Management (PBNM), Quality of Service (QoS), Service Level Agreement (SLA), Multi Criteria Decision Making (MCDM) theory, Topology, Software Defined Networking (SDN), Call Dropping Probability, Throughput, Jitter},
abstract = {Entrepreneurs, infrastructure providers, service operators and carriers are targeting the service, application and infrastructure unification over an all Internet Protocol (IP) platform while providing access technology convergence. Distinctive business goals and varying service logic in addition to distinctive resource requirements alongside scalability and extensibility over such converged infrastructure has to be addressed. Intra/Inter domain knowledge set over distinctive planes (application, service, control and network/transport) has to be induced, inherited and overridden. A decision-making system supporting dynamicity is required to handle such a platform effectively and efficiently. Multi Criteria Decision Making (MCDM) is used for decision-making framework. Ontology is used to model the structural hierarchy and semantics capturing. Ontology's inability to capture uncertainty is complemented by Bayesian mapping. Software Defined Network Controller (SDNC) on the basis of the proposed framework is developed for multimedia call/request/session routing by using embedded hardware. System exhibits greater throughput with lesser call/session/request dropping probability at the expense of a susceptible delay. Moreover system latency with and without SDNC in provisioning and outsourcing modes are compared and evaluated.}
}
@article{COMES2011108,
title = {Decision maps: A framework for multi-criteria decision support under severe uncertainty},
journal = {Decision Support Systems},
volume = {52},
number = {1},
pages = {108-118},
year = {2011},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2011.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923611001163},
author = {T. Comes and M. Hiete and N. Wijngaards and F. Schultmann},
keywords = {Multi-Criteria Decision Analysis, Scenario-based decision support, Severe uncertainty, Causal Maps},
abstract = {In complex strategic decision-making situations the need for well-structured support arises. To evaluate decision alternatives, information about the situation and its development must be determined, managed and processed by the best available experts. For various types of information different reasoning principles have been developed: deterministic, probabilistic, fuzzy and techniques for reasoning under ignorance (i.e., the likelihood of an event cannot be quantified). We propose a new approach based on Decision Maps supporting decision makers under fundamental uncertainty by generating descriptions of different possible situation developments (scenarios) in a distributed manner. The scenarios are evaluated using Multi-Criteria Decision Analysis techniques.}
}
