Title,Authors,Year,Journal,Problem_Addressed,Methods_Used,Expert_Knowledge,Main_Findings,Limitations,Relevance_to_Our_Work,DOI_Link
HEX: Human-in-the-loop explainability via deep reinforcement learning,Michael T. Lash,2024,Decision Support Systems,"The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ulti","Explainability, Interpretability, Human-in-the-loop, Deep reinforcement learning, Machine learning, Behavioral machine learning, Decision support",No,"The use of machine learning (ML) models in decision-making contexts, particularly those used in high-stakes decision-making, are fraught with issue and peril since a person – not a machine – must ultimately be held accountable for the consequences of decisions made using such systems. Machine learning explainability (MLX) promises to provide decision-makers with prediction-specific rationale, assuring them that the model-elicited predictions are made for the right reasons and are thus reliable. Few works explicitly consider this key human-in-the-loop (HITL) component, however. In this work we propose HEX, a human-in-the-loop deep reinforcement learning approach to MLX. HEX incorporates 0-distrust projection to synthesize decider-specific explainers that produce explanations strictly in terms of a decider’s preferred explanatory features using any classification model. Our formulation explicitly considers the decision boundary of the ML model in question using a proposed explanatory point mode of explanation, thus ensuring explanations are specific to the ML model in question. We empirically evaluate HEX against other competing methods, finding that HEX is competitive with the state-of-the-art and outperforms other methods in human-in-the-loop scenarios. We conduct a randomized, controlled laboratory experiment utilizing actual explanations elicited from both HEX and competing methods. We causally establish that our method increases decider’s trust and tendency to rely on trusted features.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.dss.2024.114304
Human-algorithm collaborative Bayesian optimization for engineering systems,"Tom Savage, Ehecatl Antonio {del Rio Chanona}",2024,Computers & Chemical Engineering,"Bayesian optimization has proven effective for optimizing expensive-to-evaluate functions in Chemical Engineering. However, valuable physical insights from domain experts are often overlooked. This ar","Bayesian optimization, Experimental design, Human-in-the-loop, Domain knowledge",Yes,"Bayesian optimization has proven effective for optimizing expensive-to-evaluate functions in Chemical Engineering. However, valuable physical insights from domain experts are often overlooked. This article introduces a collaborative Bayesian optimization approach that re-integrates human input into the data-driven decision-making process. By combining high-throughput Bayesian optimization with discrete decision theory, experts can influence the selection of experiments via a discrete choice. We propose a multi-objective approach togenerate a set of high-utility and distinct solutions, from which the expert selects the desired solution for evaluation at each iteration. Our methodology maintains the advantages of Bayesian optimization while incorporating expert knowledge and improving accountability. The approach is demonstrated across various case studies, including bioprocess optimization and reactor geometry design, demonstrating that even with an uninformed practitioner, the algorithm recovers the regret of standard Bayesian optimization. By including continuous expert opinion, the proposed method enables faster convergence and improved accountability for Bayesian optimization in engineering systems.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.compchemeng.2024.108810
Optimization of multi-echelon spare parts inventory systems using multi-agent deep reinforcement learning,"Yifan Zhou, Kai Guo, Cheng Yu, Zhisheng Zhang",2024,Applied Mathematical Modelling,Multi-echelon inventory systems are commonly used in practice to satisfy widely distributed random demands of spare parts in an efficient and cost-effective manner. Optimization of a multi-echelon inv,"Inventory optimization, Emergency transshipment, Multi-agent deep reinforcement learning, Twin delayed deep deterministic policy gradient, Value decomposition",Yes,"Multi-echelon inventory systems are commonly used in practice to satisfy widely distributed random demands of spare parts in an efficient and cost-effective manner. Optimization of a multi-echelon inventory system is a decision-making problem under uncertainties. Classic inventory policies (e.g. (s, S) and (R, Q)) that do not consider the inventory positions of other warehouses become suboptimal due to interrelationships among different warehouses caused by transshipment. The Markov decision process (MDP) is an effective tool for inventory optimization, which does not require a predetermined parameterized policy structure. Unfortunately, both the state and action spaces of MDP suffer from the curse of dimensionality when the number of warehouses increases. This paper optimizes the inventory of a large-scale multi-echelon inventory system using a new multi-agent deep reinforcement learning (MADRL) algorithm named EM-VDTD3 that is developed by introducing value decomposition and experience buffer modification into the twin delayed deep deterministic policy gradient (TD3) algorithm. Each agent in EM-VDTD3 manages a subsystem in the multi-echelon inventory system. Because different agents share the same network parameters, networks are customized to process subsystems with different parameters. Domain knowledge of inventory control is embedded in the learning process of EM-VDTD3 by adding expert experiences to the experience buffer. An efficient approximate method is developed to identify a teacher policy that generates expert experiences. Numerical studies about a spare part inventory system in the wind energy industry show that the proposed EM-VDTD3 outperforms benchmark methods.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.apm.2023.10.039
Bayesian Optimization for automatic tuning of digital multi-loop PID controllers,"João P.L. Coutinho, Lino O. Santos, Marco S. Reis",2023,Computers & Chemical Engineering,"In recent years, the use of Bayesian optimization (BO) for efficient automatic tuning of general controller structures through iterative closed-loop experiments, has been attracting increasing interes","Multi-loop PID tuning, Automatic tuning, Bayesian optimization, Internal model control, Sequential loop closing",Yes,"In recent years, the use of Bayesian optimization (BO) for efficient automatic tuning of general controller structures through iterative closed-loop experiments, has been attracting increasing interest. However, its potential for tuning interactive multi-loop PID controllers in Multi Input Multi Output (MIMO) processes remains largely unexplored. Even though the optimization domain greatly affects closed-loop performance and safety, it is usually defined manually, through expert knowledge or experimentation. This paper presents a novel systematic methodology for defining the optimization domain for automatic multi-loop PID tuning using BO. Sequential loop closing, system identification and tuning relations are used to constrain the bounds on controller parameters to meaningful ranges, including gains and sampling times. This provides an effective way to improve the convergence of BO and secure process safety during closed-loop experiments, without requiring a MIMO process model or extensive prior knowledge. The methodology can be applied ”as is” to single-loop PID controllers.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.compchemeng.2023.108211
Guided probabilistic reinforcement learning for sampling-efficient maintenance scheduling of multi-component system,"Yiming Zhang, Dingyang Zhang, Xiaoge Zhang, Lemiao Qiu, Felix T.S. Chan, Zili Wang, Shuyou Zhang",2023,Applied Mathematical Modelling,"In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforc","Deep Reinforcement Learning, Multi-component System, Probabilistic Machine Learning, Maintenance Scheduling, Sampling-Efficient Learning",Yes,"In recent years, multi-agent deep reinforcement learning has progressed rapidly as reflected by its increasing adoptions in industrial applications. This paper proposes a Guided Probabilistic Reinforcement Learning (Guided-PRL) model to tackle maintenance scheduling of multi-component systems in the presence of uncertainty with the goal of minimizing the overall life-cycle cost. The proposed Guided-PRL is deeply rooted in the Actor-Critic (AC) scheme. Since traditional AC falls short in sampling efficiency and suffers from getting stuck in local minima in the context of multi-agent reinforcement learning, it is thus challenging for the actor network to converge to a solution of desirable quality even when the critic network is properly configured. To address these issues, we develop a generic framework to facilitate effective training of the actor network, and the framework consists of environmental reward modeling, degradation formulation, state representation, and policy optimization. The convergence speed of the actor network is significantly improved with a guided sampling scheme for environment exploration by exploiting rules-based domain expert policies. To handle data scarcity, the environmental modeling and policy optimization are approximated with Bayesian models for effective uncertainty quantification. The Guided-PRL model is evaluated using the simulations of a 12-component system as well as GE90 and CFM56 engines. Compared with four alternative deep reinforcement learning schemes, the Guided-PRL lowers life-cycle cost by 34.92% to 88.07%. In comparison with rules-based expert policies, the Guided-PRL decreases the life-cycle cost by 23.26% to 51.36%.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.apm.2023.03.025
New paradigms for exploiting parallel experiments in Bayesian optimization,"Leonardo D. González, Victor M. Zavala",2023,Computers & Chemical Engineering,"Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential a","Bayesian optimization, High-throughput experiments, Parallelization",No,"Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential algorithm (one experiment is proposed per round) and thus cannot directly exploit high-throughput (parallel) experiments. Diverse modifications to the BO framework have been proposed in the literature to enable exploitation of parallel experiments but such approaches are limited in the degree of parallelization that they can achieve and can lead to redundant experiments (thus wasting resources and potentially compromising performance). In this work, we present new parallel BO paradigms that exploit the structure of the system to partition the design space. Specifically, we propose an approach that partitions the design space by following the level sets of the performance function and an approach that exploits the partially separable structure of the performance function found. We conduct extensive numerical experiments using a reactor case study to benchmark the effectiveness of these approaches against a variety of state-of-the-art parallel algorithms reported in the literature. Our computational results show that our approaches significantly reduce the required search time and increase the probability of finding a global (rather than local) solution.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.compchemeng.2022.108110
Approximability and efficient algorithms for constrained fixed-horizon POMDPs with durative actions,Majid Khonji,2023,Artificial Intelligence,"Partially Observable Markov Decision Process (POMDP) is a fundamental model for probabilistic planning in stochastic domains. More recently, constrained POMDP and chance-constrained POMDP extend the m","Constrained partially observable Markov decision process, Durative actions, Risk-bounded planning, Heuristic search",No,"Partially Observable Markov Decision Process (POMDP) is a fundamental model for probabilistic planning in stochastic domains. More recently, constrained POMDP and chance-constrained POMDP extend the model allowing constraints to be specified on some aspects of the policy in addition to the objective function. Despite their expressive power, these models assume all actions take a fixed duration, which poses a limitation in modeling real-world planning problems. In this work, we propose a unified model for durative POMDP and its constrained extensions. First, we convert these extensions into an Integer Linear Programming (ILP) formulation, which can be solved using existing solvers in the ILP literature. Second, a heuristic search approach is provided that can efficiently prune the search space, guided by solving successive partial ILP programs. Third, we give a theoretical analysis of the problem: unlike short-horizon POMDPs, with policies of a constant depth, which can be solved in polynomial time, the constrained extensions are NP-Hard even with a planning horizon of two and non-negative rewards. To alleviate that, we propose a Fully Polynomial Time Approximation Scheme (FPTAS) that computes (near) optimal deterministic policies in polynomial time. The FPTAS is among the best achievable in theory in terms of approximation ratio. Finally, evaluation results show that our approach is empirically superior to the state-of-the-art fixed-horizon chance-constrained POMDP solver.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2023.103968
Certified reinforcement learning with logic guidance,"Hosein Hasanbeig, Daniel Kroening, Alessandro Abate",2023,Artificial Intelligence,"Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a system","Reinforcement learning, Control synthesis, Policy synthesis, Formal methods, Temporal logics, Automata, Markov decision processes",No,"Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Büchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2023.103949
Task-guided IRL in POMDPs that scales,"Franck Djeumou, Christian Ellis, Murat Cubuktepe, Craig Lennon, Ufuk Topcu",2023,Artificial Intelligence,"In inverse reinforcement learning (IRL), a learning agent infers a reward function encoding the underlying task using demonstrations from experts. However, many existing IRL techniques make the often ","Inverse reinforcement learning, Planning in partially observable environment, Sequential convex optimization",Yes,"In inverse reinforcement learning (IRL), a learning agent infers a reward function encoding the underlying task using demonstrations from experts. However, many existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs). We address two limitations of existing IRL techniques. First, they require an excessive amount of data due to the information asymmetry between the expert and the learner. Second, most of these IRL techniques require solving the computationally intractable forward problem—computing an optimal policy given a reward function—in POMDPs. The developed algorithm reduces the information asymmetry while increasing the data efficiency by incorporating task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations. Further, the algorithm avoids a common source of algorithmic complexity by building on causal entropy as the measure of the likelihood of the demonstrations as opposed to entropy. Nevertheless, the resulting problem is nonconvex due to the so-called forward problem. We solve the intrinsic nonconvexity of the forward problem in a scalable manner through a sequential linear programming scheme that guarantees to converge to a locally optimal policy. In a series of examples, including experiments in a high-fidelity Unity simulator, we demonstrate that even with a limited amount of data and POMDPs with tens of thousands of states, our algorithm learns reward functions and policies that satisfy the task while inducing similar behavior to the expert by leveraging the provided side information.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2023.103856
Risk-aware shielding of Partially Observable Monte Carlo Planning policies,"Giulio Mazzi, Alberto Castellini, Alessandro Farinelli",2023,Artificial Intelligence,Partially Observable Monte Carlo Planning (POMCP) is a powerful online algorithm that can generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of t,"POMDP, POMCP, SMT, Risk-awareness, Shielding",Yes,"Partially Observable Monte Carlo Planning (POMCP) is a powerful online algorithm that can generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. However, the lack of an explicit policy representation hinders interpretability and a proper evaluation of the risks an agent may incur. In this work, we propose a methodology based on Maximum Satisfiability Modulo Theory (MAX-SMT) for analyzing POMCP policies by inspecting their traces, namely, sequences of belief-action pairs generated by the algorithm. The proposed method explores local properties of the policy to build a compact and informative summary of the policy behaviour. Moreover, we introduce a rich and formal language that a domain expert can use to describe the expected behaviour of a policy. In more detail, we present a formulation that directly computes the risk involved in taking actions by considering the high-level elements specified by the expert. The final formula can identify risky decisions taken by POMCP that violate the expert indications. We show that this identification process can be used offline (to improve the policy's explainability and identify anomalous behaviours) or online (to shield the risky decisions of the POMCP algorithm). We present an extended evaluation of our approach on four domains: the well-known tiger and rocksample benchmarks, a problem of velocity regulation in mobile robots, and a problem of battery management in mobile robots. We test the methodology against a state-of-the-art anomaly detection algorithm to show that our approach can be used to identify anomalous behaviours in faulty POMCP. We also show, comparing the performance of shielded and unshielded POMCP, that the shielding mechanism can improve the system's performance. We provide an open-source implementation of the proposed methodologies at https://github.com/GiuMaz/XPOMCP.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2023.103987
An empirical study of Bayesian network parameter learning with monotonic influence constraints,"Yun Zhou, Norman Fenton, Cheng Zhu",2016,Decision Support Systems,"Learning the conditional probability table (CPT) parameters of Bayesian networks (BNs) is a key challenge in real-world decision support applications, especially when there are limited data available.","BN parameter learning, Monotonic influences, Exterior constraints, Experiments on publicly available BNs, Real medical study",Yes,"Learning the conditional probability table (CPT) parameters of Bayesian networks (BNs) is a key challenge in real-world decision support applications, especially when there are limited data available. A conventional way to address this challenge is to introduce domain knowledge/expert judgments that are encoded as qualitative parameter constraints. In this paper we focus on a class of constraints which is naturally encoded in the edges of BNs with monotonic influences. Experimental results indicate that such monotonic influence constraints are widespread in practical BNs (all BNs used in the study contain such monotonic influences). To exploit expert knowledge about such constraints we have developed an improved constrained optimization algorithm, which achieves good parameter learning performance using these constraints, especially when data are limited. Specifically, this algorithm outperforms the previous state-of-the-art and is also robust to errors in labelling the monotonic influences. The method is applied to a real world medical decision support BN where we had access to expert-provided constraints and real hospital data. The results suggest that incorporating expert judgments about monotonic influence constraints can lead to more accurate BNs for decision support and risk analysis.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.dss.2016.05.001
Real-time dynamic programming for Markov decision processes with imprecise probabilities,"Karina V. Delgado, Leliane N. {de Barros}, Daniel B. Dias, Scott Sanner",2016,Artificial Intelligence,"Markov Decision Processes have become the standard model for probabilistic planning. However, when applied to many practical problems, the estimates of transition probabilities are inaccurate. This ma","Probabilistic planning, Markov decision process, Robust planning",Yes,"Markov Decision Processes have become the standard model for probabilistic planning. However, when applied to many practical problems, the estimates of transition probabilities are inaccurate. This may be due to conflicting elicitations from experts or insufficient state transition information. The Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) was introduced to obtain a robust policy where there is uncertainty in the transition. Although it has been proposed a symbolic dynamic programming algorithm for MDP-IPs (called SPUDD-IP) that can solve problems up to 22 state variables, in practice, solving MDP-IP problems is time-consuming. In this paper we propose efficient algorithms for a more general class of MDP-IPs, called Stochastic Shortest Path MDP-IPs (SSP MDP-IPs) that use initial state information to solve complex problems by focusing on reachable states. The (L)RTDP-IP algorithm, a (Labeled) Real Time Dynamic Programming algorithm for SSP MDP-IPs, is proposed together with three different methods for sampling the next state. It is shown here that the convergence of (L)RTDP-IP can be obtained by using any of these three methods, although the Bellman backups for this class of problems prescribe a minimax optimization. As far as we are aware, this is the first asynchronous algorithm for SSP MDP-IPs given in terms of a general set of probability constraints that requires non-linear optimization over imprecise probabilities in the Bellman backup. Our results show up to three orders of magnitude speedup for (L)RTDP-IP when compared with the SPUDD-IP algorithm.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2015.09.005
Software project risk analysis using Bayesian networks with causality constraints,"Yong Hu, Xiangzhou Zhang, E.W.T. Ngai, Ruichu Cai, Mei Liu",2013,Decision Support Systems,Many risks are involved in software development and risk management has become one of the key activities in software development. Bayesian networks (BNs) have been explored as a tool for various risk ,"Software project risk analysis, Bayesian networks, Causality analysis, Knowledge discovery, Expert knowledge constraint",Yes,"Many risks are involved in software development and risk management has become one of the key activities in software development. Bayesian networks (BNs) have been explored as a tool for various risk management practices, including the risk management of software development projects. However, much of the present research on software risk analysis focuses on finding the correlation between risk factors and project outcome. Software project failures are often a result of insufficient and ineffective risk management. To obtain proper and effective risk control, risk planning should be performed based on risk causality which can provide more risk information for decision making. In this study, we propose a model using BNs with causality constraints (BNCC) for risk analysis of software development projects. Through unrestricted automatic causality learning from 302 collected software project data, we demonstrated that the proposed model can not only discover causalities in accordance with the expert knowledge but also perform better in prediction than other algorithms, such as logistic regression, C4.5, Naïve Bayes, and general BNs. This research presents the first causal discovery framework for risk causality analysis of software projects and develops a model using BNCC for application in software project risk management.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.dss.2012.11.001
Reinforcement learning with limited reinforcement: Using Bayes risk for active learning in POMDPs,"Finale Doshi-Velez, Joelle Pineau, Nicholas Roy",2012,Artificial Intelligence,"Acting in domains where an agent must plan several steps ahead to achieve a goal can be a challenging task, especially if the agentʼs sensors provide only noisy or partial information. In this setting","Partially observable Markov decision process, Reinforcement learning, Bayesian methods",Yes,"Acting in domains where an agent must plan several steps ahead to achieve a goal can be a challenging task, especially if the agentʼs sensors provide only noisy or partial information. In this setting, Partially Observable Markov Decision Processes (POMDPs) provide a planning framework that optimally trades between actions that contribute to the agentʼs knowledge and actions that increase the agentʼs immediate reward. However, the task of specifying the POMDPʼs parameters is often onerous. In particular, setting the immediate rewards to achieve a desired balance between information-gathering and acting is often not intuitive. In this work, we propose an approximation based on minimizing the immediate Bayes risk for choosing actions when transition, observation, and reward models are uncertain. The Bayes-risk criterion avoids the computational intractability of solving a POMDP with a multi-dimensional continuous state space; we show it performs well in a variety of problems. We use policy queries—in which we ask an expert for the correct action—to infer the consequences of a potential pitfall without experiencing its effects. More important for human–robot interaction settings, policy queries allow the agent to learn the reward model without the reward values ever being specified.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2012.04.006
Efficient solutions to factored MDPs with imprecise transition probabilities,"Karina Valdivia Delgado, Scott Sanner, Leliane Nunes {de Barros}",2011,Artificial Intelligence,"When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilitie","Probabilistic planning, Markov Decision Process, Robust planning",Yes,"When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MDP transition models from an expert or estimation from data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs, they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP and propose efficient dynamic programming methods to exploit its structure. Noting that the key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional “flat” dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-IPs while producing the lowest error of any approximation algorithm evaluated.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2011.01.001
Decision maps: A framework for multi-criteria decision support under severe uncertainty,"T. Comes, M. Hiete, N. Wijngaards, F. Schultmann",2011,Decision Support Systems,"In complex strategic decision-making situations the need for well-structured support arises. To evaluate decision alternatives, information about the situation and its development must be determined, ","Multi-Criteria Decision Analysis, Scenario-based decision support, Severe uncertainty, Causal Maps",Yes,"In complex strategic decision-making situations the need for well-structured support arises. To evaluate decision alternatives, information about the situation and its development must be determined, managed and processed by the best available experts. For various types of information different reasoning principles have been developed: deterministic, probabilistic, fuzzy and techniques for reasoning under ignorance (i.e., the likelihood of an event cannot be quantified). We propose a new approach based on Decision Maps supporting decision makers under fundamental uncertainty by generating descriptions of different possible situation developments (scenarios) in a distributed manner. The scenarios are evaluated using Multi-Criteria Decision Analysis techniques.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.dss.2011.05.008
Practical solution techniques for first-order MDPs,"Scott Sanner, Craig Boutilier",2009,Artificial Intelligence,"Many traditional solution approaches to relationally specified decision-theoretic planning problems (e.g., those stated in the probabilistic planning domain description language, or PPDDL) ground the ","MDPs, First-order logic, Planning",No,"Many traditional solution approaches to relationally specified decision-theoretic planning problems (e.g., those stated in the probabilistic planning domain description language, or PPDDL) ground the specification with respect to a specific instantiation of domain objects and apply a solution approach directly to the resulting ground Markov decision process (MDP). Unfortunately, the space and time complexity of these grounded solution approaches are polynomial in the number of domain objects and exponential in the predicate arity and the number of nested quantifiers in the relational problem specification. An alternative to grounding a relational planning problem is to tackle the problem directly at the relational level. In this article, we propose one such approach that translates an expressive subset of the PPDDL representation to a first-order MDP (FOMDP) specification and then derives a domain-independent policy without grounding at any intermediate step. However, such generality does not come without its own set of challenges—the purpose of this article is to explore practical solution techniques for solving FOMDPs. To demonstrate the applicability of our techniques, we present proof-of-concept results of our first-order approximate linear programming (FOALP) planner on problems from the probabilistic track of the ICAPS 2004 and 2006 International Planning Competitions.",Needs full text review,Needs evaluation,https://doi.org/10.1016/j.artint.2008.11.003
Stochastic dynamic programming with factored representations,"Craig Boutilier, Richard Dearden, Moisés Goldszmidt",2000,Artificial Intelligence,"Markov decision processes (MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specificat","Decision-theoretic planning, Markov decision processes, Bayesian networks, Regression, Decision trees, Abstraction",No,"Markov decision processes (MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specifications and computations. To alleviate the combinatorial problems associated with such methods, we propose new representational and computational techniques for MDPs that exploit certain types of problem structure. We use dynamic Bayesian networks (with decision trees representing the local families of conditional probability distributions) to represent stochastic actions in an MDP, together with a decision-tree representation of rewards. Based on this representation, we develop versions of standard dynamic programming algorithms that directly manipulate decision-tree representations of policies and value functions. This generally obviates the need for state-by-state computation, aggregating states at the leaves of these trees and requiring computations only for each aggregate state. The key to these algorithms is a decision-theoretic generalization of classic regression analysis, in which we determine the features relevant to predicting expected value. We demonstrate the method empirically on several planning problems, showing significant savings for certain types of domains. We also identify certain classes of problems for which this technique fails to perform well and suggest extensions and related ideas that may prove useful in such circumstances. We also briefly describe an approximation scheme based on this approach.",Needs full text review,Needs evaluation,https://doi.org/10.1016/S0004-3702(00)00033-3
A probabilistic model for interactive decision-making,"Pierfrancesco Reverberi, Maurizio Talamo",1999,Decision Support Systems,A probabilistic reasoning model is defined where the decision maker (d.m.) is engaged in a sequential information-gathering process facing the trade-off between the reliability of the achieved solutio,"Decision-making under uncertainty, Information-gathering strategy, Myopic policy, Interactive solution procedure, Bayesian belief networks",No,"A probabilistic reasoning model is defined where the decision maker (d.m.) is engaged in a sequential information-gathering process facing the trade-off between the reliability of the achieved solution and the associated observation cost. The d.m. is directly involved in the proposed flexible control strategy, which is based on information-theoretic principles. The devised strategy works on a Bayesian belief network that allows the efficient representation and manipulation of the knowledge base relevant to the problem domain. It is shown that this strategy guarantees a constant factor approximate solution with respect to the optimum of the decision problem. Some application examples are also discussed.",Needs full text review,Needs evaluation,https://doi.org/10.1016/S0167-9236(99)00013-5
Planning under time constraints in stochastic domains,"Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, Ann Nicholson",1995,Artificial Intelligence,"We provide a method, based on the theory of Markov decision processes, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world st",N/A,No,"We provide a method, based on the theory of Markov decision processes, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods require time at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables). By using information about the starting state, the reward function, and the transition probabilities of the domain, we restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal. Using this restricted set of states, the planner can generate more or less complete plans depending on the time it has available. Our approach employs several iterative refinement routines for solving different aspects of the decision making problem. We describe the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe experimental results for both the precursor and recurrent problems that demonstrate planning times that grow slowly as a function of domain size and compare their performance to other relevant algorithms.",Needs full text review,Needs evaluation,https://doi.org/10.1016/0004-3702(94)00086-G
